{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT\n",
    "微调将最后一层的第一个token即[CLS]的隐藏向量作为句子的表示，然后输入到softmax层进行分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\learn\\anaconda3\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: boto3 in c:\\users\\learn\\anaconda3\\lib\\site-packages (from transformers) (1.14.38)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\learn\\anaconda3\\lib\\site-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\learn\\anaconda3\\lib\\site-packages (from transformers) (0.1.85)\n",
      "Requirement already satisfied: requests in c:\\users\\learn\\anaconda3\\lib\\site-packages (from transformers) (2.21.0)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\learn\\anaconda3\\lib\\site-packages (from transformers) (0.0.38)\n",
      "Requirement already satisfied: tqdm in c:\\users\\learn\\anaconda3\\lib\\site-packages (from transformers) (4.47.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\learn\\anaconda3\\lib\\site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in c:\\users\\learn\\anaconda3\\lib\\site-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\learn\\anaconda3\\lib\\site-packages (from boto3->transformers) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.18.0,>=1.17.38 in c:\\users\\learn\\anaconda3\\lib\\site-packages (from boto3->transformers) (1.17.38)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\learn\\anaconda3\\lib\\site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\learn\\anaconda3\\lib\\site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\learn\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.2)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\learn\\anaconda3\\lib\\site-packages (from requests->transformers) (1.21.1)\n",
      "Requirement already satisfied: click in c:\\users\\learn\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in c:\\users\\learn\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\learn\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\learn\\anaconda3\\lib\\site-packages (from botocore<1.18.0,>=1.17.38->boto3->transformers) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in c:\\users\\learn\\anaconda3\\lib\\site-packages (from botocore<1.18.0,>=1.17.38->boto3->transformers) (0.15.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-16 23:52:24,263 INFO: Use cuda: True, gpu id: 0.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from transformers import BasicTokenizer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup,get_cosine_with_hard_restarts_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)-15s %(levelname)s: %(message)s')\n",
    "\n",
    "# set seed\n",
    "seed = 666\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# set cuda\n",
    "gpu = 0\n",
    "use_cuda = gpu >= 0 and torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    torch.cuda.set_device(gpu)\n",
    "    device = torch.device(\"cuda\", gpu)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "logging.info(\"Use cuda: %s, gpu id: %d.\", use_cuda, gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data to 10 fold\n",
    "\n",
    "def all_data2fold(fold_num, num=10000):\n",
    "    fold_data = []\n",
    "    f = pd.read_csv(data_file, sep='\\t', encoding='UTF-8')\n",
    "    texts = f['text'].tolist()[:num]\n",
    "    labels = f['label'].tolist()[:num]\n",
    "    \n",
    "    total = len(labels)\n",
    "#     #TODO data enhancement START\n",
    "#     new_textlist = []\n",
    "#     new_labellist = []\n",
    "#     for i in range(total):\n",
    "#         label = labels[i]\n",
    "#         text = texts[i]\n",
    "#         new_arr = text.split()\n",
    "#         np.random.shuffle(new_arr)\n",
    "#         new_text = ' '.join(test)\n",
    "#         new_labellist.append(label)\n",
    "#         new_textlist.append(new_text)\n",
    "    \n",
    "#     labels.extend(new_labellist)\n",
    "#     texts.extend(new_textlist)\n",
    "\n",
    "#     #TODO data enhancement END\n",
    "#     total = len(labels)\n",
    "        \n",
    "    index = list(range(total))\n",
    "    np.random.shuffle(index)\n",
    "\n",
    "    all_texts = []\n",
    "    all_labels = []\n",
    "    for i in index:\n",
    "        all_texts.append(texts[i])\n",
    "        all_labels.append(labels[i])\n",
    "\n",
    "    label2id = {}\n",
    "    for i in range(total):\n",
    "        #print(\"{0},{1}\".format(i,all_labels[i]))\n",
    "        label = all_labels[i]\n",
    "        if label not in label2id:\n",
    "            label2id[label] = [i]\n",
    "        else:\n",
    "            label2id[label].append(i)\n",
    "\n",
    "    all_index = [[] for _ in range(fold_num)]\n",
    "    for label, data in label2id.items():\n",
    "        # print(label, len(data))\n",
    "        batch_size = int(len(data) / fold_num)\n",
    "        other = len(data) - batch_size * fold_num\n",
    "        for i in range(fold_num):\n",
    "            cur_batch_size = batch_size + 1 if i < other else batch_size\n",
    "            # print(cur_batch_size)\n",
    "            batch_data = [data[i * batch_size + b] for b in range(cur_batch_size)]\n",
    "            all_index[i].extend(batch_data)\n",
    "\n",
    "    batch_size = int(total / fold_num)\n",
    "    other_texts = []\n",
    "    other_labels = []\n",
    "    other_num = 0\n",
    "    start = 0\n",
    "    for fold in range(fold_num):\n",
    "        num = len(all_index[fold])\n",
    "        texts = [all_texts[i] for i in all_index[fold]]\n",
    "        labels = [all_labels[i] for i in all_index[fold]]\n",
    "\n",
    "        if num > batch_size:\n",
    "            fold_texts = texts[:batch_size]\n",
    "            other_texts.extend(texts[batch_size:])\n",
    "            fold_labels = labels[:batch_size]\n",
    "            other_labels.extend(labels[batch_size:])\n",
    "            other_num += num - batch_size\n",
    "        elif num < batch_size:\n",
    "            end = start + batch_size - num\n",
    "            fold_texts = texts + other_texts[start: end]\n",
    "            fold_labels = labels + other_labels[start: end]\n",
    "            start = end\n",
    "        else:\n",
    "            fold_texts = texts\n",
    "            fold_labels = labels\n",
    "\n",
    "        assert batch_size == len(fold_labels)\n",
    "\n",
    "        # shuffle\n",
    "        index = list(range(batch_size))\n",
    "        np.random.shuffle(index)\n",
    "\n",
    "        shuffle_fold_texts = []\n",
    "        shuffle_fold_labels = []\n",
    "        for i in index:\n",
    "            shuffle_fold_texts.append(fold_texts[i])\n",
    "            shuffle_fold_labels.append(fold_labels[i])\n",
    "\n",
    "        data = {'label': shuffle_fold_labels, 'text': shuffle_fold_texts}\n",
    "        fold_data.append(data)\n",
    "\n",
    "    logging.info(\"Fold lens %s\", [len(data['label']) for data in fold_data])\n",
    "\n",
    "    return fold_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocab\n",
    "basic_tokenizer = BasicTokenizer()\n",
    "\n",
    "class Vocab():\n",
    "    def __init__(self, train_data):\n",
    "        self.min_count = 5\n",
    "        self.pad = 0\n",
    "        self.unk = 1\n",
    "        self._id2word = ['[PAD]', '[UNK]']\n",
    "        self._id2extword = ['[PAD]', '[UNK]']\n",
    "\n",
    "        self._id2label = []\n",
    "        self.target_names = []\n",
    "\n",
    "        self.build_vocab(train_data)\n",
    "\n",
    "        reverse = lambda x: dict(zip(x, range(len(x))))\n",
    "        self._word2id = reverse(self._id2word)\n",
    "        self._label2id = reverse(self._id2label)\n",
    "\n",
    "        logging.info(\"Build vocab: words %d, labels %d.\" % (self.word_size, self.label_size))\n",
    "\n",
    "    def build_vocab(self, data):\n",
    "        self.word_counter = Counter()\n",
    "\n",
    "        for text in data['text']:\n",
    "            words = text.split()\n",
    "            for word in words:\n",
    "                self.word_counter[word] += 1\n",
    "\n",
    "        for word, count in self.word_counter.most_common():\n",
    "            if count >= self.min_count:\n",
    "                self._id2word.append(word)\n",
    "\n",
    "        label2name = {0: '科技', 1: '股票', 2: '体育', 3: '娱乐', 4: '时政', 5: '社会', 6: '教育', 7: '财经',\n",
    "                      8: '家居', 9: '游戏', 10: '房产', 11: '时尚', 12: '彩票', 13: '星座'}\n",
    "\n",
    "        self.label_counter = Counter(data['label'])\n",
    "\n",
    "        for label in range(len(self.label_counter)):\n",
    "            count = self.label_counter[label]\n",
    "            self._id2label.append(label)\n",
    "            self.target_names.append(label2name[label])\n",
    "\n",
    "    def load_pretrained_embs(self, embfile):\n",
    "        with open(embfile, encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            items = lines[0].split()\n",
    "            word_count, embedding_dim = int(items[0]), int(items[1])\n",
    "\n",
    "        index = len(self._id2extword)\n",
    "        embeddings = np.zeros((word_count + index, embedding_dim))\n",
    "        for line in lines[1:]:\n",
    "            values = line.split()\n",
    "            self._id2extword.append(values[0])\n",
    "            vector = np.array(values[1:], dtype='float64')\n",
    "            embeddings[self.unk] += vector\n",
    "            embeddings[index] = vector\n",
    "            index += 1\n",
    "\n",
    "        embeddings[self.unk] = embeddings[self.unk] / word_count\n",
    "        embeddings = embeddings / np.std(embeddings)\n",
    "\n",
    "        reverse = lambda x: dict(zip(x, range(len(x))))\n",
    "        self._extword2id = reverse(self._id2extword)\n",
    "\n",
    "        assert len(set(self._id2extword)) == len(self._id2extword)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def word2id(self, xs):\n",
    "        if isinstance(xs, list):\n",
    "            return [self._word2id.get(x, self.unk) for x in xs]\n",
    "        return self._word2id.get(xs, self.unk)\n",
    "\n",
    "    def extword2id(self, xs):\n",
    "        if isinstance(xs, list):\n",
    "            return [self._extword2id.get(x, self.unk) for x in xs]\n",
    "        return self._extword2id.get(xs, self.unk)\n",
    "\n",
    "    def label2id(self, xs):\n",
    "        if isinstance(xs, list):\n",
    "            return [self._label2id.get(x, self.unk) for x in xs]\n",
    "        return self._label2id.get(xs, self.unk)\n",
    "\n",
    "    @property\n",
    "    def word_size(self):\n",
    "        return len(self._id2word)\n",
    "\n",
    "    @property\n",
    "    def extword_size(self):\n",
    "        return len(self._id2extword)\n",
    "\n",
    "    @property\n",
    "    def label_size(self):\n",
    "        return len(self._id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build module\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        self.weight.data.normal_(mean=0.0, std=0.05)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        b = np.zeros(hidden_size, dtype=np.float32)\n",
    "        self.bias.data.copy_(torch.from_numpy(b))\n",
    "\n",
    "        self.query = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        self.query.data.normal_(mean=0.0, std=0.05)\n",
    "\n",
    "    def forward(self, batch_hidden, batch_masks):\n",
    "        # batch_hidden: b x len x hidden_size (2 * hidden_size of lstm)\n",
    "        # batch_masks:  b x len\n",
    "\n",
    "        # linear\n",
    "        key = torch.matmul(batch_hidden, self.weight) + self.bias  # b x len x hidden\n",
    "\n",
    "        # compute attention\n",
    "        outputs = torch.matmul(key, self.query)  # b x len\n",
    "\n",
    "        masked_outputs = outputs.masked_fill((1 - batch_masks).bool(), float(-1e32))\n",
    "\n",
    "        attn_scores = F.softmax(masked_outputs, dim=1)  # b x len\n",
    "\n",
    "        # 对于全零向量，-1e32的结果为 1/len, -inf为nan, 额外补0\n",
    "        masked_attn_scores = attn_scores.masked_fill((1 - batch_masks).bool(), 0.0)\n",
    "\n",
    "        # sum weighted sources\n",
    "        batch_outputs = torch.bmm(masked_attn_scores.unsqueeze(1), key).squeeze(1)  # b x hidden\n",
    "\n",
    "        return batch_outputs, attn_scores\n",
    "\n",
    "\n",
    "class WordBertEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WordBertEncoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.tokenizer = WhitespaceTokenizer()\n",
    "        \n",
    "        self.bert = BertModel.from_pretrained(bert_path)\n",
    "\n",
    "        self.pooled = False\n",
    "        logging.info('Build Bert encoder with pooled {}.'.format(self.pooled))\n",
    "\n",
    "    def encode(self, tokens):\n",
    "        tokens = self.tokenizer.tokenize(tokens)\n",
    "        return tokens\n",
    "\n",
    "    def get_bert_parameters(self):\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in self.bert.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in self.bert.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "    #微调将最后一层的第一个token即[CLS]的隐藏向量作为句子的表示，然后输入到softmax层进行分类。\n",
    "    def forward(self, input_ids, token_type_ids):\n",
    "        # input_ids: sen_num x bert_len\n",
    "        # token_type_ids: sen_num  x bert_len\n",
    "\n",
    "        # sen_num x bert_len x 256, sen_num x 256\n",
    "        sequence_output, pooled_output = self.bert(input_ids=input_ids, token_type_ids=token_type_ids)\n",
    "\n",
    "        if self.pooled:\n",
    "            reps = pooled_output\n",
    "        else:\n",
    "            reps = sequence_output[:, 0, :]  # sen_num x 256\n",
    "\n",
    "        if self.training:\n",
    "            reps = self.dropout(reps)\n",
    "\n",
    "        return reps\n",
    "\n",
    "\n",
    "class WhitespaceTokenizer():\n",
    "    \"\"\"WhitespaceTokenizer with vocab.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        vocab_file = bert_path + 'vocab.txt'\n",
    "        self._token2id = self.load_vocab(vocab_file)\n",
    "        self._id2token = {v: k for k, v in self._token2id.items()}\n",
    "        self.max_len = 256\n",
    "        self.unk = 1\n",
    "\n",
    "        logging.info(\"Build Bert vocab with size %d.\" % (self.vocab_size))\n",
    "\n",
    "    def load_vocab(self, vocab_file):\n",
    "        f = open(vocab_file, 'r')\n",
    "        lines = f.readlines()\n",
    "        lines = list(map(lambda x: x.strip(), lines))\n",
    "        vocab = dict(zip(lines, range(len(lines))))\n",
    "        return vocab\n",
    "\n",
    "    def tokenize(self, tokens):\n",
    "        assert len(tokens) <= self.max_len - 2\n",
    "        tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "        output_tokens = self.token2id(tokens)\n",
    "        return output_tokens\n",
    "\n",
    "    def token2id(self, xs):\n",
    "        if isinstance(xs, list):\n",
    "            return [self._token2id.get(x, self.unk) for x in xs]\n",
    "        return self._token2id.get(xs, self.unk)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self._id2token)\n",
    "\n",
    "\n",
    "class SentEncoder(nn.Module):\n",
    "    def __init__(self, sent_rep_size):\n",
    "        super(SentEncoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.sent_lstm = nn.LSTM(\n",
    "            input_size=sent_rep_size,\n",
    "            hidden_size=sent_hidden_size,\n",
    "            num_layers=sent_num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "    def forward(self, sent_reps, sent_masks):\n",
    "        # sent_reps:  b x doc_len x sent_rep_size\n",
    "        # sent_masks: b x doc_len\n",
    "\n",
    "        sent_hiddens, _ = self.sent_lstm(sent_reps)  # b x doc_len x hidden*2\n",
    "        sent_hiddens = sent_hiddens * sent_masks.unsqueeze(2)\n",
    "\n",
    "        if self.training:\n",
    "            sent_hiddens = self.dropout(sent_hiddens)\n",
    "\n",
    "        return sent_hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab):\n",
    "        super(Model, self).__init__()\n",
    "        self.sent_rep_size = 256\n",
    "        self.doc_rep_size = sent_hidden_size * 2\n",
    "        self.all_parameters = {}\n",
    "        parameters = []\n",
    "        self.word_encoder = WordBertEncoder()\n",
    "        bert_parameters = self.word_encoder.get_bert_parameters()\n",
    "\n",
    "        self.sent_encoder = SentEncoder(self.sent_rep_size)\n",
    "        self.sent_attention = Attention(self.doc_rep_size)\n",
    "        parameters.extend(list(filter(lambda p: p.requires_grad, self.sent_encoder.parameters())))\n",
    "        parameters.extend(list(filter(lambda p: p.requires_grad, self.sent_attention.parameters())))\n",
    "\n",
    "        self.out = nn.Linear(self.doc_rep_size, vocab.label_size, bias=True)\n",
    "        parameters.extend(list(filter(lambda p: p.requires_grad, self.out.parameters())))\n",
    "\n",
    "        if use_cuda:\n",
    "            self.to(device)\n",
    "\n",
    "        if len(parameters) > 0:\n",
    "            self.all_parameters[\"basic_parameters\"] = parameters\n",
    "        self.all_parameters[\"bert_parameters\"] = bert_parameters\n",
    "\n",
    "        logging.info('Build model with bert word encoder, lstm sent encoder.')\n",
    "\n",
    "        para_num = sum([np.prod(list(p.size())) for p in self.parameters()])\n",
    "        logging.info('Model param num: %.2f M.' % (para_num / 1e6))\n",
    "\n",
    "    def forward(self, batch_inputs):\n",
    "        # batch_inputs(batch_inputs1, batch_inputs2): b x doc_len x sent_len\n",
    "        # batch_masks : b x doc_len x sent_len\n",
    "        batch_inputs1, batch_inputs2, batch_masks = batch_inputs\n",
    "        batch_size, max_doc_len, max_sent_len = batch_inputs1.shape[0], batch_inputs1.shape[1], batch_inputs1.shape[2]\n",
    "        batch_inputs1 = batch_inputs1.view(batch_size * max_doc_len, max_sent_len)  # sen_num x sent_len\n",
    "        batch_inputs2 = batch_inputs2.view(batch_size * max_doc_len, max_sent_len)  # sen_num x sent_len\n",
    "        batch_masks = batch_masks.view(batch_size * max_doc_len, max_sent_len)  # sen_num x sent_len\n",
    "\n",
    "        sent_reps = self.word_encoder(batch_inputs1, batch_inputs2)  # sen_num x sent_rep_size\n",
    "\n",
    "        sent_reps = sent_reps.view(batch_size, max_doc_len, self.sent_rep_size)  # b x doc_len x sent_rep_size\n",
    "        batch_masks = batch_masks.view(batch_size, max_doc_len, max_sent_len)  # b x doc_len x max_sent_len\n",
    "        sent_masks = batch_masks.bool().any(2).float()  # b x doc_len\n",
    "\n",
    "        sent_hiddens = self.sent_encoder(sent_reps, sent_masks)  # b x doc_len x doc_rep_size\n",
    "        doc_reps, atten_scores = self.sent_attention(sent_hiddens, sent_masks)  # b x doc_rep_size\n",
    "\n",
    "        batch_outputs = self.out(doc_reps)  # b x num_labels\n",
    "\n",
    "        return batch_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Optimizer:\n",
    "    def __init__(self, model_parameters, steps):\n",
    "        self.all_params = []\n",
    "        self.optims = []\n",
    "        self.schedulers = []\n",
    "\n",
    "        for name, parameters in model_parameters.items():\n",
    "            if name.startswith(\"basic\"):\n",
    "                #optim = torch.optim.Adam(parameters, lr=learning_rate)\n",
    "                #optim = torch.optim.SGD(parameters, lr=learning_rate)\n",
    "                optim = torch.optim.RMSprop(parameters, lr=learning_rate)\n",
    "                #optim = torch.optim.Adagrad(parameters, lr=learning_rate)\n",
    "                \n",
    "                self.optims.append(optim)\n",
    "\n",
    "                l = lambda step: decay ** (step // decay_step)\n",
    "                scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=l)\n",
    "                self.schedulers.append(scheduler)\n",
    "                self.all_params.extend(parameters)\n",
    "            elif name.startswith(\"bert\"):\n",
    "                optim_bert = AdamW(parameters, bert_lr, eps=1e-8)\n",
    "                self.optims.append(optim_bert)\n",
    "\n",
    "                scheduler_bert = get_linear_schedule_with_warmup(optim_bert, 0, steps)\n",
    "                self.schedulers.append(scheduler_bert)\n",
    "\n",
    "                for group in parameters:\n",
    "                    for p in group['params']:\n",
    "                        self.all_params.append(p)\n",
    "            else:\n",
    "                Exception(\"no nameed parameters.\")\n",
    "\n",
    "        self.num = len(self.optims)\n",
    "\n",
    "    def step(self):\n",
    "        for optim, scheduler in zip(self.optims, self.schedulers):\n",
    "            optim.step()\n",
    "            scheduler.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for optim in self.optims:\n",
    "            optim.zero_grad()\n",
    "\n",
    "    def get_lr(self):\n",
    "        lrs = tuple(map(lambda x: x.get_lr()[-1], self.schedulers))\n",
    "        lr = ' %.5f' * self.num\n",
    "        res = lr % lrs\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataset\n",
    "def sentence_split(text, vocab, max_sent_len=256, max_segment=16):\n",
    "    words = text.strip().split()\n",
    "    document_len = len(words)\n",
    "\n",
    "    index = list(range(0, document_len, max_sent_len))\n",
    "    index.append(document_len)\n",
    "\n",
    "    segments = []\n",
    "    for i in range(len(index) - 1):\n",
    "        segment = words[index[i]: index[i + 1]]\n",
    "        assert len(segment) > 0\n",
    "        segment = [word if word in vocab._id2word else '<UNK>' for word in segment]\n",
    "        segments.append([len(segment), segment])\n",
    "\n",
    "    assert len(segments) > 0\n",
    "    if len(segments) > max_segment:\n",
    "        segment_ = int(max_segment / 2)\n",
    "        return segments[:segment_] + segments[-segment_:]\n",
    "    else:\n",
    "        return segments\n",
    "\n",
    "\n",
    "def get_examples(data, word_encoder, vocab, max_sent_len=256, max_segment=8):\n",
    "    label2id = vocab.label2id\n",
    "    examples = []\n",
    "\n",
    "    for text, label in zip(data['text'], data['label']):\n",
    "        # label\n",
    "        id = label2id(label)\n",
    "\n",
    "        # words\n",
    "        sents_words = sentence_split(text, vocab, max_sent_len-2, max_segment)\n",
    "        doc = []\n",
    "        for sent_len, sent_words in sents_words:\n",
    "            token_ids = word_encoder.encode(sent_words)\n",
    "            sent_len = len(token_ids)\n",
    "            token_type_ids = [0] * sent_len\n",
    "            doc.append([sent_len, token_ids, token_type_ids])\n",
    "        examples.append([id, len(doc), doc])\n",
    "\n",
    "    logging.info('Total %d docs.' % len(examples))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build loader\n",
    "\n",
    "def batch_slice(data, batch_size):\n",
    "    batch_num = int(np.ceil(len(data) / float(batch_size)))\n",
    "    for i in range(batch_num):\n",
    "        cur_batch_size = batch_size if i < batch_num - 1 else len(data) - batch_size * i\n",
    "        docs = [data[i * batch_size + b] for b in range(cur_batch_size)]\n",
    "\n",
    "        yield docs\n",
    "\n",
    "\n",
    "def data_iter(data, batch_size, shuffle=True, noise=1.0):\n",
    "    \"\"\"\n",
    "    randomly permute data, then sort by source length, and partition into batches\n",
    "    ensure that the length of  sentences in each batch\n",
    "    \"\"\"\n",
    "\n",
    "    batched_data = []\n",
    "    if shuffle:\n",
    "        np.random.shuffle(data)\n",
    "\n",
    "        lengths = [example[1] for example in data]\n",
    "        noisy_lengths = [- (l + np.random.uniform(- noise, noise)) for l in lengths]\n",
    "        sorted_indices = np.argsort(noisy_lengths).tolist()\n",
    "        sorted_data = [data[i] for i in sorted_indices]\n",
    "    else:\n",
    "        sorted_data =data\n",
    "        \n",
    "    batched_data.extend(list(batch_slice(sorted_data, batch_size)))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(batched_data)\n",
    "\n",
    "    for batch in batched_data:\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some function\n",
    "\n",
    "def get_score(y_ture, y_pred):\n",
    "    y_ture = np.array(y_ture)\n",
    "    y_pred = np.array(y_pred)\n",
    "    f1 = f1_score(y_ture, y_pred, average='macro') * 100\n",
    "    p = precision_score(y_ture, y_pred, average='macro') * 100\n",
    "    r = recall_score(y_ture, y_pred, average='macro') * 100\n",
    "\n",
    "    return ((reformat(p), reformat(r), reformat(f1))), reformat(f1)\n",
    "\n",
    "\n",
    "def reformat(num):\n",
    "    return float(\"{:.2f}\".format(num))\n",
    "\n",
    "def reformat4(num):\n",
    "    return float(\"{:.4f}\".format(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build trainer\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, model, vocab):\n",
    "        self.model = model\n",
    "        self.report = True\n",
    "        \n",
    "        self.train_data = get_examples(train_data, model.word_encoder, vocab)\n",
    "        self.batch_num = int(np.ceil(len(self.train_data) / float(train_batch_size)))\n",
    "        self.dev_data = get_examples(dev_data, model.word_encoder, vocab)\n",
    "        self.test_data = get_examples(test_data, model.word_encoder, vocab)\n",
    "\n",
    "        # criterion\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # label name\n",
    "        self.target_names = vocab.target_names\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer = Optimizer(model.all_parameters, steps=self.batch_num * epochs)\n",
    "\n",
    "        # count\n",
    "        self.step = 0\n",
    "        self.early_stop = -1\n",
    "        self.best_train_f1, self.best_dev_f1 = 0, 0\n",
    "        self.last_epoch = epochs\n",
    "\n",
    "    def train(self):\n",
    "        logging.info('Start training...')\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train_f1 = self._train(epoch)\n",
    "\n",
    "            dev_f1 = self._eval(epoch)\n",
    "\n",
    "            if self.best_dev_f1 <= dev_f1:\n",
    "                logging.info(\n",
    "                    \"Exceed history dev = %.2f, current dev = %.2f\" % (self.best_dev_f1, dev_f1))\n",
    "                torch.save(self.model.state_dict(), save_model)\n",
    "\n",
    "                self.best_train_f1 = train_f1\n",
    "                self.best_dev_f1 = dev_f1\n",
    "                self.early_stop = 0\n",
    "            else:\n",
    "                self.early_stop += 1\n",
    "                if self.early_stop == early_stops:\n",
    "                    logging.info(\n",
    "                        \"Eearly stop in epoch %d, best train: %.2f, dev: %.2f\" % (\n",
    "                            epoch - early_stops, self.best_train_f1, self.best_dev_f1))\n",
    "                    self.last_epoch = epoch\n",
    "                    break\n",
    "    def test(self):\n",
    "        self.model.load_state_dict(torch.load(save_model))\n",
    "        self._eval(self.last_epoch + 1, test=True)\n",
    "\n",
    "    def _train(self, epoch):\n",
    "        self.optimizer.zero_grad()\n",
    "        self.model.train()\n",
    "\n",
    "        start_time = time.time()\n",
    "        epoch_start_time = time.time()\n",
    "        overall_losses = 0\n",
    "        losses = 0\n",
    "        batch_idx = 1\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for batch_data in data_iter(self.train_data, train_batch_size, shuffle=True):\n",
    "            torch.cuda.empty_cache()\n",
    "            batch_inputs, batch_labels = self.batch2tensor(batch_data)\n",
    "            batch_outputs = self.model(batch_inputs)\n",
    "            loss = self.criterion(batch_outputs, batch_labels)\n",
    "            loss.backward()\n",
    "\n",
    "            loss_value = loss.detach().cpu().item()\n",
    "            losses += loss_value\n",
    "            overall_losses += loss_value\n",
    "\n",
    "            y_pred.extend(torch.max(batch_outputs, dim=1)[1].cpu().numpy().tolist())\n",
    "            y_true.extend(batch_labels.cpu().numpy().tolist())\n",
    "\n",
    "            nn.utils.clip_grad_norm_(self.optimizer.all_params, max_norm=clip)\n",
    "            for optimizer, scheduler in zip(self.optimizer.optims, self.optimizer.schedulers):\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            self.step += 1\n",
    "\n",
    "            if batch_idx % log_interval == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "\n",
    "                lrs = self.optimizer.get_lr()\n",
    "                logging.info(\n",
    "                    '| epoch {:3d} | step {:3d} | batch {:3d}/{:3d} | lr{} | loss {:.4f} | s/batch {:.2f}'.format(\n",
    "                        epoch, self.step, batch_idx, self.batch_num, lrs,\n",
    "                        losses / log_interval,\n",
    "                        elapsed / log_interval))\n",
    "\n",
    "                losses = 0\n",
    "                start_time = time.time()\n",
    "\n",
    "            batch_idx += 1\n",
    "\n",
    "        overall_losses /= self.batch_num\n",
    "        during_time = time.time() - epoch_start_time\n",
    "\n",
    "        # reformat\n",
    "        overall_losses = reformat4(overall_losses)\n",
    "        score, f1 = get_score(y_true, y_pred)\n",
    "\n",
    "        logging.info(\n",
    "            '| epoch {:3d} | score {} | f1 {} | loss {:.4f} | time {:.2f}'.format(epoch, score, f1,\n",
    "                                                                                  overall_losses,\n",
    "                                                                                  during_time))\n",
    "        if set(y_true) == set(y_pred) and self.report:\n",
    "            report = classification_report(y_true, y_pred, digits=4, target_names=self.target_names)\n",
    "            logging.info('\\n' + report)\n",
    "\n",
    "        return f1\n",
    "\n",
    "    def _eval(self, epoch, test=False):\n",
    "        self.model.eval()\n",
    "        start_time = time.time()\n",
    "        data = self.test_data if test else self.dev_data\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        with torch.no_grad():\n",
    "            for batch_data in data_iter(data, test_batch_size, shuffle=False):\n",
    "                torch.cuda.empty_cache()\n",
    "                batch_inputs, batch_labels = self.batch2tensor(batch_data)\n",
    "                batch_outputs = self.model(batch_inputs)\n",
    "                y_pred.extend(torch.max(batch_outputs, dim=1)[1].cpu().numpy().tolist())\n",
    "                y_true.extend(batch_labels.cpu().numpy().tolist())\n",
    "\n",
    "            score, f1 = get_score(y_true, y_pred)\n",
    "\n",
    "            during_time = time.time() - start_time\n",
    "            \n",
    "            if test:\n",
    "                df = pd.DataFrame({'label': y_pred})\n",
    "                df.to_csv(save_test, index=False, sep=',')\n",
    "            else:\n",
    "                logging.info(\n",
    "                    '| epoch {:3d} | dev | score {} | f1 {} | time {:.2f}'.format(epoch, score, f1,\n",
    "                                                                              during_time))\n",
    "                if set(y_true) == set(y_pred) and self.report:\n",
    "                    report = classification_report(y_true, y_pred, digits=4, target_names=self.target_names)\n",
    "                    logging.info('\\n' + report)\n",
    "\n",
    "        return f1\n",
    "\n",
    "    def batch2tensor(self, batch_data):\n",
    "        '''\n",
    "            [[label, doc_len, [[sent_len, [sent_id0, ...], [sent_id1, ...]], ...]]\n",
    "        '''\n",
    "        batch_size = len(batch_data)\n",
    "        doc_labels = []\n",
    "        doc_lens = []\n",
    "        doc_max_sent_len = []\n",
    "        for doc_data in batch_data:\n",
    "            doc_labels.append(doc_data[0])\n",
    "            doc_lens.append(doc_data[1])\n",
    "            sent_lens = [sent_data[0] for sent_data in doc_data[2]]\n",
    "            max_sent_len = max(sent_lens)\n",
    "            doc_max_sent_len.append(max_sent_len)\n",
    "\n",
    "        max_doc_len = max(doc_lens)\n",
    "        max_sent_len = max(doc_max_sent_len)\n",
    "\n",
    "        batch_inputs1 = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.int64)\n",
    "        batch_inputs2 = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.int64)\n",
    "        batch_masks = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.float32)\n",
    "        batch_labels = torch.LongTensor(doc_labels)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            for sent_idx in range(doc_lens[b]):\n",
    "                sent_data = batch_data[b][2][sent_idx]\n",
    "                for word_idx in range(sent_data[0]):\n",
    "                    batch_inputs1[b, sent_idx, word_idx] = sent_data[1][word_idx]\n",
    "                    batch_inputs2[b, sent_idx, word_idx] = sent_data[2][word_idx]\n",
    "                    batch_masks[b, sent_idx, word_idx] = 1\n",
    "\n",
    "        if use_cuda:\n",
    "            batch_inputs1 = batch_inputs1.to(device)\n",
    "            batch_inputs2 = batch_inputs2.to(device)\n",
    "            batch_masks = batch_masks.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "        return (batch_inputs1, batch_inputs2, batch_masks), batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始参数设定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_num = 10\n",
    "fold_id = fold_num - 1\n",
    "data_file = '../input/train_set.csv'\n",
    "test_data_file = '../input/test_a.csv'\n",
    "\n",
    "save_model = '../output/bert20200817.bin'\n",
    "save_test = '../output/bert20200817.csv'\n",
    "\n",
    "# build word encoder\n",
    "bert_path = '../emb/bert-mini/'\n",
    "\n",
    "# build sent encoder\n",
    "sent_hidden_size = 256\n",
    "sent_num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-16 23:52:48,215 INFO: Fold lens [20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000, 20000]\n"
     ]
    }
   ],
   "source": [
    "fold_data = all_data2fold(fold_num,200000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build train, dev, test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev\n",
    "dev_data = fold_data[fold_id]\n",
    "\n",
    "# train\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "for i in range(0, fold_id):\n",
    "    data = fold_data[i]\n",
    "    train_texts.extend(data['text'])\n",
    "    train_labels.extend(data['label'])\n",
    "\n",
    "train_data = {'label': train_labels, 'text': train_texts}\n",
    "\n",
    "# test\n",
    "f = pd.read_csv(test_data_file, sep='\\t', encoding='UTF-8')\n",
    "texts = f['text'].tolist()\n",
    "test_data = {'label': [0] * len(texts), 'text': texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-16 23:53:42,901 INFO: Build vocab: words 5983, labels 14.\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置超参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build optimizer\n",
    "learning_rate = 2e-4\n",
    "dropout = 0.25\n",
    "bert_lr = 5e-5\n",
    "decay = .75\n",
    "decay_step = 1000\n",
    "clip = 5.0\n",
    "epochs = 3\n",
    "early_stops = 3\n",
    "log_interval = 50\n",
    "test_batch_size = 10\n",
    "train_batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-16 23:53:42,938 INFO: Build Bert vocab with size 5981.\n",
      "2020-08-16 23:53:42,941 INFO: loading configuration file ../emb/bert-mini/config.json\n",
      "2020-08-16 23:53:42,944 INFO: Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 256,\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 5981\n",
      "}\n",
      "\n",
      "2020-08-16 23:53:42,947 INFO: loading weights file ../emb/bert-mini/pytorch_model.bin\n",
      "2020-08-16 23:53:43,028 INFO: Build Bert encoder with pooled False.\n",
      "2020-08-16 23:53:43,096 INFO: Build model with bert word encoder, lstm sent encoder.\n",
      "2020-08-16 23:53:43,101 INFO: Model param num: 7.72 M.\n"
     ]
    }
   ],
   "source": [
    "model = Model(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-17 00:08:50,473 INFO: Total 180000 docs.\n",
      "2020-08-17 00:10:29,734 INFO: Total 20000 docs.\n",
      "2020-08-17 00:14:43,636 INFO: Total 50000 docs.\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "trainer = Trainer(model, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目标使loss尽量最小之后，再进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-17 00:14:46,015 INFO: Start training...\n",
      "2020-08-17 00:15:12,415 INFO: | epoch   1 | step  50 | batch  50/18000 | lr 0.00020 0.00005 | loss 1.8312 | s/batch 0.53\n",
      "2020-08-17 00:15:34,471 INFO: | epoch   1 | step 100 | batch 100/18000 | lr 0.00020 0.00005 | loss 1.0157 | s/batch 0.44\n",
      "2020-08-17 00:15:57,242 INFO: | epoch   1 | step 150 | batch 150/18000 | lr 0.00020 0.00005 | loss 0.7807 | s/batch 0.46\n",
      "2020-08-17 00:16:21,822 INFO: | epoch   1 | step 200 | batch 200/18000 | lr 0.00020 0.00005 | loss 0.7633 | s/batch 0.49\n",
      "2020-08-17 00:16:45,623 INFO: | epoch   1 | step 250 | batch 250/18000 | lr 0.00020 0.00005 | loss 0.7405 | s/batch 0.48\n",
      "2020-08-17 00:17:08,194 INFO: | epoch   1 | step 300 | batch 300/18000 | lr 0.00020 0.00005 | loss 0.5798 | s/batch 0.45\n",
      "2020-08-17 00:17:31,828 INFO: | epoch   1 | step 350 | batch 350/18000 | lr 0.00020 0.00005 | loss 0.5903 | s/batch 0.47\n",
      "2020-08-17 00:17:55,783 INFO: | epoch   1 | step 400 | batch 400/18000 | lr 0.00020 0.00005 | loss 0.4864 | s/batch 0.48\n",
      "2020-08-17 00:18:18,524 INFO: | epoch   1 | step 450 | batch 450/18000 | lr 0.00020 0.00005 | loss 0.4659 | s/batch 0.45\n",
      "2020-08-17 00:18:44,104 INFO: | epoch   1 | step 500 | batch 500/18000 | lr 0.00020 0.00005 | loss 0.5233 | s/batch 0.51\n",
      "2020-08-17 00:19:08,313 INFO: | epoch   1 | step 550 | batch 550/18000 | lr 0.00020 0.00005 | loss 0.4618 | s/batch 0.48\n",
      "2020-08-17 00:19:31,186 INFO: | epoch   1 | step 600 | batch 600/18000 | lr 0.00020 0.00005 | loss 0.4439 | s/batch 0.46\n",
      "2020-08-17 00:19:51,626 INFO: | epoch   1 | step 650 | batch 650/18000 | lr 0.00020 0.00005 | loss 0.4746 | s/batch 0.41\n",
      "2020-08-17 00:20:18,585 INFO: | epoch   1 | step 700 | batch 700/18000 | lr 0.00020 0.00005 | loss 0.4172 | s/batch 0.54\n",
      "2020-08-17 00:20:40,659 INFO: | epoch   1 | step 750 | batch 750/18000 | lr 0.00020 0.00005 | loss 0.4164 | s/batch 0.44\n",
      "2020-08-17 00:21:04,845 INFO: | epoch   1 | step 800 | batch 800/18000 | lr 0.00020 0.00005 | loss 0.3783 | s/batch 0.48\n",
      "2020-08-17 00:21:30,550 INFO: | epoch   1 | step 850 | batch 850/18000 | lr 0.00020 0.00005 | loss 0.5273 | s/batch 0.51\n",
      "2020-08-17 00:21:54,944 INFO: | epoch   1 | step 900 | batch 900/18000 | lr 0.00020 0.00005 | loss 0.4367 | s/batch 0.49\n",
      "2020-08-17 00:22:17,901 INFO: | epoch   1 | step 950 | batch 950/18000 | lr 0.00020 0.00005 | loss 0.3675 | s/batch 0.46\n",
      "2020-08-17 00:22:39,332 INFO: | epoch   1 | step 1000 | batch 1000/18000 | lr 0.00015 0.00005 | loss 0.4729 | s/batch 0.43\n",
      "2020-08-17 00:23:05,573 INFO: | epoch   1 | step 1050 | batch 1050/18000 | lr 0.00015 0.00005 | loss 0.4063 | s/batch 0.52\n",
      "2020-08-17 00:23:30,542 INFO: | epoch   1 | step 1100 | batch 1100/18000 | lr 0.00015 0.00005 | loss 0.5265 | s/batch 0.50\n",
      "2020-08-17 00:23:55,338 INFO: | epoch   1 | step 1150 | batch 1150/18000 | lr 0.00015 0.00005 | loss 0.4669 | s/batch 0.50\n",
      "2020-08-17 00:24:16,270 INFO: | epoch   1 | step 1200 | batch 1200/18000 | lr 0.00015 0.00005 | loss 0.3299 | s/batch 0.42\n",
      "2020-08-17 00:24:38,738 INFO: | epoch   1 | step 1250 | batch 1250/18000 | lr 0.00015 0.00005 | loss 0.3837 | s/batch 0.45\n",
      "2020-08-17 00:25:02,460 INFO: | epoch   1 | step 1300 | batch 1300/18000 | lr 0.00015 0.00005 | loss 0.4350 | s/batch 0.47\n",
      "2020-08-17 00:25:24,731 INFO: | epoch   1 | step 1350 | batch 1350/18000 | lr 0.00015 0.00005 | loss 0.4485 | s/batch 0.45\n",
      "2020-08-17 00:25:50,947 INFO: | epoch   1 | step 1400 | batch 1400/18000 | lr 0.00015 0.00005 | loss 0.3295 | s/batch 0.52\n",
      "2020-08-17 00:26:15,227 INFO: | epoch   1 | step 1450 | batch 1450/18000 | lr 0.00015 0.00005 | loss 0.4690 | s/batch 0.49\n",
      "2020-08-17 00:26:39,075 INFO: | epoch   1 | step 1500 | batch 1500/18000 | lr 0.00015 0.00005 | loss 0.3706 | s/batch 0.48\n",
      "2020-08-17 00:27:02,175 INFO: | epoch   1 | step 1550 | batch 1550/18000 | lr 0.00015 0.00005 | loss 0.3926 | s/batch 0.46\n",
      "2020-08-17 00:27:26,400 INFO: | epoch   1 | step 1600 | batch 1600/18000 | lr 0.00015 0.00005 | loss 0.3338 | s/batch 0.48\n",
      "2020-08-17 00:27:46,852 INFO: | epoch   1 | step 1650 | batch 1650/18000 | lr 0.00015 0.00005 | loss 0.3383 | s/batch 0.41\n",
      "2020-08-17 00:28:10,589 INFO: | epoch   1 | step 1700 | batch 1700/18000 | lr 0.00015 0.00005 | loss 0.3607 | s/batch 0.47\n",
      "2020-08-17 00:28:32,796 INFO: | epoch   1 | step 1750 | batch 1750/18000 | lr 0.00015 0.00005 | loss 0.3446 | s/batch 0.44\n",
      "2020-08-17 00:28:57,357 INFO: | epoch   1 | step 1800 | batch 1800/18000 | lr 0.00015 0.00005 | loss 0.3600 | s/batch 0.49\n",
      "2020-08-17 00:29:21,248 INFO: | epoch   1 | step 1850 | batch 1850/18000 | lr 0.00015 0.00005 | loss 0.4600 | s/batch 0.48\n",
      "2020-08-17 00:29:46,872 INFO: | epoch   1 | step 1900 | batch 1900/18000 | lr 0.00015 0.00005 | loss 0.4201 | s/batch 0.51\n",
      "2020-08-17 00:30:10,223 INFO: | epoch   1 | step 1950 | batch 1950/18000 | lr 0.00015 0.00005 | loss 0.4101 | s/batch 0.47\n",
      "2020-08-17 00:30:32,115 INFO: | epoch   1 | step 2000 | batch 2000/18000 | lr 0.00011 0.00005 | loss 0.3191 | s/batch 0.44\n",
      "2020-08-17 00:30:55,824 INFO: | epoch   1 | step 2050 | batch 2050/18000 | lr 0.00011 0.00005 | loss 0.2989 | s/batch 0.47\n",
      "2020-08-17 00:31:22,264 INFO: | epoch   1 | step 2100 | batch 2100/18000 | lr 0.00011 0.00005 | loss 0.2999 | s/batch 0.53\n",
      "2020-08-17 00:31:49,041 INFO: | epoch   1 | step 2150 | batch 2150/18000 | lr 0.00011 0.00005 | loss 0.4160 | s/batch 0.54\n",
      "2020-08-17 00:32:10,900 INFO: | epoch   1 | step 2200 | batch 2200/18000 | lr 0.00011 0.00005 | loss 0.3663 | s/batch 0.44\n",
      "2020-08-17 00:32:32,367 INFO: | epoch   1 | step 2250 | batch 2250/18000 | lr 0.00011 0.00005 | loss 0.3627 | s/batch 0.43\n",
      "2020-08-17 00:32:55,895 INFO: | epoch   1 | step 2300 | batch 2300/18000 | lr 0.00011 0.00005 | loss 0.3210 | s/batch 0.47\n",
      "2020-08-17 00:33:17,356 INFO: | epoch   1 | step 2350 | batch 2350/18000 | lr 0.00011 0.00005 | loss 0.3494 | s/batch 0.43\n",
      "2020-08-17 00:33:38,306 INFO: | epoch   1 | step 2400 | batch 2400/18000 | lr 0.00011 0.00005 | loss 0.2434 | s/batch 0.42\n",
      "2020-08-17 00:34:03,620 INFO: | epoch   1 | step 2450 | batch 2450/18000 | lr 0.00011 0.00005 | loss 0.2546 | s/batch 0.51\n",
      "2020-08-17 00:34:27,455 INFO: | epoch   1 | step 2500 | batch 2500/18000 | lr 0.00011 0.00005 | loss 0.2971 | s/batch 0.48\n",
      "2020-08-17 00:34:53,049 INFO: | epoch   1 | step 2550 | batch 2550/18000 | lr 0.00011 0.00005 | loss 0.3618 | s/batch 0.51\n",
      "2020-08-17 00:35:17,116 INFO: | epoch   1 | step 2600 | batch 2600/18000 | lr 0.00011 0.00005 | loss 0.3461 | s/batch 0.48\n",
      "2020-08-17 00:35:42,017 INFO: | epoch   1 | step 2650 | batch 2650/18000 | lr 0.00011 0.00005 | loss 0.2539 | s/batch 0.50\n",
      "2020-08-17 00:36:05,636 INFO: | epoch   1 | step 2700 | batch 2700/18000 | lr 0.00011 0.00005 | loss 0.2753 | s/batch 0.47\n",
      "2020-08-17 00:36:26,635 INFO: | epoch   1 | step 2750 | batch 2750/18000 | lr 0.00011 0.00005 | loss 0.3459 | s/batch 0.42\n",
      "2020-08-17 00:36:52,082 INFO: | epoch   1 | step 2800 | batch 2800/18000 | lr 0.00011 0.00005 | loss 0.2796 | s/batch 0.51\n",
      "2020-08-17 00:37:14,217 INFO: | epoch   1 | step 2850 | batch 2850/18000 | lr 0.00011 0.00005 | loss 0.2592 | s/batch 0.44\n",
      "2020-08-17 00:37:40,937 INFO: | epoch   1 | step 2900 | batch 2900/18000 | lr 0.00011 0.00005 | loss 0.3224 | s/batch 0.53\n",
      "2020-08-17 00:38:05,998 INFO: | epoch   1 | step 2950 | batch 2950/18000 | lr 0.00011 0.00005 | loss 0.2762 | s/batch 0.50\n",
      "2020-08-17 00:38:29,339 INFO: | epoch   1 | step 3000 | batch 3000/18000 | lr 0.00008 0.00005 | loss 0.3079 | s/batch 0.47\n",
      "2020-08-17 00:38:52,203 INFO: | epoch   1 | step 3050 | batch 3050/18000 | lr 0.00008 0.00005 | loss 0.2789 | s/batch 0.46\n",
      "2020-08-17 00:39:16,512 INFO: | epoch   1 | step 3100 | batch 3100/18000 | lr 0.00008 0.00005 | loss 0.4162 | s/batch 0.49\n",
      "2020-08-17 00:39:41,352 INFO: | epoch   1 | step 3150 | batch 3150/18000 | lr 0.00008 0.00005 | loss 0.2173 | s/batch 0.50\n",
      "2020-08-17 00:40:02,235 INFO: | epoch   1 | step 3200 | batch 3200/18000 | lr 0.00008 0.00005 | loss 0.2728 | s/batch 0.42\n",
      "2020-08-17 00:40:27,869 INFO: | epoch   1 | step 3250 | batch 3250/18000 | lr 0.00008 0.00005 | loss 0.2464 | s/batch 0.51\n",
      "2020-08-17 00:40:49,242 INFO: | epoch   1 | step 3300 | batch 3300/18000 | lr 0.00008 0.00005 | loss 0.3028 | s/batch 0.43\n",
      "2020-08-17 00:41:14,978 INFO: | epoch   1 | step 3350 | batch 3350/18000 | lr 0.00008 0.00005 | loss 0.3019 | s/batch 0.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-17 00:41:40,306 INFO: | epoch   1 | step 3400 | batch 3400/18000 | lr 0.00008 0.00005 | loss 0.3152 | s/batch 0.51\n",
      "2020-08-17 00:42:03,890 INFO: | epoch   1 | step 3450 | batch 3450/18000 | lr 0.00008 0.00005 | loss 0.3905 | s/batch 0.47\n",
      "2020-08-17 00:42:27,096 INFO: | epoch   1 | step 3500 | batch 3500/18000 | lr 0.00008 0.00005 | loss 0.3249 | s/batch 0.46\n",
      "2020-08-17 00:42:52,343 INFO: | epoch   1 | step 3550 | batch 3550/18000 | lr 0.00008 0.00005 | loss 0.3488 | s/batch 0.50\n",
      "2020-08-17 00:43:15,038 INFO: | epoch   1 | step 3600 | batch 3600/18000 | lr 0.00008 0.00005 | loss 0.3331 | s/batch 0.45\n",
      "2020-08-17 00:43:38,816 INFO: | epoch   1 | step 3650 | batch 3650/18000 | lr 0.00008 0.00005 | loss 0.2393 | s/batch 0.48\n",
      "2020-08-17 00:44:03,492 INFO: | epoch   1 | step 3700 | batch 3700/18000 | lr 0.00008 0.00005 | loss 0.2966 | s/batch 0.49\n",
      "2020-08-17 00:44:26,885 INFO: | epoch   1 | step 3750 | batch 3750/18000 | lr 0.00008 0.00005 | loss 0.3827 | s/batch 0.47\n",
      "2020-08-17 00:44:48,755 INFO: | epoch   1 | step 3800 | batch 3800/18000 | lr 0.00008 0.00005 | loss 0.2985 | s/batch 0.44\n",
      "2020-08-17 00:45:12,713 INFO: | epoch   1 | step 3850 | batch 3850/18000 | lr 0.00008 0.00005 | loss 0.3022 | s/batch 0.48\n",
      "2020-08-17 00:45:38,398 INFO: | epoch   1 | step 3900 | batch 3900/18000 | lr 0.00008 0.00005 | loss 0.2761 | s/batch 0.51\n",
      "2020-08-17 00:46:00,981 INFO: | epoch   1 | step 3950 | batch 3950/18000 | lr 0.00008 0.00005 | loss 0.2793 | s/batch 0.45\n",
      "2020-08-17 00:46:25,488 INFO: | epoch   1 | step 4000 | batch 4000/18000 | lr 0.00006 0.00005 | loss 0.3809 | s/batch 0.49\n",
      "2020-08-17 00:46:47,525 INFO: | epoch   1 | step 4050 | batch 4050/18000 | lr 0.00006 0.00005 | loss 0.2137 | s/batch 0.44\n",
      "2020-08-17 00:47:08,606 INFO: | epoch   1 | step 4100 | batch 4100/18000 | lr 0.00006 0.00005 | loss 0.2490 | s/batch 0.42\n",
      "2020-08-17 00:47:33,256 INFO: | epoch   1 | step 4150 | batch 4150/18000 | lr 0.00006 0.00005 | loss 0.2448 | s/batch 0.49\n",
      "2020-08-17 00:47:56,966 INFO: | epoch   1 | step 4200 | batch 4200/18000 | lr 0.00006 0.00005 | loss 0.2611 | s/batch 0.47\n",
      "2020-08-17 00:48:20,542 INFO: | epoch   1 | step 4250 | batch 4250/18000 | lr 0.00006 0.00005 | loss 0.2760 | s/batch 0.47\n",
      "2020-08-17 00:48:45,560 INFO: | epoch   1 | step 4300 | batch 4300/18000 | lr 0.00006 0.00005 | loss 0.3020 | s/batch 0.50\n",
      "2020-08-17 00:49:08,556 INFO: | epoch   1 | step 4350 | batch 4350/18000 | lr 0.00006 0.00005 | loss 0.2277 | s/batch 0.46\n",
      "2020-08-17 00:49:32,038 INFO: | epoch   1 | step 4400 | batch 4400/18000 | lr 0.00006 0.00005 | loss 0.2238 | s/batch 0.47\n",
      "2020-08-17 00:49:57,496 INFO: | epoch   1 | step 4450 | batch 4450/18000 | lr 0.00006 0.00005 | loss 0.2137 | s/batch 0.51\n",
      "2020-08-17 00:50:18,617 INFO: | epoch   1 | step 4500 | batch 4500/18000 | lr 0.00006 0.00005 | loss 0.2467 | s/batch 0.42\n",
      "2020-08-17 00:50:41,335 INFO: | epoch   1 | step 4550 | batch 4550/18000 | lr 0.00006 0.00005 | loss 0.2624 | s/batch 0.45\n",
      "2020-08-17 00:51:04,656 INFO: | epoch   1 | step 4600 | batch 4600/18000 | lr 0.00006 0.00005 | loss 0.2578 | s/batch 0.47\n",
      "2020-08-17 00:51:26,605 INFO: | epoch   1 | step 4650 | batch 4650/18000 | lr 0.00006 0.00005 | loss 0.3501 | s/batch 0.44\n",
      "2020-08-17 00:51:47,373 INFO: | epoch   1 | step 4700 | batch 4700/18000 | lr 0.00006 0.00005 | loss 0.2463 | s/batch 0.42\n",
      "2020-08-17 00:52:09,081 INFO: | epoch   1 | step 4750 | batch 4750/18000 | lr 0.00006 0.00005 | loss 0.2943 | s/batch 0.43\n",
      "2020-08-17 00:52:34,109 INFO: | epoch   1 | step 4800 | batch 4800/18000 | lr 0.00006 0.00005 | loss 0.2780 | s/batch 0.50\n",
      "2020-08-17 00:52:57,861 INFO: | epoch   1 | step 4850 | batch 4850/18000 | lr 0.00006 0.00005 | loss 0.3204 | s/batch 0.47\n",
      "2020-08-17 00:53:23,561 INFO: | epoch   1 | step 4900 | batch 4900/18000 | lr 0.00006 0.00005 | loss 0.3096 | s/batch 0.51\n",
      "2020-08-17 00:53:49,451 INFO: | epoch   1 | step 4950 | batch 4950/18000 | lr 0.00006 0.00005 | loss 0.2580 | s/batch 0.52\n",
      "2020-08-17 00:54:15,460 INFO: | epoch   1 | step 5000 | batch 5000/18000 | lr 0.00005 0.00005 | loss 0.2520 | s/batch 0.52\n",
      "2020-08-17 00:54:39,500 INFO: | epoch   1 | step 5050 | batch 5050/18000 | lr 0.00005 0.00005 | loss 0.2244 | s/batch 0.48\n",
      "2020-08-17 00:55:04,978 INFO: | epoch   1 | step 5100 | batch 5100/18000 | lr 0.00005 0.00005 | loss 0.2253 | s/batch 0.51\n",
      "2020-08-17 00:55:26,052 INFO: | epoch   1 | step 5150 | batch 5150/18000 | lr 0.00005 0.00005 | loss 0.2285 | s/batch 0.42\n",
      "2020-08-17 00:55:48,515 INFO: | epoch   1 | step 5200 | batch 5200/18000 | lr 0.00005 0.00005 | loss 0.2833 | s/batch 0.45\n",
      "2020-08-17 00:56:09,508 INFO: | epoch   1 | step 5250 | batch 5250/18000 | lr 0.00005 0.00005 | loss 0.2946 | s/batch 0.42\n",
      "2020-08-17 00:56:32,203 INFO: | epoch   1 | step 5300 | batch 5300/18000 | lr 0.00005 0.00005 | loss 0.2720 | s/batch 0.45\n",
      "2020-08-17 00:56:52,672 INFO: | epoch   1 | step 5350 | batch 5350/18000 | lr 0.00005 0.00005 | loss 0.2706 | s/batch 0.41\n",
      "2020-08-17 00:57:14,960 INFO: | epoch   1 | step 5400 | batch 5400/18000 | lr 0.00005 0.00005 | loss 0.3113 | s/batch 0.45\n",
      "2020-08-17 00:57:41,712 INFO: | epoch   1 | step 5450 | batch 5450/18000 | lr 0.00005 0.00004 | loss 0.2408 | s/batch 0.53\n",
      "2020-08-17 00:58:05,115 INFO: | epoch   1 | step 5500 | batch 5500/18000 | lr 0.00005 0.00004 | loss 0.3016 | s/batch 0.47\n",
      "2020-08-17 00:58:26,931 INFO: | epoch   1 | step 5550 | batch 5550/18000 | lr 0.00005 0.00004 | loss 0.2531 | s/batch 0.44\n",
      "2020-08-17 00:58:49,386 INFO: | epoch   1 | step 5600 | batch 5600/18000 | lr 0.00005 0.00004 | loss 0.2889 | s/batch 0.45\n",
      "2020-08-17 00:59:12,636 INFO: | epoch   1 | step 5650 | batch 5650/18000 | lr 0.00005 0.00004 | loss 0.2214 | s/batch 0.46\n",
      "2020-08-17 00:59:36,404 INFO: | epoch   1 | step 5700 | batch 5700/18000 | lr 0.00005 0.00004 | loss 0.2751 | s/batch 0.48\n",
      "2020-08-17 01:00:00,666 INFO: | epoch   1 | step 5750 | batch 5750/18000 | lr 0.00005 0.00004 | loss 0.1940 | s/batch 0.49\n",
      "2020-08-17 01:00:25,407 INFO: | epoch   1 | step 5800 | batch 5800/18000 | lr 0.00005 0.00004 | loss 0.2460 | s/batch 0.49\n",
      "2020-08-17 01:00:51,543 INFO: | epoch   1 | step 5850 | batch 5850/18000 | lr 0.00005 0.00004 | loss 0.2385 | s/batch 0.52\n",
      "2020-08-17 01:01:15,241 INFO: | epoch   1 | step 5900 | batch 5900/18000 | lr 0.00005 0.00004 | loss 0.2614 | s/batch 0.47\n",
      "2020-08-17 01:01:37,828 INFO: | epoch   1 | step 5950 | batch 5950/18000 | lr 0.00005 0.00004 | loss 0.3171 | s/batch 0.45\n",
      "2020-08-17 01:02:02,658 INFO: | epoch   1 | step 6000 | batch 6000/18000 | lr 0.00004 0.00004 | loss 0.2493 | s/batch 0.50\n",
      "2020-08-17 01:02:25,273 INFO: | epoch   1 | step 6050 | batch 6050/18000 | lr 0.00004 0.00004 | loss 0.3005 | s/batch 0.45\n",
      "2020-08-17 01:02:47,315 INFO: | epoch   1 | step 6100 | batch 6100/18000 | lr 0.00004 0.00004 | loss 0.2181 | s/batch 0.44\n",
      "2020-08-17 01:03:10,924 INFO: | epoch   1 | step 6150 | batch 6150/18000 | lr 0.00004 0.00004 | loss 0.3171 | s/batch 0.47\n",
      "2020-08-17 01:03:33,766 INFO: | epoch   1 | step 6200 | batch 6200/18000 | lr 0.00004 0.00004 | loss 0.2154 | s/batch 0.46\n",
      "2020-08-17 01:03:55,550 INFO: | epoch   1 | step 6250 | batch 6250/18000 | lr 0.00004 0.00004 | loss 0.2966 | s/batch 0.44\n",
      "2020-08-17 01:04:19,418 INFO: | epoch   1 | step 6300 | batch 6300/18000 | lr 0.00004 0.00004 | loss 0.2665 | s/batch 0.48\n",
      "2020-08-17 01:04:42,759 INFO: | epoch   1 | step 6350 | batch 6350/18000 | lr 0.00004 0.00004 | loss 0.2745 | s/batch 0.47\n",
      "2020-08-17 01:05:08,589 INFO: | epoch   1 | step 6400 | batch 6400/18000 | lr 0.00004 0.00004 | loss 0.1938 | s/batch 0.52\n",
      "2020-08-17 01:05:28,548 INFO: | epoch   1 | step 6450 | batch 6450/18000 | lr 0.00004 0.00004 | loss 0.2324 | s/batch 0.40\n",
      "2020-08-17 01:05:54,890 INFO: | epoch   1 | step 6500 | batch 6500/18000 | lr 0.00004 0.00004 | loss 0.2385 | s/batch 0.53\n",
      "2020-08-17 01:06:17,545 INFO: | epoch   1 | step 6550 | batch 6550/18000 | lr 0.00004 0.00004 | loss 0.2245 | s/batch 0.45\n",
      "2020-08-17 01:06:39,283 INFO: | epoch   1 | step 6600 | batch 6600/18000 | lr 0.00004 0.00004 | loss 0.2486 | s/batch 0.43\n",
      "2020-08-17 01:07:02,279 INFO: | epoch   1 | step 6650 | batch 6650/18000 | lr 0.00004 0.00004 | loss 0.2376 | s/batch 0.46\n",
      "2020-08-17 01:07:23,866 INFO: | epoch   1 | step 6700 | batch 6700/18000 | lr 0.00004 0.00004 | loss 0.2713 | s/batch 0.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-17 01:07:45,413 INFO: | epoch   1 | step 6750 | batch 6750/18000 | lr 0.00004 0.00004 | loss 0.2847 | s/batch 0.43\n",
      "2020-08-17 01:08:07,147 INFO: | epoch   1 | step 6800 | batch 6800/18000 | lr 0.00004 0.00004 | loss 0.2452 | s/batch 0.43\n",
      "2020-08-17 01:08:27,659 INFO: | epoch   1 | step 6850 | batch 6850/18000 | lr 0.00004 0.00004 | loss 0.2017 | s/batch 0.41\n",
      "2020-08-17 01:08:49,003 INFO: | epoch   1 | step 6900 | batch 6900/18000 | lr 0.00004 0.00004 | loss 0.2438 | s/batch 0.43\n",
      "2020-08-17 01:09:11,457 INFO: | epoch   1 | step 6950 | batch 6950/18000 | lr 0.00004 0.00004 | loss 0.1810 | s/batch 0.45\n",
      "2020-08-17 01:09:35,740 INFO: | epoch   1 | step 7000 | batch 7000/18000 | lr 0.00003 0.00004 | loss 0.2358 | s/batch 0.49\n",
      "2020-08-17 01:09:59,828 INFO: | epoch   1 | step 7050 | batch 7050/18000 | lr 0.00003 0.00004 | loss 0.2500 | s/batch 0.48\n",
      "2020-08-17 01:10:20,579 INFO: | epoch   1 | step 7100 | batch 7100/18000 | lr 0.00003 0.00004 | loss 0.2326 | s/batch 0.41\n",
      "2020-08-17 01:10:44,768 INFO: | epoch   1 | step 7150 | batch 7150/18000 | lr 0.00003 0.00004 | loss 0.2559 | s/batch 0.48\n",
      "2020-08-17 01:11:06,642 INFO: | epoch   1 | step 7200 | batch 7200/18000 | lr 0.00003 0.00004 | loss 0.2611 | s/batch 0.44\n",
      "2020-08-17 01:11:30,132 INFO: | epoch   1 | step 7250 | batch 7250/18000 | lr 0.00003 0.00004 | loss 0.2735 | s/batch 0.47\n",
      "2020-08-17 01:11:56,588 INFO: | epoch   1 | step 7300 | batch 7300/18000 | lr 0.00003 0.00004 | loss 0.2721 | s/batch 0.53\n",
      "2020-08-17 01:12:19,680 INFO: | epoch   1 | step 7350 | batch 7350/18000 | lr 0.00003 0.00004 | loss 0.2195 | s/batch 0.46\n",
      "2020-08-17 01:12:41,935 INFO: | epoch   1 | step 7400 | batch 7400/18000 | lr 0.00003 0.00004 | loss 0.1931 | s/batch 0.45\n",
      "2020-08-17 01:13:04,414 INFO: | epoch   1 | step 7450 | batch 7450/18000 | lr 0.00003 0.00004 | loss 0.2567 | s/batch 0.45\n",
      "2020-08-17 01:13:25,911 INFO: | epoch   1 | step 7500 | batch 7500/18000 | lr 0.00003 0.00004 | loss 0.2097 | s/batch 0.43\n",
      "2020-08-17 01:13:50,382 INFO: | epoch   1 | step 7550 | batch 7550/18000 | lr 0.00003 0.00004 | loss 0.2499 | s/batch 0.49\n",
      "2020-08-17 01:14:15,575 INFO: | epoch   1 | step 7600 | batch 7600/18000 | lr 0.00003 0.00004 | loss 0.2422 | s/batch 0.50\n",
      "2020-08-17 01:14:41,413 INFO: | epoch   1 | step 7650 | batch 7650/18000 | lr 0.00003 0.00004 | loss 0.2823 | s/batch 0.52\n",
      "2020-08-17 01:15:05,877 INFO: | epoch   1 | step 7700 | batch 7700/18000 | lr 0.00003 0.00004 | loss 0.2612 | s/batch 0.49\n",
      "2020-08-17 01:15:32,929 INFO: | epoch   1 | step 7750 | batch 7750/18000 | lr 0.00003 0.00004 | loss 0.2424 | s/batch 0.54\n",
      "2020-08-17 01:15:54,625 INFO: | epoch   1 | step 7800 | batch 7800/18000 | lr 0.00003 0.00004 | loss 0.2933 | s/batch 0.43\n",
      "2020-08-17 01:16:19,881 INFO: | epoch   1 | step 7850 | batch 7850/18000 | lr 0.00003 0.00004 | loss 0.2411 | s/batch 0.51\n",
      "2020-08-17 01:16:43,683 INFO: | epoch   1 | step 7900 | batch 7900/18000 | lr 0.00003 0.00004 | loss 0.3295 | s/batch 0.48\n",
      "2020-08-17 01:17:09,514 INFO: | epoch   1 | step 7950 | batch 7950/18000 | lr 0.00003 0.00004 | loss 0.2769 | s/batch 0.52\n",
      "2020-08-17 01:17:38,041 INFO: | epoch   1 | step 8000 | batch 8000/18000 | lr 0.00002 0.00004 | loss 0.2779 | s/batch 0.57\n",
      "2020-08-17 01:18:03,250 INFO: | epoch   1 | step 8050 | batch 8050/18000 | lr 0.00002 0.00004 | loss 0.2784 | s/batch 0.50\n",
      "2020-08-17 01:18:26,645 INFO: | epoch   1 | step 8100 | batch 8100/18000 | lr 0.00002 0.00004 | loss 0.2551 | s/batch 0.47\n",
      "2020-08-17 01:18:49,075 INFO: | epoch   1 | step 8150 | batch 8150/18000 | lr 0.00002 0.00004 | loss 0.2602 | s/batch 0.45\n",
      "2020-08-17 01:19:14,271 INFO: | epoch   1 | step 8200 | batch 8200/18000 | lr 0.00002 0.00004 | loss 0.2634 | s/batch 0.50\n",
      "2020-08-17 01:19:36,484 INFO: | epoch   1 | step 8250 | batch 8250/18000 | lr 0.00002 0.00004 | loss 0.1653 | s/batch 0.44\n",
      "2020-08-17 01:19:56,030 INFO: | epoch   1 | step 8300 | batch 8300/18000 | lr 0.00002 0.00004 | loss 0.2113 | s/batch 0.39\n",
      "2020-08-17 01:20:18,149 INFO: | epoch   1 | step 8350 | batch 8350/18000 | lr 0.00002 0.00004 | loss 0.1857 | s/batch 0.44\n",
      "2020-08-17 01:20:40,358 INFO: | epoch   1 | step 8400 | batch 8400/18000 | lr 0.00002 0.00004 | loss 0.2315 | s/batch 0.44\n",
      "2020-08-17 01:21:05,744 INFO: | epoch   1 | step 8450 | batch 8450/18000 | lr 0.00002 0.00004 | loss 0.2440 | s/batch 0.51\n",
      "2020-08-17 01:21:29,669 INFO: | epoch   1 | step 8500 | batch 8500/18000 | lr 0.00002 0.00004 | loss 0.2508 | s/batch 0.48\n",
      "2020-08-17 01:21:53,353 INFO: | epoch   1 | step 8550 | batch 8550/18000 | lr 0.00002 0.00004 | loss 0.2453 | s/batch 0.47\n",
      "2020-08-17 01:22:17,353 INFO: | epoch   1 | step 8600 | batch 8600/18000 | lr 0.00002 0.00004 | loss 0.2310 | s/batch 0.48\n",
      "2020-08-17 01:22:45,291 INFO: | epoch   1 | step 8650 | batch 8650/18000 | lr 0.00002 0.00004 | loss 0.2302 | s/batch 0.56\n",
      "2020-08-17 01:23:07,004 INFO: | epoch   1 | step 8700 | batch 8700/18000 | lr 0.00002 0.00004 | loss 0.2523 | s/batch 0.43\n",
      "2020-08-17 01:23:29,700 INFO: | epoch   1 | step 8750 | batch 8750/18000 | lr 0.00002 0.00004 | loss 0.1811 | s/batch 0.45\n",
      "2020-08-17 01:23:50,240 INFO: | epoch   1 | step 8800 | batch 8800/18000 | lr 0.00002 0.00004 | loss 0.2301 | s/batch 0.41\n",
      "2020-08-17 01:24:12,528 INFO: | epoch   1 | step 8850 | batch 8850/18000 | lr 0.00002 0.00004 | loss 0.2590 | s/batch 0.45\n",
      "2020-08-17 01:24:38,075 INFO: | epoch   1 | step 8900 | batch 8900/18000 | lr 0.00002 0.00004 | loss 0.2430 | s/batch 0.51\n",
      "2020-08-17 01:25:01,123 INFO: | epoch   1 | step 8950 | batch 8950/18000 | lr 0.00002 0.00004 | loss 0.1995 | s/batch 0.46\n",
      "2020-08-17 01:25:23,706 INFO: | epoch   1 | step 9000 | batch 9000/18000 | lr 0.00002 0.00004 | loss 0.2151 | s/batch 0.45\n",
      "2020-08-17 01:25:49,240 INFO: | epoch   1 | step 9050 | batch 9050/18000 | lr 0.00002 0.00004 | loss 0.2450 | s/batch 0.51\n",
      "2020-08-17 01:26:12,860 INFO: | epoch   1 | step 9100 | batch 9100/18000 | lr 0.00002 0.00004 | loss 0.2598 | s/batch 0.47\n",
      "2020-08-17 01:26:33,136 INFO: | epoch   1 | step 9150 | batch 9150/18000 | lr 0.00002 0.00004 | loss 0.2715 | s/batch 0.41\n",
      "2020-08-17 01:26:55,584 INFO: | epoch   1 | step 9200 | batch 9200/18000 | lr 0.00002 0.00004 | loss 0.2220 | s/batch 0.45\n",
      "2020-08-17 01:27:18,730 INFO: | epoch   1 | step 9250 | batch 9250/18000 | lr 0.00002 0.00004 | loss 0.2334 | s/batch 0.46\n",
      "2020-08-17 01:27:42,599 INFO: | epoch   1 | step 9300 | batch 9300/18000 | lr 0.00002 0.00004 | loss 0.1785 | s/batch 0.48\n",
      "2020-08-17 01:28:06,303 INFO: | epoch   1 | step 9350 | batch 9350/18000 | lr 0.00002 0.00004 | loss 0.2387 | s/batch 0.47\n",
      "2020-08-17 01:28:28,925 INFO: | epoch   1 | step 9400 | batch 9400/18000 | lr 0.00002 0.00004 | loss 0.2203 | s/batch 0.45\n",
      "2020-08-17 01:28:50,362 INFO: | epoch   1 | step 9450 | batch 9450/18000 | lr 0.00002 0.00004 | loss 0.1988 | s/batch 0.43\n",
      "2020-08-17 01:29:12,163 INFO: | epoch   1 | step 9500 | batch 9500/18000 | lr 0.00002 0.00004 | loss 0.2285 | s/batch 0.44\n",
      "2020-08-17 01:29:34,510 INFO: | epoch   1 | step 9550 | batch 9550/18000 | lr 0.00002 0.00004 | loss 0.1403 | s/batch 0.45\n",
      "2020-08-17 01:29:55,506 INFO: | epoch   1 | step 9600 | batch 9600/18000 | lr 0.00002 0.00004 | loss 0.2294 | s/batch 0.42\n",
      "2020-08-17 01:30:18,223 INFO: | epoch   1 | step 9650 | batch 9650/18000 | lr 0.00002 0.00004 | loss 0.2188 | s/batch 0.45\n",
      "2020-08-17 01:30:44,904 INFO: | epoch   1 | step 9700 | batch 9700/18000 | lr 0.00002 0.00004 | loss 0.2274 | s/batch 0.53\n",
      "2020-08-17 01:31:08,504 INFO: | epoch   1 | step 9750 | batch 9750/18000 | lr 0.00002 0.00004 | loss 0.2664 | s/batch 0.47\n",
      "2020-08-17 01:31:33,316 INFO: | epoch   1 | step 9800 | batch 9800/18000 | lr 0.00002 0.00004 | loss 0.1872 | s/batch 0.50\n",
      "2020-08-17 01:31:58,272 INFO: | epoch   1 | step 9850 | batch 9850/18000 | lr 0.00002 0.00004 | loss 0.1885 | s/batch 0.50\n",
      "2020-08-17 01:32:21,684 INFO: | epoch   1 | step 9900 | batch 9900/18000 | lr 0.00002 0.00004 | loss 0.1816 | s/batch 0.47\n",
      "2020-08-17 01:32:44,997 INFO: | epoch   1 | step 9950 | batch 9950/18000 | lr 0.00002 0.00004 | loss 0.2147 | s/batch 0.47\n",
      "2020-08-17 01:33:08,571 INFO: | epoch   1 | step 10000 | batch 10000/18000 | lr 0.00001 0.00004 | loss 0.2216 | s/batch 0.47\n",
      "2020-08-17 01:33:33,006 INFO: | epoch   1 | step 10050 | batch 10050/18000 | lr 0.00001 0.00004 | loss 0.2414 | s/batch 0.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-17 01:34:00,324 INFO: | epoch   1 | step 10100 | batch 10100/18000 | lr 0.00001 0.00004 | loss 0.2781 | s/batch 0.55\n",
      "2020-08-17 01:34:23,554 INFO: | epoch   1 | step 10150 | batch 10150/18000 | lr 0.00001 0.00004 | loss 0.2379 | s/batch 0.46\n",
      "2020-08-17 01:34:44,320 INFO: | epoch   1 | step 10200 | batch 10200/18000 | lr 0.00001 0.00004 | loss 0.1466 | s/batch 0.42\n",
      "2020-08-17 01:35:07,105 INFO: | epoch   1 | step 10250 | batch 10250/18000 | lr 0.00001 0.00004 | loss 0.2260 | s/batch 0.46\n",
      "2020-08-17 01:35:30,512 INFO: | epoch   1 | step 10300 | batch 10300/18000 | lr 0.00001 0.00004 | loss 0.1925 | s/batch 0.47\n",
      "2020-08-17 01:35:52,736 INFO: | epoch   1 | step 10350 | batch 10350/18000 | lr 0.00001 0.00004 | loss 0.2543 | s/batch 0.44\n",
      "2020-08-17 01:36:16,017 INFO: | epoch   1 | step 10400 | batch 10400/18000 | lr 0.00001 0.00004 | loss 0.2486 | s/batch 0.47\n",
      "2020-08-17 01:36:38,957 INFO: | epoch   1 | step 10450 | batch 10450/18000 | lr 0.00001 0.00004 | loss 0.2336 | s/batch 0.46\n",
      "2020-08-17 01:37:00,671 INFO: | epoch   1 | step 10500 | batch 10500/18000 | lr 0.00001 0.00004 | loss 0.2529 | s/batch 0.43\n",
      "2020-08-17 01:37:22,985 INFO: | epoch   1 | step 10550 | batch 10550/18000 | lr 0.00001 0.00004 | loss 0.2475 | s/batch 0.45\n",
      "2020-08-17 01:37:45,423 INFO: | epoch   1 | step 10600 | batch 10600/18000 | lr 0.00001 0.00004 | loss 0.1980 | s/batch 0.45\n",
      "2020-08-17 01:38:09,706 INFO: | epoch   1 | step 10650 | batch 10650/18000 | lr 0.00001 0.00004 | loss 0.2105 | s/batch 0.49\n",
      "2020-08-17 01:38:33,488 INFO: | epoch   1 | step 10700 | batch 10700/18000 | lr 0.00001 0.00004 | loss 0.2327 | s/batch 0.48\n",
      "2020-08-17 01:38:56,296 INFO: | epoch   1 | step 10750 | batch 10750/18000 | lr 0.00001 0.00004 | loss 0.1400 | s/batch 0.46\n",
      "2020-08-17 01:39:19,932 INFO: | epoch   1 | step 10800 | batch 10800/18000 | lr 0.00001 0.00004 | loss 0.1759 | s/batch 0.47\n",
      "2020-08-17 01:39:45,316 INFO: | epoch   1 | step 10850 | batch 10850/18000 | lr 0.00001 0.00004 | loss 0.2422 | s/batch 0.51\n",
      "2020-08-17 01:40:09,601 INFO: | epoch   1 | step 10900 | batch 10900/18000 | lr 0.00001 0.00004 | loss 0.1891 | s/batch 0.49\n",
      "2020-08-17 01:40:31,353 INFO: | epoch   1 | step 10950 | batch 10950/18000 | lr 0.00001 0.00004 | loss 0.2534 | s/batch 0.43\n",
      "2020-08-17 01:40:55,683 INFO: | epoch   1 | step 11000 | batch 11000/18000 | lr 0.00001 0.00004 | loss 0.1734 | s/batch 0.49\n",
      "2020-08-17 01:41:17,847 INFO: | epoch   1 | step 11050 | batch 11050/18000 | lr 0.00001 0.00004 | loss 0.2406 | s/batch 0.44\n",
      "2020-08-17 01:41:39,771 INFO: | epoch   1 | step 11100 | batch 11100/18000 | lr 0.00001 0.00004 | loss 0.1683 | s/batch 0.44\n",
      "2020-08-17 01:42:03,407 INFO: | epoch   1 | step 11150 | batch 11150/18000 | lr 0.00001 0.00004 | loss 0.2025 | s/batch 0.47\n",
      "2020-08-17 01:42:26,285 INFO: | epoch   1 | step 11200 | batch 11200/18000 | lr 0.00001 0.00004 | loss 0.2074 | s/batch 0.46\n",
      "2020-08-17 01:42:47,174 INFO: | epoch   1 | step 11250 | batch 11250/18000 | lr 0.00001 0.00004 | loss 0.1732 | s/batch 0.42\n",
      "2020-08-17 01:43:12,143 INFO: | epoch   1 | step 11300 | batch 11300/18000 | lr 0.00001 0.00004 | loss 0.2871 | s/batch 0.50\n",
      "2020-08-17 01:43:35,667 INFO: | epoch   1 | step 11350 | batch 11350/18000 | lr 0.00001 0.00004 | loss 0.2347 | s/batch 0.47\n",
      "2020-08-17 01:43:58,764 INFO: | epoch   1 | step 11400 | batch 11400/18000 | lr 0.00001 0.00004 | loss 0.1889 | s/batch 0.46\n",
      "2020-08-17 01:44:22,011 INFO: | epoch   1 | step 11450 | batch 11450/18000 | lr 0.00001 0.00004 | loss 0.1994 | s/batch 0.46\n",
      "2020-08-17 01:44:47,397 INFO: | epoch   1 | step 11500 | batch 11500/18000 | lr 0.00001 0.00004 | loss 0.2510 | s/batch 0.51\n",
      "2020-08-17 01:45:12,060 INFO: | epoch   1 | step 11550 | batch 11550/18000 | lr 0.00001 0.00004 | loss 0.2177 | s/batch 0.49\n",
      "2020-08-17 01:45:35,316 INFO: | epoch   1 | step 11600 | batch 11600/18000 | lr 0.00001 0.00004 | loss 0.2058 | s/batch 0.47\n",
      "2020-08-17 01:46:01,332 INFO: | epoch   1 | step 11650 | batch 11650/18000 | lr 0.00001 0.00004 | loss 0.1548 | s/batch 0.52\n",
      "2020-08-17 01:46:26,242 INFO: | epoch   1 | step 11700 | batch 11700/18000 | lr 0.00001 0.00004 | loss 0.2317 | s/batch 0.50\n",
      "2020-08-17 01:46:50,139 INFO: | epoch   1 | step 11750 | batch 11750/18000 | lr 0.00001 0.00004 | loss 0.1819 | s/batch 0.48\n",
      "2020-08-17 01:47:12,090 INFO: | epoch   1 | step 11800 | batch 11800/18000 | lr 0.00001 0.00004 | loss 0.2350 | s/batch 0.44\n",
      "2020-08-17 01:47:36,375 INFO: | epoch   1 | step 11850 | batch 11850/18000 | lr 0.00001 0.00004 | loss 0.1730 | s/batch 0.49\n",
      "2020-08-17 01:48:03,145 INFO: | epoch   1 | step 11900 | batch 11900/18000 | lr 0.00001 0.00004 | loss 0.2552 | s/batch 0.54\n",
      "2020-08-17 01:48:26,892 INFO: | epoch   1 | step 11950 | batch 11950/18000 | lr 0.00001 0.00004 | loss 0.2358 | s/batch 0.47\n",
      "2020-08-17 01:48:50,265 INFO: | epoch   1 | step 12000 | batch 12000/18000 | lr 0.00001 0.00004 | loss 0.1877 | s/batch 0.47\n",
      "2020-08-17 01:49:16,695 INFO: | epoch   1 | step 12050 | batch 12050/18000 | lr 0.00001 0.00004 | loss 0.2747 | s/batch 0.53\n",
      "2020-08-17 01:49:37,984 INFO: | epoch   1 | step 12100 | batch 12100/18000 | lr 0.00001 0.00004 | loss 0.2249 | s/batch 0.43\n",
      "2020-08-17 01:50:02,471 INFO: | epoch   1 | step 12150 | batch 12150/18000 | lr 0.00001 0.00004 | loss 0.2319 | s/batch 0.49\n",
      "2020-08-17 01:50:25,721 INFO: | epoch   1 | step 12200 | batch 12200/18000 | lr 0.00001 0.00004 | loss 0.1799 | s/batch 0.46\n",
      "2020-08-17 01:50:48,392 INFO: | epoch   1 | step 12250 | batch 12250/18000 | lr 0.00001 0.00004 | loss 0.2376 | s/batch 0.45\n",
      "2020-08-17 01:51:11,581 INFO: | epoch   1 | step 12300 | batch 12300/18000 | lr 0.00001 0.00004 | loss 0.2168 | s/batch 0.46\n",
      "2020-08-17 01:51:32,776 INFO: | epoch   1 | step 12350 | batch 12350/18000 | lr 0.00001 0.00004 | loss 0.2260 | s/batch 0.42\n",
      "2020-08-17 01:51:55,217 INFO: | epoch   1 | step 12400 | batch 12400/18000 | lr 0.00001 0.00004 | loss 0.2344 | s/batch 0.45\n",
      "2020-08-17 01:52:18,728 INFO: | epoch   1 | step 12450 | batch 12450/18000 | lr 0.00001 0.00004 | loss 0.1729 | s/batch 0.47\n",
      "2020-08-17 01:52:41,273 INFO: | epoch   1 | step 12500 | batch 12500/18000 | lr 0.00001 0.00004 | loss 0.2240 | s/batch 0.45\n",
      "2020-08-17 01:53:04,134 INFO: | epoch   1 | step 12550 | batch 12550/18000 | lr 0.00001 0.00004 | loss 0.1459 | s/batch 0.46\n",
      "2020-08-17 01:53:28,268 INFO: | epoch   1 | step 12600 | batch 12600/18000 | lr 0.00001 0.00004 | loss 0.1998 | s/batch 0.48\n",
      "2020-08-17 01:53:50,879 INFO: | epoch   1 | step 12650 | batch 12650/18000 | lr 0.00001 0.00004 | loss 0.2277 | s/batch 0.45\n",
      "2020-08-17 01:54:15,258 INFO: | epoch   1 | step 12700 | batch 12700/18000 | lr 0.00001 0.00004 | loss 0.2580 | s/batch 0.49\n",
      "2020-08-17 01:54:38,081 INFO: | epoch   1 | step 12750 | batch 12750/18000 | lr 0.00001 0.00004 | loss 0.1666 | s/batch 0.46\n",
      "2020-08-17 01:55:03,518 INFO: | epoch   1 | step 12800 | batch 12800/18000 | lr 0.00001 0.00004 | loss 0.2424 | s/batch 0.51\n",
      "2020-08-17 01:55:25,263 INFO: | epoch   1 | step 12850 | batch 12850/18000 | lr 0.00001 0.00004 | loss 0.2194 | s/batch 0.43\n",
      "2020-08-17 01:55:49,700 INFO: | epoch   1 | step 12900 | batch 12900/18000 | lr 0.00001 0.00004 | loss 0.2735 | s/batch 0.49\n",
      "2020-08-17 01:56:12,042 INFO: | epoch   1 | step 12950 | batch 12950/18000 | lr 0.00001 0.00004 | loss 0.2594 | s/batch 0.45\n",
      "2020-08-17 01:56:36,256 INFO: | epoch   1 | step 13000 | batch 13000/18000 | lr 0.00000 0.00004 | loss 0.2609 | s/batch 0.48\n",
      "2020-08-17 01:56:57,991 INFO: | epoch   1 | step 13050 | batch 13050/18000 | lr 0.00000 0.00004 | loss 0.2512 | s/batch 0.43\n",
      "2020-08-17 01:57:21,162 INFO: | epoch   1 | step 13100 | batch 13100/18000 | lr 0.00000 0.00004 | loss 0.2589 | s/batch 0.46\n",
      "2020-08-17 01:57:47,206 INFO: | epoch   1 | step 13150 | batch 13150/18000 | lr 0.00000 0.00004 | loss 0.2084 | s/batch 0.52\n",
      "2020-08-17 01:58:08,761 INFO: | epoch   1 | step 13200 | batch 13200/18000 | lr 0.00000 0.00004 | loss 0.1440 | s/batch 0.43\n",
      "2020-08-17 01:58:30,464 INFO: | epoch   1 | step 13250 | batch 13250/18000 | lr 0.00000 0.00004 | loss 0.1124 | s/batch 0.43\n",
      "2020-08-17 01:58:53,282 INFO: | epoch   1 | step 13300 | batch 13300/18000 | lr 0.00000 0.00004 | loss 0.2421 | s/batch 0.46\n",
      "2020-08-17 01:59:14,028 INFO: | epoch   1 | step 13350 | batch 13350/18000 | lr 0.00000 0.00004 | loss 0.1926 | s/batch 0.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-17 01:59:37,139 INFO: | epoch   1 | step 13400 | batch 13400/18000 | lr 0.00000 0.00004 | loss 0.1803 | s/batch 0.46\n",
      "2020-08-17 02:00:02,112 INFO: | epoch   1 | step 13450 | batch 13450/18000 | lr 0.00000 0.00004 | loss 0.2223 | s/batch 0.50\n",
      "2020-08-17 02:00:25,864 INFO: | epoch   1 | step 13500 | batch 13500/18000 | lr 0.00000 0.00004 | loss 0.2231 | s/batch 0.47\n",
      "2020-08-17 02:00:49,060 INFO: | epoch   1 | step 13550 | batch 13550/18000 | lr 0.00000 0.00004 | loss 0.2474 | s/batch 0.46\n",
      "2020-08-17 02:01:11,991 INFO: | epoch   1 | step 13600 | batch 13600/18000 | lr 0.00000 0.00004 | loss 0.2172 | s/batch 0.46\n",
      "2020-08-17 02:01:33,442 INFO: | epoch   1 | step 13650 | batch 13650/18000 | lr 0.00000 0.00004 | loss 0.1821 | s/batch 0.43\n",
      "2020-08-17 02:01:57,236 INFO: | epoch   1 | step 13700 | batch 13700/18000 | lr 0.00000 0.00004 | loss 0.2518 | s/batch 0.48\n",
      "2020-08-17 02:02:19,566 INFO: | epoch   1 | step 13750 | batch 13750/18000 | lr 0.00000 0.00004 | loss 0.2038 | s/batch 0.45\n",
      "2020-08-17 02:02:43,066 INFO: | epoch   1 | step 13800 | batch 13800/18000 | lr 0.00000 0.00004 | loss 0.1791 | s/batch 0.47\n",
      "2020-08-17 02:03:07,973 INFO: | epoch   1 | step 13850 | batch 13850/18000 | lr 0.00000 0.00004 | loss 0.2552 | s/batch 0.50\n",
      "2020-08-17 02:03:33,451 INFO: | epoch   1 | step 13900 | batch 13900/18000 | lr 0.00000 0.00004 | loss 0.2057 | s/batch 0.51\n",
      "2020-08-17 02:03:59,833 INFO: | epoch   1 | step 13950 | batch 13950/18000 | lr 0.00000 0.00004 | loss 0.2448 | s/batch 0.53\n",
      "2020-08-17 02:04:22,234 INFO: | epoch   1 | step 14000 | batch 14000/18000 | lr 0.00000 0.00004 | loss 0.2249 | s/batch 0.45\n",
      "2020-08-17 02:04:44,778 INFO: | epoch   1 | step 14050 | batch 14050/18000 | lr 0.00000 0.00004 | loss 0.1719 | s/batch 0.45\n",
      "2020-08-17 02:05:05,673 INFO: | epoch   1 | step 14100 | batch 14100/18000 | lr 0.00000 0.00004 | loss 0.2650 | s/batch 0.42\n",
      "2020-08-17 02:05:30,205 INFO: | epoch   1 | step 14150 | batch 14150/18000 | lr 0.00000 0.00004 | loss 0.2196 | s/batch 0.49\n",
      "2020-08-17 02:05:53,637 INFO: | epoch   1 | step 14200 | batch 14200/18000 | lr 0.00000 0.00004 | loss 0.1717 | s/batch 0.47\n",
      "2020-08-17 02:06:15,599 INFO: | epoch   1 | step 14250 | batch 14250/18000 | lr 0.00000 0.00004 | loss 0.1787 | s/batch 0.44\n",
      "2020-08-17 02:06:38,023 INFO: | epoch   1 | step 14300 | batch 14300/18000 | lr 0.00000 0.00004 | loss 0.1860 | s/batch 0.45\n",
      "2020-08-17 02:07:00,955 INFO: | epoch   1 | step 14350 | batch 14350/18000 | lr 0.00000 0.00004 | loss 0.2238 | s/batch 0.46\n",
      "2020-08-17 02:07:20,912 INFO: | epoch   1 | step 14400 | batch 14400/18000 | lr 0.00000 0.00004 | loss 0.2351 | s/batch 0.40\n",
      "2020-08-17 02:07:44,865 INFO: | epoch   1 | step 14450 | batch 14450/18000 | lr 0.00000 0.00004 | loss 0.1874 | s/batch 0.48\n",
      "2020-08-17 02:08:07,268 INFO: | epoch   1 | step 14500 | batch 14500/18000 | lr 0.00000 0.00004 | loss 0.1744 | s/batch 0.45\n",
      "2020-08-17 02:08:30,958 INFO: | epoch   1 | step 14550 | batch 14550/18000 | lr 0.00000 0.00004 | loss 0.2211 | s/batch 0.47\n",
      "2020-08-17 02:08:55,029 INFO: | epoch   1 | step 14600 | batch 14600/18000 | lr 0.00000 0.00004 | loss 0.1807 | s/batch 0.48\n",
      "2020-08-17 02:09:14,763 INFO: | epoch   1 | step 14650 | batch 14650/18000 | lr 0.00000 0.00004 | loss 0.1429 | s/batch 0.39\n",
      "2020-08-17 02:09:39,095 INFO: | epoch   1 | step 14700 | batch 14700/18000 | lr 0.00000 0.00004 | loss 0.1864 | s/batch 0.49\n",
      "2020-08-17 02:10:00,459 INFO: | epoch   1 | step 14750 | batch 14750/18000 | lr 0.00000 0.00004 | loss 0.1804 | s/batch 0.43\n",
      "2020-08-17 02:10:22,397 INFO: | epoch   1 | step 14800 | batch 14800/18000 | lr 0.00000 0.00004 | loss 0.3005 | s/batch 0.44\n",
      "2020-08-17 02:10:44,874 INFO: | epoch   1 | step 14850 | batch 14850/18000 | lr 0.00000 0.00004 | loss 0.2526 | s/batch 0.45\n",
      "2020-08-17 02:11:06,839 INFO: | epoch   1 | step 14900 | batch 14900/18000 | lr 0.00000 0.00004 | loss 0.1971 | s/batch 0.44\n",
      "2020-08-17 02:11:31,443 INFO: | epoch   1 | step 14950 | batch 14950/18000 | lr 0.00000 0.00004 | loss 0.2516 | s/batch 0.49\n",
      "2020-08-17 02:11:55,169 INFO: | epoch   1 | step 15000 | batch 15000/18000 | lr 0.00000 0.00004 | loss 0.1840 | s/batch 0.47\n",
      "2020-08-17 02:12:17,530 INFO: | epoch   1 | step 15050 | batch 15050/18000 | lr 0.00000 0.00004 | loss 0.2272 | s/batch 0.45\n",
      "2020-08-17 02:12:38,406 INFO: | epoch   1 | step 15100 | batch 15100/18000 | lr 0.00000 0.00004 | loss 0.2174 | s/batch 0.42\n",
      "2020-08-17 02:13:00,750 INFO: | epoch   1 | step 15150 | batch 15150/18000 | lr 0.00000 0.00004 | loss 0.1594 | s/batch 0.45\n",
      "2020-08-17 02:13:27,933 INFO: | epoch   1 | step 15200 | batch 15200/18000 | lr 0.00000 0.00004 | loss 0.2044 | s/batch 0.54\n",
      "2020-08-17 02:13:50,726 INFO: | epoch   1 | step 15250 | batch 15250/18000 | lr 0.00000 0.00004 | loss 0.2005 | s/batch 0.46\n",
      "2020-08-17 02:14:13,043 INFO: | epoch   1 | step 15300 | batch 15300/18000 | lr 0.00000 0.00004 | loss 0.2036 | s/batch 0.45\n",
      "2020-08-17 02:14:35,084 INFO: | epoch   1 | step 15350 | batch 15350/18000 | lr 0.00000 0.00004 | loss 0.1913 | s/batch 0.44\n",
      "2020-08-17 02:14:55,172 INFO: | epoch   1 | step 15400 | batch 15400/18000 | lr 0.00000 0.00004 | loss 0.1831 | s/batch 0.40\n",
      "2020-08-17 02:15:19,145 INFO: | epoch   1 | step 15450 | batch 15450/18000 | lr 0.00000 0.00004 | loss 0.1825 | s/batch 0.48\n",
      "2020-08-17 02:15:41,861 INFO: | epoch   1 | step 15500 | batch 15500/18000 | lr 0.00000 0.00004 | loss 0.1685 | s/batch 0.45\n",
      "2020-08-17 02:16:04,127 INFO: | epoch   1 | step 15550 | batch 15550/18000 | lr 0.00000 0.00004 | loss 0.1761 | s/batch 0.45\n",
      "2020-08-17 02:16:29,447 INFO: | epoch   1 | step 15600 | batch 15600/18000 | lr 0.00000 0.00004 | loss 0.1690 | s/batch 0.51\n",
      "2020-08-17 02:16:55,948 INFO: | epoch   1 | step 15650 | batch 15650/18000 | lr 0.00000 0.00004 | loss 0.1997 | s/batch 0.53\n",
      "2020-08-17 02:17:25,359 INFO: | epoch   1 | step 15700 | batch 15700/18000 | lr 0.00000 0.00004 | loss 0.2379 | s/batch 0.59\n",
      "2020-08-17 02:17:52,025 INFO: | epoch   1 | step 15750 | batch 15750/18000 | lr 0.00000 0.00004 | loss 0.2167 | s/batch 0.53\n",
      "2020-08-17 02:18:16,976 INFO: | epoch   1 | step 15800 | batch 15800/18000 | lr 0.00000 0.00004 | loss 0.1898 | s/batch 0.50\n",
      "2020-08-17 02:18:43,450 INFO: | epoch   1 | step 15850 | batch 15850/18000 | lr 0.00000 0.00004 | loss 0.2310 | s/batch 0.53\n",
      "2020-08-17 02:19:06,932 INFO: | epoch   1 | step 15900 | batch 15900/18000 | lr 0.00000 0.00004 | loss 0.1550 | s/batch 0.47\n",
      "2020-08-17 02:19:31,518 INFO: | epoch   1 | step 15950 | batch 15950/18000 | lr 0.00000 0.00004 | loss 0.2102 | s/batch 0.49\n",
      "2020-08-17 02:19:53,927 INFO: | epoch   1 | step 16000 | batch 16000/18000 | lr 0.00000 0.00004 | loss 0.1780 | s/batch 0.45\n",
      "2020-08-17 02:20:17,239 INFO: | epoch   1 | step 16050 | batch 16050/18000 | lr 0.00000 0.00004 | loss 0.2144 | s/batch 0.47\n",
      "2020-08-17 02:20:38,369 INFO: | epoch   1 | step 16100 | batch 16100/18000 | lr 0.00000 0.00004 | loss 0.1488 | s/batch 0.42\n",
      "2020-08-17 02:21:00,820 INFO: | epoch   1 | step 16150 | batch 16150/18000 | lr 0.00000 0.00004 | loss 0.1995 | s/batch 0.45\n",
      "2020-08-17 02:21:20,650 INFO: | epoch   1 | step 16200 | batch 16200/18000 | lr 0.00000 0.00003 | loss 0.2030 | s/batch 0.40\n",
      "2020-08-17 02:21:44,646 INFO: | epoch   1 | step 16250 | batch 16250/18000 | lr 0.00000 0.00003 | loss 0.1565 | s/batch 0.48\n",
      "2020-08-17 02:22:11,300 INFO: | epoch   1 | step 16300 | batch 16300/18000 | lr 0.00000 0.00003 | loss 0.1988 | s/batch 0.53\n",
      "2020-08-17 02:22:31,870 INFO: | epoch   1 | step 16350 | batch 16350/18000 | lr 0.00000 0.00003 | loss 0.1796 | s/batch 0.41\n",
      "2020-08-17 02:22:56,681 INFO: | epoch   1 | step 16400 | batch 16400/18000 | lr 0.00000 0.00003 | loss 0.2256 | s/batch 0.50\n",
      "2020-08-17 02:23:21,218 INFO: | epoch   1 | step 16450 | batch 16450/18000 | lr 0.00000 0.00003 | loss 0.1865 | s/batch 0.49\n",
      "2020-08-17 02:23:44,089 INFO: | epoch   1 | step 16500 | batch 16500/18000 | lr 0.00000 0.00003 | loss 0.1827 | s/batch 0.46\n",
      "2020-08-17 02:24:06,761 INFO: | epoch   1 | step 16550 | batch 16550/18000 | lr 0.00000 0.00003 | loss 0.1764 | s/batch 0.45\n",
      "2020-08-17 02:24:30,667 INFO: | epoch   1 | step 16600 | batch 16600/18000 | lr 0.00000 0.00003 | loss 0.2759 | s/batch 0.48\n",
      "2020-08-17 02:24:55,310 INFO: | epoch   1 | step 16650 | batch 16650/18000 | lr 0.00000 0.00003 | loss 0.1899 | s/batch 0.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-17 02:25:18,774 INFO: | epoch   1 | step 16700 | batch 16700/18000 | lr 0.00000 0.00003 | loss 0.1464 | s/batch 0.47\n",
      "2020-08-17 02:25:41,669 INFO: | epoch   1 | step 16750 | batch 16750/18000 | lr 0.00000 0.00003 | loss 0.2280 | s/batch 0.46\n",
      "2020-08-17 02:26:07,351 INFO: | epoch   1 | step 16800 | batch 16800/18000 | lr 0.00000 0.00003 | loss 0.2047 | s/batch 0.51\n",
      "2020-08-17 02:26:30,099 INFO: | epoch   1 | step 16850 | batch 16850/18000 | lr 0.00000 0.00003 | loss 0.1569 | s/batch 0.45\n",
      "2020-08-17 02:26:55,255 INFO: | epoch   1 | step 16900 | batch 16900/18000 | lr 0.00000 0.00003 | loss 0.2223 | s/batch 0.50\n",
      "2020-08-17 02:27:19,489 INFO: | epoch   1 | step 16950 | batch 16950/18000 | lr 0.00000 0.00003 | loss 0.1519 | s/batch 0.48\n",
      "2020-08-17 02:27:45,897 INFO: | epoch   1 | step 17000 | batch 17000/18000 | lr 0.00000 0.00003 | loss 0.2087 | s/batch 0.53\n",
      "2020-08-17 02:28:09,946 INFO: | epoch   1 | step 17050 | batch 17050/18000 | lr 0.00000 0.00003 | loss 0.2173 | s/batch 0.48\n",
      "2020-08-17 02:28:34,762 INFO: | epoch   1 | step 17100 | batch 17100/18000 | lr 0.00000 0.00003 | loss 0.1963 | s/batch 0.50\n",
      "2020-08-17 02:28:58,760 INFO: | epoch   1 | step 17150 | batch 17150/18000 | lr 0.00000 0.00003 | loss 0.2162 | s/batch 0.48\n",
      "2020-08-17 02:29:24,423 INFO: | epoch   1 | step 17200 | batch 17200/18000 | lr 0.00000 0.00003 | loss 0.1852 | s/batch 0.51\n",
      "2020-08-17 02:29:46,600 INFO: | epoch   1 | step 17250 | batch 17250/18000 | lr 0.00000 0.00003 | loss 0.1899 | s/batch 0.44\n",
      "2020-08-17 02:30:08,584 INFO: | epoch   1 | step 17300 | batch 17300/18000 | lr 0.00000 0.00003 | loss 0.1815 | s/batch 0.44\n",
      "2020-08-17 02:30:30,594 INFO: | epoch   1 | step 17350 | batch 17350/18000 | lr 0.00000 0.00003 | loss 0.1959 | s/batch 0.44\n",
      "2020-08-17 02:30:54,374 INFO: | epoch   1 | step 17400 | batch 17400/18000 | lr 0.00000 0.00003 | loss 0.1373 | s/batch 0.48\n",
      "2020-08-17 02:31:17,193 INFO: | epoch   1 | step 17450 | batch 17450/18000 | lr 0.00000 0.00003 | loss 0.1890 | s/batch 0.46\n",
      "2020-08-17 02:31:42,124 INFO: | epoch   1 | step 17500 | batch 17500/18000 | lr 0.00000 0.00003 | loss 0.2473 | s/batch 0.50\n",
      "2020-08-17 02:32:02,796 INFO: | epoch   1 | step 17550 | batch 17550/18000 | lr 0.00000 0.00003 | loss 0.1652 | s/batch 0.41\n",
      "2020-08-17 02:32:27,585 INFO: | epoch   1 | step 17600 | batch 17600/18000 | lr 0.00000 0.00003 | loss 0.1729 | s/batch 0.50\n",
      "2020-08-17 02:32:52,441 INFO: | epoch   1 | step 17650 | batch 17650/18000 | lr 0.00000 0.00003 | loss 0.2066 | s/batch 0.50\n",
      "2020-08-17 02:33:18,048 INFO: | epoch   1 | step 17700 | batch 17700/18000 | lr 0.00000 0.00003 | loss 0.2022 | s/batch 0.51\n",
      "2020-08-17 02:33:39,736 INFO: | epoch   1 | step 17750 | batch 17750/18000 | lr 0.00000 0.00003 | loss 0.2011 | s/batch 0.43\n",
      "2020-08-17 02:34:05,503 INFO: | epoch   1 | step 17800 | batch 17800/18000 | lr 0.00000 0.00003 | loss 0.1978 | s/batch 0.52\n",
      "2020-08-17 02:34:28,233 INFO: | epoch   1 | step 17850 | batch 17850/18000 | lr 0.00000 0.00003 | loss 0.2497 | s/batch 0.45\n",
      "2020-08-17 02:34:51,088 INFO: | epoch   1 | step 17900 | batch 17900/18000 | lr 0.00000 0.00003 | loss 0.1372 | s/batch 0.46\n",
      "2020-08-17 02:35:14,147 INFO: | epoch   1 | step 17950 | batch 17950/18000 | lr 0.00000 0.00003 | loss 0.1917 | s/batch 0.46\n",
      "2020-08-17 02:35:38,505 INFO: | epoch   1 | step 18000 | batch 18000/18000 | lr 0.00000 0.00003 | loss 0.2740 | s/batch 0.49\n",
      "2020-08-17 02:35:38,729 INFO: | epoch   1 | score (90.72, 89.04, 89.85) | f1 89.85 | loss 0.2645 | time 8452.51\n",
      "2020-08-17 02:35:39,068 INFO: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9271    0.9301    0.9286     35027\n",
      "          股票     0.9266    0.9394    0.9330     33251\n",
      "          体育     0.9788    0.9823    0.9805     28283\n",
      "          娱乐     0.9366    0.9523    0.9444     19920\n",
      "          时政     0.8780    0.8942    0.8860     13515\n",
      "          社会     0.8683    0.8604    0.8643     11009\n",
      "          教育     0.9271    0.9236    0.9253      8987\n",
      "          财经     0.8646    0.8073    0.8350      7957\n",
      "          家居     0.9028    0.8890    0.8958      7063\n",
      "          游戏     0.9002    0.8781    0.8890      5291\n",
      "          房产     0.9215    0.9015    0.9114      4428\n",
      "          时尚     0.8750    0.8520    0.8634      2818\n",
      "          彩票     0.9220    0.8585    0.8891      1639\n",
      "          星座     0.8720    0.7968    0.8327       812\n",
      "\n",
      "    accuracy                         0.9234    180000\n",
      "   macro avg     0.9072    0.8904    0.8985    180000\n",
      "weighted avg     0.9232    0.9234    0.9232    180000\n",
      "\n",
      "2020-08-17 02:46:07,361 INFO: | epoch   1 | dev | score (94.69, 94.59, 94.63) | f1 94.63 | time 628.29\n",
      "2020-08-17 02:46:07,403 INFO: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9437    0.9525    0.9481      3891\n",
      "          股票     0.9624    0.9499    0.9561      3694\n",
      "          体育     0.9886    0.9901    0.9893      3142\n",
      "          娱乐     0.9682    0.9629    0.9656      2213\n",
      "          时政     0.9004    0.9334    0.9166      1501\n",
      "          社会     0.9331    0.9125    0.9227      1223\n",
      "          教育     0.9589    0.9579    0.9584       998\n",
      "          财经     0.8710    0.9016    0.8860       884\n",
      "          家居     0.9435    0.9375    0.9405       784\n",
      "          游戏     0.9550    0.9046    0.9291       587\n",
      "          房产     0.9836    0.9736    0.9785       492\n",
      "          时尚     0.9335    0.9425    0.9380       313\n",
      "          彩票     0.9551    0.9341    0.9444       182\n",
      "          星座     0.9596    0.9896    0.9744        96\n",
      "\n",
      "    accuracy                         0.9516     20000\n",
      "   macro avg     0.9469    0.9459    0.9463     20000\n",
      "weighted avg     0.9519    0.9516    0.9517     20000\n",
      "\n",
      "2020-08-17 02:46:07,406 INFO: Exceed history dev = 0.00, current dev = 94.63\n",
      "2020-08-17 02:46:33,892 INFO: | epoch   2 | step 18050 | batch  50/18000 | lr 0.00000 0.00003 | loss 0.1167 | s/batch 0.53\n",
      "2020-08-17 02:46:59,163 INFO: | epoch   2 | step 18100 | batch 100/18000 | lr 0.00000 0.00003 | loss 0.1912 | s/batch 0.51\n",
      "2020-08-17 02:47:22,271 INFO: | epoch   2 | step 18150 | batch 150/18000 | lr 0.00000 0.00003 | loss 0.1630 | s/batch 0.46\n",
      "2020-08-17 02:47:46,988 INFO: | epoch   2 | step 18200 | batch 200/18000 | lr 0.00000 0.00003 | loss 0.2211 | s/batch 0.49\n",
      "2020-08-17 02:48:10,117 INFO: | epoch   2 | step 18250 | batch 250/18000 | lr 0.00000 0.00003 | loss 0.1616 | s/batch 0.46\n",
      "2020-08-17 02:48:34,804 INFO: | epoch   2 | step 18300 | batch 300/18000 | lr 0.00000 0.00003 | loss 0.1208 | s/batch 0.49\n",
      "2020-08-17 02:48:59,906 INFO: | epoch   2 | step 18350 | batch 350/18000 | lr 0.00000 0.00003 | loss 0.1517 | s/batch 0.50\n",
      "2020-08-17 02:49:25,766 INFO: | epoch   2 | step 18400 | batch 400/18000 | lr 0.00000 0.00003 | loss 0.1645 | s/batch 0.52\n",
      "2020-08-17 02:49:48,736 INFO: | epoch   2 | step 18450 | batch 450/18000 | lr 0.00000 0.00003 | loss 0.1907 | s/batch 0.46\n",
      "2020-08-17 02:50:12,637 INFO: | epoch   2 | step 18500 | batch 500/18000 | lr 0.00000 0.00003 | loss 0.1426 | s/batch 0.48\n",
      "2020-08-17 02:50:34,116 INFO: | epoch   2 | step 18550 | batch 550/18000 | lr 0.00000 0.00003 | loss 0.1489 | s/batch 0.43\n",
      "2020-08-17 02:50:57,660 INFO: | epoch   2 | step 18600 | batch 600/18000 | lr 0.00000 0.00003 | loss 0.1727 | s/batch 0.47\n",
      "2020-08-17 02:51:17,543 INFO: | epoch   2 | step 18650 | batch 650/18000 | lr 0.00000 0.00003 | loss 0.1414 | s/batch 0.40\n",
      "2020-08-17 02:51:42,206 INFO: | epoch   2 | step 18700 | batch 700/18000 | lr 0.00000 0.00003 | loss 0.1127 | s/batch 0.49\n",
      "2020-08-17 02:52:02,912 INFO: | epoch   2 | step 18750 | batch 750/18000 | lr 0.00000 0.00003 | loss 0.1455 | s/batch 0.41\n",
      "2020-08-17 02:52:24,046 INFO: | epoch   2 | step 18800 | batch 800/18000 | lr 0.00000 0.00003 | loss 0.1822 | s/batch 0.42\n",
      "2020-08-17 02:52:46,259 INFO: | epoch   2 | step 18850 | batch 850/18000 | lr 0.00000 0.00003 | loss 0.1385 | s/batch 0.44\n",
      "2020-08-17 02:53:12,097 INFO: | epoch   2 | step 18900 | batch 900/18000 | lr 0.00000 0.00003 | loss 0.1429 | s/batch 0.52\n",
      "2020-08-17 02:53:35,174 INFO: | epoch   2 | step 18950 | batch 950/18000 | lr 0.00000 0.00003 | loss 0.2217 | s/batch 0.46\n",
      "2020-08-17 02:53:58,227 INFO: | epoch   2 | step 19000 | batch 1000/18000 | lr 0.00000 0.00003 | loss 0.2493 | s/batch 0.46\n",
      "2020-08-17 02:54:23,328 INFO: | epoch   2 | step 19050 | batch 1050/18000 | lr 0.00000 0.00003 | loss 0.1716 | s/batch 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-17 02:54:46,204 INFO: | epoch   2 | step 19100 | batch 1100/18000 | lr 0.00000 0.00003 | loss 0.2087 | s/batch 0.46\n",
      "2020-08-17 02:55:07,835 INFO: | epoch   2 | step 19150 | batch 1150/18000 | lr 0.00000 0.00003 | loss 0.1364 | s/batch 0.43\n",
      "2020-08-17 02:55:30,506 INFO: | epoch   2 | step 19200 | batch 1200/18000 | lr 0.00000 0.00003 | loss 0.1769 | s/batch 0.45\n",
      "2020-08-17 02:55:51,757 INFO: | epoch   2 | step 19250 | batch 1250/18000 | lr 0.00000 0.00003 | loss 0.1839 | s/batch 0.42\n",
      "2020-08-17 02:56:15,329 INFO: | epoch   2 | step 19300 | batch 1300/18000 | lr 0.00000 0.00003 | loss 0.1490 | s/batch 0.47\n",
      "2020-08-17 02:56:36,307 INFO: | epoch   2 | step 19350 | batch 1350/18000 | lr 0.00000 0.00003 | loss 0.1664 | s/batch 0.42\n",
      "2020-08-17 02:56:57,793 INFO: | epoch   2 | step 19400 | batch 1400/18000 | lr 0.00000 0.00003 | loss 0.1815 | s/batch 0.43\n",
      "2020-08-17 02:57:22,128 INFO: | epoch   2 | step 19450 | batch 1450/18000 | lr 0.00000 0.00003 | loss 0.1288 | s/batch 0.49\n",
      "2020-08-17 02:57:45,671 INFO: | epoch   2 | step 19500 | batch 1500/18000 | lr 0.00000 0.00003 | loss 0.2461 | s/batch 0.47\n",
      "2020-08-17 02:58:09,653 INFO: | epoch   2 | step 19550 | batch 1550/18000 | lr 0.00000 0.00003 | loss 0.1686 | s/batch 0.48\n",
      "2020-08-17 02:58:36,160 INFO: | epoch   2 | step 19600 | batch 1600/18000 | lr 0.00000 0.00003 | loss 0.1833 | s/batch 0.53\n",
      "2020-08-17 02:58:59,237 INFO: | epoch   2 | step 19650 | batch 1650/18000 | lr 0.00000 0.00003 | loss 0.1453 | s/batch 0.46\n",
      "2020-08-17 02:59:23,372 INFO: | epoch   2 | step 19700 | batch 1700/18000 | lr 0.00000 0.00003 | loss 0.1712 | s/batch 0.48\n",
      "2020-08-17 02:59:48,080 INFO: | epoch   2 | step 19750 | batch 1750/18000 | lr 0.00000 0.00003 | loss 0.1271 | s/batch 0.49\n",
      "2020-08-17 03:00:13,856 INFO: | epoch   2 | step 19800 | batch 1800/18000 | lr 0.00000 0.00003 | loss 0.1924 | s/batch 0.52\n",
      "2020-08-17 03:00:36,092 INFO: | epoch   2 | step 19850 | batch 1850/18000 | lr 0.00000 0.00003 | loss 0.1528 | s/batch 0.44\n",
      "2020-08-17 03:00:59,211 INFO: | epoch   2 | step 19900 | batch 1900/18000 | lr 0.00000 0.00003 | loss 0.1583 | s/batch 0.46\n",
      "2020-08-17 03:01:23,560 INFO: | epoch   2 | step 19950 | batch 1950/18000 | lr 0.00000 0.00003 | loss 0.1840 | s/batch 0.49\n",
      "2020-08-17 03:01:48,992 INFO: | epoch   2 | step 20000 | batch 2000/18000 | lr 0.00000 0.00003 | loss 0.1281 | s/batch 0.51\n",
      "2020-08-17 03:02:12,744 INFO: | epoch   2 | step 20050 | batch 2050/18000 | lr 0.00000 0.00003 | loss 0.1955 | s/batch 0.47\n",
      "2020-08-17 03:02:37,737 INFO: | epoch   2 | step 20100 | batch 2100/18000 | lr 0.00000 0.00003 | loss 0.1330 | s/batch 0.50\n",
      "2020-08-17 03:03:02,423 INFO: | epoch   2 | step 20150 | batch 2150/18000 | lr 0.00000 0.00003 | loss 0.2318 | s/batch 0.49\n",
      "2020-08-17 03:03:25,632 INFO: | epoch   2 | step 20200 | batch 2200/18000 | lr 0.00000 0.00003 | loss 0.1722 | s/batch 0.46\n",
      "2020-08-17 03:03:48,689 INFO: | epoch   2 | step 20250 | batch 2250/18000 | lr 0.00000 0.00003 | loss 0.1042 | s/batch 0.46\n",
      "2020-08-17 03:04:10,912 INFO: | epoch   2 | step 20300 | batch 2300/18000 | lr 0.00000 0.00003 | loss 0.1574 | s/batch 0.44\n",
      "2020-08-17 03:04:31,741 INFO: | epoch   2 | step 20350 | batch 2350/18000 | lr 0.00000 0.00003 | loss 0.1201 | s/batch 0.42\n",
      "2020-08-17 03:04:54,272 INFO: | epoch   2 | step 20400 | batch 2400/18000 | lr 0.00000 0.00003 | loss 0.1297 | s/batch 0.45\n",
      "2020-08-17 03:05:17,085 INFO: | epoch   2 | step 20450 | batch 2450/18000 | lr 0.00000 0.00003 | loss 0.2232 | s/batch 0.46\n",
      "2020-08-17 03:05:39,572 INFO: | epoch   2 | step 20500 | batch 2500/18000 | lr 0.00000 0.00003 | loss 0.1321 | s/batch 0.45\n",
      "2020-08-17 03:06:01,870 INFO: | epoch   2 | step 20550 | batch 2550/18000 | lr 0.00000 0.00003 | loss 0.1353 | s/batch 0.45\n",
      "2020-08-17 03:06:24,446 INFO: | epoch   2 | step 20600 | batch 2600/18000 | lr 0.00000 0.00003 | loss 0.1339 | s/batch 0.45\n",
      "2020-08-17 03:06:51,460 INFO: | epoch   2 | step 20650 | batch 2650/18000 | lr 0.00000 0.00003 | loss 0.2323 | s/batch 0.54\n",
      "2020-08-17 03:07:18,069 INFO: | epoch   2 | step 20700 | batch 2700/18000 | lr 0.00000 0.00003 | loss 0.1406 | s/batch 0.53\n",
      "2020-08-17 03:07:43,192 INFO: | epoch   2 | step 20750 | batch 2750/18000 | lr 0.00000 0.00003 | loss 0.1698 | s/batch 0.50\n",
      "2020-08-17 03:08:07,400 INFO: | epoch   2 | step 20800 | batch 2800/18000 | lr 0.00000 0.00003 | loss 0.1313 | s/batch 0.48\n",
      "2020-08-17 03:08:31,524 INFO: | epoch   2 | step 20850 | batch 2850/18000 | lr 0.00000 0.00003 | loss 0.1995 | s/batch 0.48\n",
      "2020-08-17 03:08:54,705 INFO: | epoch   2 | step 20900 | batch 2900/18000 | lr 0.00000 0.00003 | loss 0.1249 | s/batch 0.46\n",
      "2020-08-17 03:09:17,803 INFO: | epoch   2 | step 20950 | batch 2950/18000 | lr 0.00000 0.00003 | loss 0.1659 | s/batch 0.46\n",
      "2020-08-17 03:09:42,441 INFO: | epoch   2 | step 21000 | batch 3000/18000 | lr 0.00000 0.00003 | loss 0.1503 | s/batch 0.49\n",
      "2020-08-17 03:10:05,653 INFO: | epoch   2 | step 21050 | batch 3050/18000 | lr 0.00000 0.00003 | loss 0.1703 | s/batch 0.46\n",
      "2020-08-17 03:10:27,461 INFO: | epoch   2 | step 21100 | batch 3100/18000 | lr 0.00000 0.00003 | loss 0.1981 | s/batch 0.44\n",
      "2020-08-17 03:10:50,563 INFO: | epoch   2 | step 21150 | batch 3150/18000 | lr 0.00000 0.00003 | loss 0.1578 | s/batch 0.46\n",
      "2020-08-17 03:11:15,696 INFO: | epoch   2 | step 21200 | batch 3200/18000 | lr 0.00000 0.00003 | loss 0.1329 | s/batch 0.50\n",
      "2020-08-17 03:11:41,069 INFO: | epoch   2 | step 21250 | batch 3250/18000 | lr 0.00000 0.00003 | loss 0.2078 | s/batch 0.51\n",
      "2020-08-17 03:12:04,924 INFO: | epoch   2 | step 21300 | batch 3300/18000 | lr 0.00000 0.00003 | loss 0.1182 | s/batch 0.48\n",
      "2020-08-17 03:12:27,342 INFO: | epoch   2 | step 21350 | batch 3350/18000 | lr 0.00000 0.00003 | loss 0.1242 | s/batch 0.45\n",
      "2020-08-17 03:12:52,747 INFO: | epoch   2 | step 21400 | batch 3400/18000 | lr 0.00000 0.00003 | loss 0.1619 | s/batch 0.51\n",
      "2020-08-17 03:13:16,940 INFO: | epoch   2 | step 21450 | batch 3450/18000 | lr 0.00000 0.00003 | loss 0.1304 | s/batch 0.48\n",
      "2020-08-17 03:13:41,884 INFO: | epoch   2 | step 21500 | batch 3500/18000 | lr 0.00000 0.00003 | loss 0.2436 | s/batch 0.50\n",
      "2020-08-17 03:14:06,882 INFO: | epoch   2 | step 21550 | batch 3550/18000 | lr 0.00000 0.00003 | loss 0.1662 | s/batch 0.50\n",
      "2020-08-17 03:14:27,372 INFO: | epoch   2 | step 21600 | batch 3600/18000 | lr 0.00000 0.00003 | loss 0.1148 | s/batch 0.41\n",
      "2020-08-17 03:14:49,558 INFO: | epoch   2 | step 21650 | batch 3650/18000 | lr 0.00000 0.00003 | loss 0.1271 | s/batch 0.44\n",
      "2020-08-17 03:15:14,351 INFO: | epoch   2 | step 21700 | batch 3700/18000 | lr 0.00000 0.00003 | loss 0.1251 | s/batch 0.50\n",
      "2020-08-17 03:15:38,872 INFO: | epoch   2 | step 21750 | batch 3750/18000 | lr 0.00000 0.00003 | loss 0.2063 | s/batch 0.49\n",
      "2020-08-17 03:15:59,969 INFO: | epoch   2 | step 21800 | batch 3800/18000 | lr 0.00000 0.00003 | loss 0.1765 | s/batch 0.42\n",
      "2020-08-17 03:16:25,428 INFO: | epoch   2 | step 21850 | batch 3850/18000 | lr 0.00000 0.00003 | loss 0.1824 | s/batch 0.51\n",
      "2020-08-17 03:16:47,462 INFO: | epoch   2 | step 21900 | batch 3900/18000 | lr 0.00000 0.00003 | loss 0.0778 | s/batch 0.44\n",
      "2020-08-17 03:17:12,656 INFO: | epoch   2 | step 21950 | batch 3950/18000 | lr 0.00000 0.00003 | loss 0.1540 | s/batch 0.50\n",
      "2020-08-17 03:17:36,880 INFO: | epoch   2 | step 22000 | batch 4000/18000 | lr 0.00000 0.00003 | loss 0.1295 | s/batch 0.48\n",
      "2020-08-17 03:18:00,600 INFO: | epoch   2 | step 22050 | batch 4050/18000 | lr 0.00000 0.00003 | loss 0.1512 | s/batch 0.47\n",
      "2020-08-17 03:18:23,615 INFO: | epoch   2 | step 22100 | batch 4100/18000 | lr 0.00000 0.00003 | loss 0.1581 | s/batch 0.46\n",
      "2020-08-17 03:18:47,192 INFO: | epoch   2 | step 22150 | batch 4150/18000 | lr 0.00000 0.00003 | loss 0.1144 | s/batch 0.47\n",
      "2020-08-17 03:19:09,789 INFO: | epoch   2 | step 22200 | batch 4200/18000 | lr 0.00000 0.00003 | loss 0.1535 | s/batch 0.45\n",
      "2020-08-17 03:19:33,019 INFO: | epoch   2 | step 22250 | batch 4250/18000 | lr 0.00000 0.00003 | loss 0.1310 | s/batch 0.46\n",
      "2020-08-17 03:19:54,536 INFO: | epoch   2 | step 22300 | batch 4300/18000 | lr 0.00000 0.00003 | loss 0.1799 | s/batch 0.43\n",
      "2020-08-17 03:20:19,012 INFO: | epoch   2 | step 22350 | batch 4350/18000 | lr 0.00000 0.00003 | loss 0.1880 | s/batch 0.49\n",
      "2020-08-17 03:20:42,445 INFO: | epoch   2 | step 22400 | batch 4400/18000 | lr 0.00000 0.00003 | loss 0.1557 | s/batch 0.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-17 03:21:07,243 INFO: | epoch   2 | step 22450 | batch 4450/18000 | lr 0.00000 0.00003 | loss 0.1606 | s/batch 0.50\n",
      "2020-08-17 03:21:29,811 INFO: | epoch   2 | step 22500 | batch 4500/18000 | lr 0.00000 0.00003 | loss 0.1504 | s/batch 0.45\n",
      "2020-08-17 03:21:54,083 INFO: | epoch   2 | step 22550 | batch 4550/18000 | lr 0.00000 0.00003 | loss 0.1494 | s/batch 0.49\n",
      "2020-08-17 03:22:16,843 INFO: | epoch   2 | step 22600 | batch 4600/18000 | lr 0.00000 0.00003 | loss 0.1512 | s/batch 0.46\n",
      "2020-08-17 03:22:40,275 INFO: | epoch   2 | step 22650 | batch 4650/18000 | lr 0.00000 0.00003 | loss 0.1952 | s/batch 0.47\n",
      "2020-08-17 03:23:00,929 INFO: | epoch   2 | step 22700 | batch 4700/18000 | lr 0.00000 0.00003 | loss 0.1953 | s/batch 0.41\n",
      "2020-08-17 03:23:20,733 INFO: | epoch   2 | step 22750 | batch 4750/18000 | lr 0.00000 0.00003 | loss 0.0758 | s/batch 0.40\n",
      "2020-08-17 03:23:43,573 INFO: | epoch   2 | step 22800 | batch 4800/18000 | lr 0.00000 0.00003 | loss 0.1189 | s/batch 0.46\n",
      "2020-08-17 03:24:07,113 INFO: | epoch   2 | step 22850 | batch 4850/18000 | lr 0.00000 0.00003 | loss 0.1217 | s/batch 0.47\n",
      "2020-08-17 03:24:28,643 INFO: | epoch   2 | step 22900 | batch 4900/18000 | lr 0.00000 0.00003 | loss 0.0859 | s/batch 0.43\n",
      "2020-08-17 03:24:52,051 INFO: | epoch   2 | step 22950 | batch 4950/18000 | lr 0.00000 0.00003 | loss 0.1951 | s/batch 0.47\n",
      "2020-08-17 03:25:15,930 INFO: | epoch   2 | step 23000 | batch 5000/18000 | lr 0.00000 0.00003 | loss 0.1555 | s/batch 0.48\n",
      "2020-08-17 03:25:41,256 INFO: | epoch   2 | step 23050 | batch 5050/18000 | lr 0.00000 0.00003 | loss 0.1513 | s/batch 0.51\n",
      "2020-08-17 03:26:04,615 INFO: | epoch   2 | step 23100 | batch 5100/18000 | lr 0.00000 0.00003 | loss 0.2096 | s/batch 0.47\n",
      "2020-08-17 03:26:27,043 INFO: | epoch   2 | step 23150 | batch 5150/18000 | lr 0.00000 0.00003 | loss 0.1135 | s/batch 0.45\n",
      "2020-08-17 03:26:50,653 INFO: | epoch   2 | step 23200 | batch 5200/18000 | lr 0.00000 0.00003 | loss 0.1895 | s/batch 0.47\n",
      "2020-08-17 03:27:14,486 INFO: | epoch   2 | step 23250 | batch 5250/18000 | lr 0.00000 0.00003 | loss 0.1605 | s/batch 0.48\n",
      "2020-08-17 03:27:36,610 INFO: | epoch   2 | step 23300 | batch 5300/18000 | lr 0.00000 0.00003 | loss 0.1719 | s/batch 0.44\n",
      "2020-08-17 03:27:59,143 INFO: | epoch   2 | step 23350 | batch 5350/18000 | lr 0.00000 0.00003 | loss 0.1659 | s/batch 0.45\n",
      "2020-08-17 03:28:24,287 INFO: | epoch   2 | step 23400 | batch 5400/18000 | lr 0.00000 0.00003 | loss 0.1780 | s/batch 0.50\n",
      "2020-08-17 03:28:49,717 INFO: | epoch   2 | step 23450 | batch 5450/18000 | lr 0.00000 0.00003 | loss 0.1595 | s/batch 0.51\n",
      "2020-08-17 03:29:15,911 INFO: | epoch   2 | step 23500 | batch 5500/18000 | lr 0.00000 0.00003 | loss 0.1875 | s/batch 0.52\n",
      "2020-08-17 03:29:37,617 INFO: | epoch   2 | step 23550 | batch 5550/18000 | lr 0.00000 0.00003 | loss 0.1580 | s/batch 0.43\n",
      "2020-08-17 03:29:58,452 INFO: | epoch   2 | step 23600 | batch 5600/18000 | lr 0.00000 0.00003 | loss 0.1605 | s/batch 0.42\n",
      "2020-08-17 03:30:23,193 INFO: | epoch   2 | step 23650 | batch 5650/18000 | lr 0.00000 0.00003 | loss 0.2011 | s/batch 0.49\n",
      "2020-08-17 03:30:46,234 INFO: | epoch   2 | step 23700 | batch 5700/18000 | lr 0.00000 0.00003 | loss 0.2214 | s/batch 0.46\n",
      "2020-08-17 03:31:10,127 INFO: | epoch   2 | step 23750 | batch 5750/18000 | lr 0.00000 0.00003 | loss 0.1659 | s/batch 0.48\n",
      "2020-08-17 03:31:33,983 INFO: | epoch   2 | step 23800 | batch 5800/18000 | lr 0.00000 0.00003 | loss 0.1209 | s/batch 0.48\n",
      "2020-08-17 03:32:00,051 INFO: | epoch   2 | step 23850 | batch 5850/18000 | lr 0.00000 0.00003 | loss 0.1387 | s/batch 0.52\n",
      "2020-08-17 03:32:22,068 INFO: | epoch   2 | step 23900 | batch 5900/18000 | lr 0.00000 0.00003 | loss 0.2033 | s/batch 0.44\n",
      "2020-08-17 03:32:44,243 INFO: | epoch   2 | step 23950 | batch 5950/18000 | lr 0.00000 0.00003 | loss 0.1357 | s/batch 0.44\n",
      "2020-08-17 03:33:08,846 INFO: | epoch   2 | step 24000 | batch 6000/18000 | lr 0.00000 0.00003 | loss 0.1867 | s/batch 0.49\n",
      "2020-08-17 03:33:31,329 INFO: | epoch   2 | step 24050 | batch 6050/18000 | lr 0.00000 0.00003 | loss 0.1525 | s/batch 0.45\n",
      "2020-08-17 03:33:55,005 INFO: | epoch   2 | step 24100 | batch 6100/18000 | lr 0.00000 0.00003 | loss 0.1157 | s/batch 0.47\n",
      "2020-08-17 03:34:21,182 INFO: | epoch   2 | step 24150 | batch 6150/18000 | lr 0.00000 0.00003 | loss 0.1701 | s/batch 0.52\n",
      "2020-08-17 03:34:45,012 INFO: | epoch   2 | step 24200 | batch 6200/18000 | lr 0.00000 0.00003 | loss 0.1489 | s/batch 0.48\n",
      "2020-08-17 03:35:08,886 INFO: | epoch   2 | step 24250 | batch 6250/18000 | lr 0.00000 0.00003 | loss 0.1206 | s/batch 0.48\n",
      "2020-08-17 03:35:32,501 INFO: | epoch   2 | step 24300 | batch 6300/18000 | lr 0.00000 0.00003 | loss 0.2259 | s/batch 0.47\n",
      "2020-08-17 03:35:54,921 INFO: | epoch   2 | step 24350 | batch 6350/18000 | lr 0.00000 0.00003 | loss 0.1918 | s/batch 0.45\n",
      "2020-08-17 03:36:18,208 INFO: | epoch   2 | step 24400 | batch 6400/18000 | lr 0.00000 0.00003 | loss 0.1217 | s/batch 0.47\n",
      "2020-08-17 03:36:41,753 INFO: | epoch   2 | step 24450 | batch 6450/18000 | lr 0.00000 0.00003 | loss 0.1613 | s/batch 0.47\n",
      "2020-08-17 03:37:06,582 INFO: | epoch   2 | step 24500 | batch 6500/18000 | lr 0.00000 0.00003 | loss 0.1236 | s/batch 0.50\n",
      "2020-08-17 03:37:26,933 INFO: | epoch   2 | step 24550 | batch 6550/18000 | lr 0.00000 0.00003 | loss 0.0776 | s/batch 0.41\n",
      "2020-08-17 03:37:48,530 INFO: | epoch   2 | step 24600 | batch 6600/18000 | lr 0.00000 0.00003 | loss 0.1313 | s/batch 0.43\n",
      "2020-08-17 03:38:12,078 INFO: | epoch   2 | step 24650 | batch 6650/18000 | lr 0.00000 0.00003 | loss 0.1895 | s/batch 0.47\n",
      "2020-08-17 03:38:38,103 INFO: | epoch   2 | step 24700 | batch 6700/18000 | lr 0.00000 0.00003 | loss 0.1480 | s/batch 0.52\n",
      "2020-08-17 03:39:01,811 INFO: | epoch   2 | step 24750 | batch 6750/18000 | lr 0.00000 0.00003 | loss 0.1588 | s/batch 0.47\n",
      "2020-08-17 03:39:25,015 INFO: | epoch   2 | step 24800 | batch 6800/18000 | lr 0.00000 0.00003 | loss 0.1795 | s/batch 0.46\n",
      "2020-08-17 03:39:49,029 INFO: | epoch   2 | step 24850 | batch 6850/18000 | lr 0.00000 0.00003 | loss 0.2507 | s/batch 0.48\n",
      "2020-08-17 03:40:08,718 INFO: | epoch   2 | step 24900 | batch 6900/18000 | lr 0.00000 0.00003 | loss 0.0963 | s/batch 0.39\n",
      "2020-08-17 03:40:35,213 INFO: | epoch   2 | step 24950 | batch 6950/18000 | lr 0.00000 0.00003 | loss 0.1886 | s/batch 0.53\n",
      "2020-08-17 03:40:56,179 INFO: | epoch   2 | step 25000 | batch 7000/18000 | lr 0.00000 0.00003 | loss 0.0850 | s/batch 0.42\n",
      "2020-08-17 03:41:20,133 INFO: | epoch   2 | step 25050 | batch 7050/18000 | lr 0.00000 0.00003 | loss 0.1717 | s/batch 0.48\n",
      "2020-08-17 03:41:42,562 INFO: | epoch   2 | step 25100 | batch 7100/18000 | lr 0.00000 0.00003 | loss 0.1148 | s/batch 0.45\n",
      "2020-08-17 03:42:03,721 INFO: | epoch   2 | step 25150 | batch 7150/18000 | lr 0.00000 0.00003 | loss 0.2067 | s/batch 0.42\n",
      "2020-08-17 03:42:25,659 INFO: | epoch   2 | step 25200 | batch 7200/18000 | lr 0.00000 0.00003 | loss 0.1501 | s/batch 0.44\n",
      "2020-08-17 03:42:47,866 INFO: | epoch   2 | step 25250 | batch 7250/18000 | lr 0.00000 0.00003 | loss 0.1172 | s/batch 0.44\n",
      "2020-08-17 03:43:11,335 INFO: | epoch   2 | step 25300 | batch 7300/18000 | lr 0.00000 0.00003 | loss 0.1299 | s/batch 0.47\n",
      "2020-08-17 03:43:35,673 INFO: | epoch   2 | step 25350 | batch 7350/18000 | lr 0.00000 0.00003 | loss 0.1690 | s/batch 0.49\n",
      "2020-08-17 03:43:56,628 INFO: | epoch   2 | step 25400 | batch 7400/18000 | lr 0.00000 0.00003 | loss 0.2182 | s/batch 0.42\n",
      "2020-08-17 03:44:20,234 INFO: | epoch   2 | step 25450 | batch 7450/18000 | lr 0.00000 0.00003 | loss 0.1356 | s/batch 0.47\n",
      "2020-08-17 03:44:43,706 INFO: | epoch   2 | step 25500 | batch 7500/18000 | lr 0.00000 0.00003 | loss 0.1674 | s/batch 0.47\n",
      "2020-08-17 03:45:06,998 INFO: | epoch   2 | step 25550 | batch 7550/18000 | lr 0.00000 0.00003 | loss 0.1890 | s/batch 0.47\n",
      "2020-08-17 03:45:31,624 INFO: | epoch   2 | step 25600 | batch 7600/18000 | lr 0.00000 0.00003 | loss 0.1298 | s/batch 0.49\n",
      "2020-08-17 03:45:55,368 INFO: | epoch   2 | step 25650 | batch 7650/18000 | lr 0.00000 0.00003 | loss 0.1407 | s/batch 0.47\n",
      "2020-08-17 03:46:19,208 INFO: | epoch   2 | step 25700 | batch 7700/18000 | lr 0.00000 0.00003 | loss 0.2346 | s/batch 0.48\n",
      "2020-08-17 03:46:40,412 INFO: | epoch   2 | step 25750 | batch 7750/18000 | lr 0.00000 0.00003 | loss 0.1738 | s/batch 0.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-17 03:47:04,481 INFO: | epoch   2 | step 25800 | batch 7800/18000 | lr 0.00000 0.00003 | loss 0.1404 | s/batch 0.48\n",
      "2020-08-17 03:47:26,551 INFO: | epoch   2 | step 25850 | batch 7850/18000 | lr 0.00000 0.00003 | loss 0.1695 | s/batch 0.44\n",
      "2020-08-17 03:47:47,787 INFO: | epoch   2 | step 25900 | batch 7900/18000 | lr 0.00000 0.00003 | loss 0.1872 | s/batch 0.42\n",
      "2020-08-17 03:48:08,288 INFO: | epoch   2 | step 25950 | batch 7950/18000 | lr 0.00000 0.00003 | loss 0.1615 | s/batch 0.41\n",
      "2020-08-17 03:48:32,901 INFO: | epoch   2 | step 26000 | batch 8000/18000 | lr 0.00000 0.00003 | loss 0.1470 | s/batch 0.49\n",
      "2020-08-17 03:48:55,346 INFO: | epoch   2 | step 26050 | batch 8050/18000 | lr 0.00000 0.00003 | loss 0.2089 | s/batch 0.45\n",
      "2020-08-17 03:49:22,011 INFO: | epoch   2 | step 26100 | batch 8100/18000 | lr 0.00000 0.00003 | loss 0.2394 | s/batch 0.53\n",
      "2020-08-17 03:49:43,817 INFO: | epoch   2 | step 26150 | batch 8150/18000 | lr 0.00000 0.00003 | loss 0.1551 | s/batch 0.44\n",
      "2020-08-17 03:50:07,933 INFO: | epoch   2 | step 26200 | batch 8200/18000 | lr 0.00000 0.00003 | loss 0.1609 | s/batch 0.48\n",
      "2020-08-17 03:50:32,334 INFO: | epoch   2 | step 26250 | batch 8250/18000 | lr 0.00000 0.00003 | loss 0.1637 | s/batch 0.49\n",
      "2020-08-17 03:50:54,232 INFO: | epoch   2 | step 26300 | batch 8300/18000 | lr 0.00000 0.00003 | loss 0.1666 | s/batch 0.44\n",
      "2020-08-17 03:51:20,629 INFO: | epoch   2 | step 26350 | batch 8350/18000 | lr 0.00000 0.00003 | loss 0.1852 | s/batch 0.53\n",
      "2020-08-17 03:51:45,836 INFO: | epoch   2 | step 26400 | batch 8400/18000 | lr 0.00000 0.00003 | loss 0.1693 | s/batch 0.50\n",
      "2020-08-17 03:52:07,419 INFO: | epoch   2 | step 26450 | batch 8450/18000 | lr 0.00000 0.00003 | loss 0.1675 | s/batch 0.43\n",
      "2020-08-17 03:52:30,260 INFO: | epoch   2 | step 26500 | batch 8500/18000 | lr 0.00000 0.00003 | loss 0.1824 | s/batch 0.46\n",
      "2020-08-17 03:52:53,748 INFO: | epoch   2 | step 26550 | batch 8550/18000 | lr 0.00000 0.00003 | loss 0.1593 | s/batch 0.47\n",
      "2020-08-17 03:53:17,782 INFO: | epoch   2 | step 26600 | batch 8600/18000 | lr 0.00000 0.00003 | loss 0.1294 | s/batch 0.48\n",
      "2020-08-17 03:53:40,636 INFO: | epoch   2 | step 26650 | batch 8650/18000 | lr 0.00000 0.00003 | loss 0.1893 | s/batch 0.46\n",
      "2020-08-17 03:54:03,626 INFO: | epoch   2 | step 26700 | batch 8700/18000 | lr 0.00000 0.00003 | loss 0.1500 | s/batch 0.46\n",
      "2020-08-17 03:54:28,849 INFO: | epoch   2 | step 26750 | batch 8750/18000 | lr 0.00000 0.00003 | loss 0.1811 | s/batch 0.50\n",
      "2020-08-17 03:54:56,166 INFO: | epoch   2 | step 26800 | batch 8800/18000 | lr 0.00000 0.00003 | loss 0.2036 | s/batch 0.55\n",
      "2020-08-17 03:55:18,404 INFO: | epoch   2 | step 26850 | batch 8850/18000 | lr 0.00000 0.00003 | loss 0.1338 | s/batch 0.44\n",
      "2020-08-17 03:55:40,705 INFO: | epoch   2 | step 26900 | batch 8900/18000 | lr 0.00000 0.00003 | loss 0.1945 | s/batch 0.45\n",
      "2020-08-17 03:56:04,818 INFO: | epoch   2 | step 26950 | batch 8950/18000 | lr 0.00000 0.00003 | loss 0.1312 | s/batch 0.48\n",
      "2020-08-17 03:56:30,059 INFO: | epoch   2 | step 27000 | batch 9000/18000 | lr 0.00000 0.00003 | loss 0.1097 | s/batch 0.50\n",
      "2020-08-17 03:56:53,435 INFO: | epoch   2 | step 27050 | batch 9050/18000 | lr 0.00000 0.00002 | loss 0.2191 | s/batch 0.47\n",
      "2020-08-17 03:57:15,200 INFO: | epoch   2 | step 27100 | batch 9100/18000 | lr 0.00000 0.00002 | loss 0.1606 | s/batch 0.44\n",
      "2020-08-17 03:57:39,286 INFO: | epoch   2 | step 27150 | batch 9150/18000 | lr 0.00000 0.00002 | loss 0.1671 | s/batch 0.48\n",
      "2020-08-17 03:58:01,532 INFO: | epoch   2 | step 27200 | batch 9200/18000 | lr 0.00000 0.00002 | loss 0.1779 | s/batch 0.44\n",
      "2020-08-17 03:58:23,702 INFO: | epoch   2 | step 27250 | batch 9250/18000 | lr 0.00000 0.00002 | loss 0.2071 | s/batch 0.44\n",
      "2020-08-17 03:58:46,098 INFO: | epoch   2 | step 27300 | batch 9300/18000 | lr 0.00000 0.00002 | loss 0.1445 | s/batch 0.45\n",
      "2020-08-17 03:59:09,785 INFO: | epoch   2 | step 27350 | batch 9350/18000 | lr 0.00000 0.00002 | loss 0.1593 | s/batch 0.47\n",
      "2020-08-17 03:59:31,637 INFO: | epoch   2 | step 27400 | batch 9400/18000 | lr 0.00000 0.00002 | loss 0.2029 | s/batch 0.44\n",
      "2020-08-17 03:59:58,338 INFO: | epoch   2 | step 27450 | batch 9450/18000 | lr 0.00000 0.00002 | loss 0.1870 | s/batch 0.53\n",
      "2020-08-17 04:00:20,066 INFO: | epoch   2 | step 27500 | batch 9500/18000 | lr 0.00000 0.00002 | loss 0.1172 | s/batch 0.43\n",
      "2020-08-17 04:00:44,766 INFO: | epoch   2 | step 27550 | batch 9550/18000 | lr 0.00000 0.00002 | loss 0.1481 | s/batch 0.49\n",
      "2020-08-17 04:01:10,667 INFO: | epoch   2 | step 27600 | batch 9600/18000 | lr 0.00000 0.00002 | loss 0.1004 | s/batch 0.52\n",
      "2020-08-17 04:01:32,232 INFO: | epoch   2 | step 27650 | batch 9650/18000 | lr 0.00000 0.00002 | loss 0.1169 | s/batch 0.43\n",
      "2020-08-17 04:01:55,664 INFO: | epoch   2 | step 27700 | batch 9700/18000 | lr 0.00000 0.00002 | loss 0.1424 | s/batch 0.47\n",
      "2020-08-17 04:02:20,238 INFO: | epoch   2 | step 27750 | batch 9750/18000 | lr 0.00000 0.00002 | loss 0.2047 | s/batch 0.49\n",
      "2020-08-17 04:02:46,680 INFO: | epoch   2 | step 27800 | batch 9800/18000 | lr 0.00000 0.00002 | loss 0.1898 | s/batch 0.53\n",
      "2020-08-17 04:03:10,233 INFO: | epoch   2 | step 27850 | batch 9850/18000 | lr 0.00000 0.00002 | loss 0.1294 | s/batch 0.47\n",
      "2020-08-17 04:03:35,136 INFO: | epoch   2 | step 27900 | batch 9900/18000 | lr 0.00000 0.00002 | loss 0.2342 | s/batch 0.50\n",
      "2020-08-17 04:04:02,108 INFO: | epoch   2 | step 27950 | batch 9950/18000 | lr 0.00000 0.00002 | loss 0.1396 | s/batch 0.54\n",
      "2020-08-17 04:04:30,059 INFO: | epoch   2 | step 28000 | batch 10000/18000 | lr 0.00000 0.00002 | loss 0.2326 | s/batch 0.56\n",
      "2020-08-17 04:04:54,349 INFO: | epoch   2 | step 28050 | batch 10050/18000 | lr 0.00000 0.00002 | loss 0.1118 | s/batch 0.49\n",
      "2020-08-17 04:05:17,881 INFO: | epoch   2 | step 28100 | batch 10100/18000 | lr 0.00000 0.00002 | loss 0.1724 | s/batch 0.47\n",
      "2020-08-17 04:05:40,759 INFO: | epoch   2 | step 28150 | batch 10150/18000 | lr 0.00000 0.00002 | loss 0.1518 | s/batch 0.46\n",
      "2020-08-17 04:06:04,604 INFO: | epoch   2 | step 28200 | batch 10200/18000 | lr 0.00000 0.00002 | loss 0.1647 | s/batch 0.48\n",
      "2020-08-17 04:06:26,758 INFO: | epoch   2 | step 28250 | batch 10250/18000 | lr 0.00000 0.00002 | loss 0.1263 | s/batch 0.44\n",
      "2020-08-17 04:06:52,250 INFO: | epoch   2 | step 28300 | batch 10300/18000 | lr 0.00000 0.00002 | loss 0.1962 | s/batch 0.51\n",
      "2020-08-17 04:07:15,954 INFO: | epoch   2 | step 28350 | batch 10350/18000 | lr 0.00000 0.00002 | loss 0.2049 | s/batch 0.47\n",
      "2020-08-17 04:07:37,735 INFO: | epoch   2 | step 28400 | batch 10400/18000 | lr 0.00000 0.00002 | loss 0.1089 | s/batch 0.44\n",
      "2020-08-17 04:08:03,455 INFO: | epoch   2 | step 28450 | batch 10450/18000 | lr 0.00000 0.00002 | loss 0.1312 | s/batch 0.51\n",
      "2020-08-17 04:08:29,529 INFO: | epoch   2 | step 28500 | batch 10500/18000 | lr 0.00000 0.00002 | loss 0.1312 | s/batch 0.52\n",
      "2020-08-17 04:08:53,239 INFO: | epoch   2 | step 28550 | batch 10550/18000 | lr 0.00000 0.00002 | loss 0.1626 | s/batch 0.47\n",
      "2020-08-17 04:09:15,951 INFO: | epoch   2 | step 28600 | batch 10600/18000 | lr 0.00000 0.00002 | loss 0.1229 | s/batch 0.45\n",
      "2020-08-17 04:09:40,643 INFO: | epoch   2 | step 28650 | batch 10650/18000 | lr 0.00000 0.00002 | loss 0.1822 | s/batch 0.49\n",
      "2020-08-17 04:10:01,152 INFO: | epoch   2 | step 28700 | batch 10700/18000 | lr 0.00000 0.00002 | loss 0.2236 | s/batch 0.41\n",
      "2020-08-17 04:10:25,119 INFO: | epoch   2 | step 28750 | batch 10750/18000 | lr 0.00000 0.00002 | loss 0.1340 | s/batch 0.48\n",
      "2020-08-17 04:10:47,352 INFO: | epoch   2 | step 28800 | batch 10800/18000 | lr 0.00000 0.00002 | loss 0.1303 | s/batch 0.44\n",
      "2020-08-17 04:11:12,349 INFO: | epoch   2 | step 28850 | batch 10850/18000 | lr 0.00000 0.00002 | loss 0.2045 | s/batch 0.50\n",
      "2020-08-17 04:11:35,068 INFO: | epoch   2 | step 28900 | batch 10900/18000 | lr 0.00000 0.00002 | loss 0.1449 | s/batch 0.45\n",
      "2020-08-17 04:11:56,197 INFO: | epoch   2 | step 28950 | batch 10950/18000 | lr 0.00000 0.00002 | loss 0.1633 | s/batch 0.42\n",
      "2020-08-17 04:12:20,291 INFO: | epoch   2 | step 29000 | batch 11000/18000 | lr 0.00000 0.00002 | loss 0.1651 | s/batch 0.48\n",
      "2020-08-17 04:12:43,392 INFO: | epoch   2 | step 29050 | batch 11050/18000 | lr 0.00000 0.00002 | loss 0.1764 | s/batch 0.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-17 04:13:07,825 INFO: | epoch   2 | step 29100 | batch 11100/18000 | lr 0.00000 0.00002 | loss 0.1311 | s/batch 0.49\n",
      "2020-08-17 04:13:31,715 INFO: | epoch   2 | step 29150 | batch 11150/18000 | lr 0.00000 0.00002 | loss 0.1083 | s/batch 0.48\n",
      "2020-08-17 04:13:54,378 INFO: | epoch   2 | step 29200 | batch 11200/18000 | lr 0.00000 0.00002 | loss 0.1840 | s/batch 0.45\n",
      "2020-08-17 04:14:16,093 INFO: | epoch   2 | step 29250 | batch 11250/18000 | lr 0.00000 0.00002 | loss 0.1501 | s/batch 0.43\n",
      "2020-08-17 04:14:37,926 INFO: | epoch   2 | step 29300 | batch 11300/18000 | lr 0.00000 0.00002 | loss 0.1298 | s/batch 0.44\n",
      "2020-08-17 04:15:03,360 INFO: | epoch   2 | step 29350 | batch 11350/18000 | lr 0.00000 0.00002 | loss 0.1774 | s/batch 0.51\n",
      "2020-08-17 04:15:31,164 INFO: | epoch   2 | step 29400 | batch 11400/18000 | lr 0.00000 0.00002 | loss 0.1378 | s/batch 0.56\n",
      "2020-08-17 04:15:54,851 INFO: | epoch   2 | step 29450 | batch 11450/18000 | lr 0.00000 0.00002 | loss 0.1511 | s/batch 0.47\n",
      "2020-08-17 04:16:20,703 INFO: | epoch   2 | step 29500 | batch 11500/18000 | lr 0.00000 0.00002 | loss 0.1270 | s/batch 0.52\n",
      "2020-08-17 04:16:44,829 INFO: | epoch   2 | step 29550 | batch 11550/18000 | lr 0.00000 0.00002 | loss 0.1862 | s/batch 0.48\n",
      "2020-08-17 04:17:06,665 INFO: | epoch   2 | step 29600 | batch 11600/18000 | lr 0.00000 0.00002 | loss 0.1410 | s/batch 0.44\n",
      "2020-08-17 04:17:27,169 INFO: | epoch   2 | step 29650 | batch 11650/18000 | lr 0.00000 0.00002 | loss 0.1433 | s/batch 0.41\n",
      "2020-08-17 04:17:51,534 INFO: | epoch   2 | step 29700 | batch 11700/18000 | lr 0.00000 0.00002 | loss 0.1875 | s/batch 0.49\n",
      "2020-08-17 04:18:15,116 INFO: | epoch   2 | step 29750 | batch 11750/18000 | lr 0.00000 0.00002 | loss 0.1468 | s/batch 0.47\n",
      "2020-08-17 04:18:39,583 INFO: | epoch   2 | step 29800 | batch 11800/18000 | lr 0.00000 0.00002 | loss 0.1228 | s/batch 0.49\n",
      "2020-08-17 04:19:05,196 INFO: | epoch   2 | step 29850 | batch 11850/18000 | lr 0.00000 0.00002 | loss 0.1542 | s/batch 0.51\n",
      "2020-08-17 04:19:30,636 INFO: | epoch   2 | step 29900 | batch 11900/18000 | lr 0.00000 0.00002 | loss 0.2078 | s/batch 0.51\n",
      "2020-08-17 04:19:52,820 INFO: | epoch   2 | step 29950 | batch 11950/18000 | lr 0.00000 0.00002 | loss 0.1709 | s/batch 0.44\n",
      "2020-08-17 04:20:13,536 INFO: | epoch   2 | step 30000 | batch 12000/18000 | lr 0.00000 0.00002 | loss 0.1175 | s/batch 0.41\n",
      "2020-08-17 04:20:35,819 INFO: | epoch   2 | step 30050 | batch 12050/18000 | lr 0.00000 0.00002 | loss 0.1352 | s/batch 0.45\n",
      "2020-08-17 04:20:59,452 INFO: | epoch   2 | step 30100 | batch 12100/18000 | lr 0.00000 0.00002 | loss 0.1305 | s/batch 0.47\n",
      "2020-08-17 04:21:23,011 INFO: | epoch   2 | step 30150 | batch 12150/18000 | lr 0.00000 0.00002 | loss 0.1254 | s/batch 0.47\n",
      "2020-08-17 04:21:45,139 INFO: | epoch   2 | step 30200 | batch 12200/18000 | lr 0.00000 0.00002 | loss 0.0879 | s/batch 0.44\n",
      "2020-08-17 04:22:06,397 INFO: | epoch   2 | step 30250 | batch 12250/18000 | lr 0.00000 0.00002 | loss 0.1243 | s/batch 0.43\n",
      "2020-08-17 04:22:30,924 INFO: | epoch   2 | step 30300 | batch 12300/18000 | lr 0.00000 0.00002 | loss 0.1153 | s/batch 0.49\n",
      "2020-08-17 04:22:54,712 INFO: | epoch   2 | step 30350 | batch 12350/18000 | lr 0.00000 0.00002 | loss 0.1264 | s/batch 0.48\n",
      "2020-08-17 04:23:17,825 INFO: | epoch   2 | step 30400 | batch 12400/18000 | lr 0.00000 0.00002 | loss 0.1723 | s/batch 0.46\n",
      "2020-08-17 04:23:41,253 INFO: | epoch   2 | step 30450 | batch 12450/18000 | lr 0.00000 0.00002 | loss 0.1321 | s/batch 0.47\n",
      "2020-08-17 04:24:04,277 INFO: | epoch   2 | step 30500 | batch 12500/18000 | lr 0.00000 0.00002 | loss 0.1870 | s/batch 0.46\n",
      "2020-08-17 04:24:28,805 INFO: | epoch   2 | step 30550 | batch 12550/18000 | lr 0.00000 0.00002 | loss 0.1469 | s/batch 0.49\n",
      "2020-08-17 04:24:57,239 INFO: | epoch   2 | step 30600 | batch 12600/18000 | lr 0.00000 0.00002 | loss 0.1717 | s/batch 0.57\n",
      "2020-08-17 04:25:22,230 INFO: | epoch   2 | step 30650 | batch 12650/18000 | lr 0.00000 0.00002 | loss 0.1438 | s/batch 0.50\n",
      "2020-08-17 04:25:43,644 INFO: | epoch   2 | step 30700 | batch 12700/18000 | lr 0.00000 0.00002 | loss 0.2316 | s/batch 0.43\n",
      "2020-08-17 04:26:06,678 INFO: | epoch   2 | step 30750 | batch 12750/18000 | lr 0.00000 0.00002 | loss 0.1689 | s/batch 0.46\n",
      "2020-08-17 04:26:30,680 INFO: | epoch   2 | step 30800 | batch 12800/18000 | lr 0.00000 0.00002 | loss 0.1294 | s/batch 0.48\n",
      "2020-08-17 04:26:52,815 INFO: | epoch   2 | step 30850 | batch 12850/18000 | lr 0.00000 0.00002 | loss 0.2039 | s/batch 0.44\n",
      "2020-08-17 04:27:14,476 INFO: | epoch   2 | step 30900 | batch 12900/18000 | lr 0.00000 0.00002 | loss 0.1369 | s/batch 0.43\n",
      "2020-08-17 04:27:36,697 INFO: | epoch   2 | step 30950 | batch 12950/18000 | lr 0.00000 0.00002 | loss 0.1836 | s/batch 0.44\n",
      "2020-08-17 04:27:58,917 INFO: | epoch   2 | step 31000 | batch 13000/18000 | lr 0.00000 0.00002 | loss 0.1506 | s/batch 0.44\n",
      "2020-08-17 04:28:21,625 INFO: | epoch   2 | step 31050 | batch 13050/18000 | lr 0.00000 0.00002 | loss 0.1466 | s/batch 0.45\n",
      "2020-08-17 04:28:46,219 INFO: | epoch   2 | step 31100 | batch 13100/18000 | lr 0.00000 0.00002 | loss 0.0527 | s/batch 0.49\n",
      "2020-08-17 04:29:06,963 INFO: | epoch   2 | step 31150 | batch 13150/18000 | lr 0.00000 0.00002 | loss 0.1509 | s/batch 0.41\n",
      "2020-08-17 04:29:29,844 INFO: | epoch   2 | step 31200 | batch 13200/18000 | lr 0.00000 0.00002 | loss 0.2238 | s/batch 0.46\n",
      "2020-08-17 04:29:54,559 INFO: | epoch   2 | step 31250 | batch 13250/18000 | lr 0.00000 0.00002 | loss 0.1356 | s/batch 0.49\n",
      "2020-08-17 04:30:19,715 INFO: | epoch   2 | step 31300 | batch 13300/18000 | lr 0.00000 0.00002 | loss 0.1062 | s/batch 0.50\n",
      "2020-08-17 04:30:40,328 INFO: | epoch   2 | step 31350 | batch 13350/18000 | lr 0.00000 0.00002 | loss 0.1805 | s/batch 0.41\n",
      "2020-08-17 04:31:04,107 INFO: | epoch   2 | step 31400 | batch 13400/18000 | lr 0.00000 0.00002 | loss 0.1196 | s/batch 0.48\n",
      "2020-08-17 04:31:26,084 INFO: | epoch   2 | step 31450 | batch 13450/18000 | lr 0.00000 0.00002 | loss 0.1038 | s/batch 0.44\n",
      "2020-08-17 04:31:49,646 INFO: | epoch   2 | step 31500 | batch 13500/18000 | lr 0.00000 0.00002 | loss 0.1382 | s/batch 0.47\n",
      "2020-08-17 04:32:15,059 INFO: | epoch   2 | step 31550 | batch 13550/18000 | lr 0.00000 0.00002 | loss 0.1336 | s/batch 0.51\n",
      "2020-08-17 04:32:36,514 INFO: | epoch   2 | step 31600 | batch 13600/18000 | lr 0.00000 0.00002 | loss 0.1316 | s/batch 0.43\n",
      "2020-08-17 04:32:59,240 INFO: | epoch   2 | step 31650 | batch 13650/18000 | lr 0.00000 0.00002 | loss 0.1017 | s/batch 0.45\n",
      "2020-08-17 04:33:23,987 INFO: | epoch   2 | step 31700 | batch 13700/18000 | lr 0.00000 0.00002 | loss 0.1461 | s/batch 0.49\n",
      "2020-08-17 04:33:44,797 INFO: | epoch   2 | step 31750 | batch 13750/18000 | lr 0.00000 0.00002 | loss 0.1225 | s/batch 0.42\n",
      "2020-08-17 04:34:10,534 INFO: | epoch   2 | step 31800 | batch 13800/18000 | lr 0.00000 0.00002 | loss 0.1476 | s/batch 0.51\n",
      "2020-08-17 04:34:34,732 INFO: | epoch   2 | step 31850 | batch 13850/18000 | lr 0.00000 0.00002 | loss 0.1372 | s/batch 0.48\n",
      "2020-08-17 04:34:54,869 INFO: | epoch   2 | step 31900 | batch 13900/18000 | lr 0.00000 0.00002 | loss 0.1681 | s/batch 0.40\n",
      "2020-08-17 04:35:17,771 INFO: | epoch   2 | step 31950 | batch 13950/18000 | lr 0.00000 0.00002 | loss 0.1628 | s/batch 0.46\n",
      "2020-08-17 04:35:40,891 INFO: | epoch   2 | step 32000 | batch 14000/18000 | lr 0.00000 0.00002 | loss 0.1485 | s/batch 0.46\n",
      "2020-08-17 04:36:03,342 INFO: | epoch   2 | step 32050 | batch 14050/18000 | lr 0.00000 0.00002 | loss 0.1533 | s/batch 0.45\n",
      "2020-08-17 04:36:25,297 INFO: | epoch   2 | step 32100 | batch 14100/18000 | lr 0.00000 0.00002 | loss 0.1365 | s/batch 0.44\n",
      "2020-08-17 04:36:46,973 INFO: | epoch   2 | step 32150 | batch 14150/18000 | lr 0.00000 0.00002 | loss 0.1517 | s/batch 0.43\n",
      "2020-08-17 04:37:09,109 INFO: | epoch   2 | step 32200 | batch 14200/18000 | lr 0.00000 0.00002 | loss 0.1468 | s/batch 0.44\n",
      "2020-08-17 04:37:33,191 INFO: | epoch   2 | step 32250 | batch 14250/18000 | lr 0.00000 0.00002 | loss 0.1246 | s/batch 0.48\n",
      "2020-08-17 04:37:59,947 INFO: | epoch   2 | step 32300 | batch 14300/18000 | lr 0.00000 0.00002 | loss 0.1301 | s/batch 0.54\n",
      "2020-08-17 04:38:25,924 INFO: | epoch   2 | step 32350 | batch 14350/18000 | lr 0.00000 0.00002 | loss 0.1554 | s/batch 0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-17 04:38:48,521 INFO: | epoch   2 | step 32400 | batch 14400/18000 | lr 0.00000 0.00002 | loss 0.1083 | s/batch 0.45\n",
      "2020-08-17 04:39:13,009 INFO: | epoch   2 | step 32450 | batch 14450/18000 | lr 0.00000 0.00002 | loss 0.1698 | s/batch 0.49\n",
      "2020-08-17 04:39:34,039 INFO: | epoch   2 | step 32500 | batch 14500/18000 | lr 0.00000 0.00002 | loss 0.1325 | s/batch 0.42\n",
      "2020-08-17 04:39:57,063 INFO: | epoch   2 | step 32550 | batch 14550/18000 | lr 0.00000 0.00002 | loss 0.1371 | s/batch 0.46\n",
      "2020-08-17 04:40:21,802 INFO: | epoch   2 | step 32600 | batch 14600/18000 | lr 0.00000 0.00002 | loss 0.1667 | s/batch 0.49\n",
      "2020-08-17 04:40:43,978 INFO: | epoch   2 | step 32650 | batch 14650/18000 | lr 0.00000 0.00002 | loss 0.1923 | s/batch 0.44\n",
      "2020-08-17 04:41:08,329 INFO: | epoch   2 | step 32700 | batch 14700/18000 | lr 0.00000 0.00002 | loss 0.1553 | s/batch 0.49\n",
      "2020-08-17 04:41:34,974 INFO: | epoch   2 | step 32750 | batch 14750/18000 | lr 0.00000 0.00002 | loss 0.2058 | s/batch 0.53\n",
      "2020-08-17 04:41:54,357 INFO: | epoch   2 | step 32800 | batch 14800/18000 | lr 0.00000 0.00002 | loss 0.0903 | s/batch 0.39\n",
      "2020-08-17 04:42:14,073 INFO: | epoch   2 | step 32850 | batch 14850/18000 | lr 0.00000 0.00002 | loss 0.1223 | s/batch 0.39\n",
      "2020-08-17 04:42:35,949 INFO: | epoch   2 | step 32900 | batch 14900/18000 | lr 0.00000 0.00002 | loss 0.2054 | s/batch 0.44\n",
      "2020-08-17 04:43:00,326 INFO: | epoch   2 | step 32950 | batch 14950/18000 | lr 0.00000 0.00002 | loss 0.1427 | s/batch 0.49\n",
      "2020-08-17 04:43:21,542 INFO: | epoch   2 | step 33000 | batch 15000/18000 | lr 0.00000 0.00002 | loss 0.1404 | s/batch 0.42\n",
      "2020-08-17 04:43:43,601 INFO: | epoch   2 | step 33050 | batch 15050/18000 | lr 0.00000 0.00002 | loss 0.1392 | s/batch 0.44\n",
      "2020-08-17 04:44:06,843 INFO: | epoch   2 | step 33100 | batch 15100/18000 | lr 0.00000 0.00002 | loss 0.1229 | s/batch 0.46\n",
      "2020-08-17 04:44:30,844 INFO: | epoch   2 | step 33150 | batch 15150/18000 | lr 0.00000 0.00002 | loss 0.1246 | s/batch 0.48\n",
      "2020-08-17 04:44:57,377 INFO: | epoch   2 | step 33200 | batch 15200/18000 | lr 0.00000 0.00002 | loss 0.1472 | s/batch 0.53\n",
      "2020-08-17 04:45:17,740 INFO: | epoch   2 | step 33250 | batch 15250/18000 | lr 0.00000 0.00002 | loss 0.1257 | s/batch 0.41\n",
      "2020-08-17 04:45:42,055 INFO: | epoch   2 | step 33300 | batch 15300/18000 | lr 0.00000 0.00002 | loss 0.1779 | s/batch 0.49\n",
      "2020-08-17 04:46:03,783 INFO: | epoch   2 | step 33350 | batch 15350/18000 | lr 0.00000 0.00002 | loss 0.1352 | s/batch 0.43\n",
      "2020-08-17 04:46:31,015 INFO: | epoch   2 | step 33400 | batch 15400/18000 | lr 0.00000 0.00002 | loss 0.1642 | s/batch 0.54\n",
      "2020-08-17 04:46:52,413 INFO: | epoch   2 | step 33450 | batch 15450/18000 | lr 0.00000 0.00002 | loss 0.1504 | s/batch 0.43\n",
      "2020-08-17 04:47:17,956 INFO: | epoch   2 | step 33500 | batch 15500/18000 | lr 0.00000 0.00002 | loss 0.1536 | s/batch 0.51\n",
      "2020-08-17 04:47:43,064 INFO: | epoch   2 | step 33550 | batch 15550/18000 | lr 0.00000 0.00002 | loss 0.1110 | s/batch 0.50\n",
      "2020-08-17 04:48:06,296 INFO: | epoch   2 | step 33600 | batch 15600/18000 | lr 0.00000 0.00002 | loss 0.1103 | s/batch 0.46\n",
      "2020-08-17 04:48:28,042 INFO: | epoch   2 | step 33650 | batch 15650/18000 | lr 0.00000 0.00002 | loss 0.1435 | s/batch 0.43\n",
      "2020-08-17 04:48:52,509 INFO: | epoch   2 | step 33700 | batch 15700/18000 | lr 0.00000 0.00002 | loss 0.1405 | s/batch 0.49\n",
      "2020-08-17 04:49:15,457 INFO: | epoch   2 | step 33750 | batch 15750/18000 | lr 0.00000 0.00002 | loss 0.1487 | s/batch 0.46\n",
      "2020-08-17 04:49:40,796 INFO: | epoch   2 | step 33800 | batch 15800/18000 | lr 0.00000 0.00002 | loss 0.1422 | s/batch 0.51\n",
      "2020-08-17 04:50:02,555 INFO: | epoch   2 | step 33850 | batch 15850/18000 | lr 0.00000 0.00002 | loss 0.0748 | s/batch 0.44\n",
      "2020-08-17 04:50:26,060 INFO: | epoch   2 | step 33900 | batch 15900/18000 | lr 0.00000 0.00002 | loss 0.1978 | s/batch 0.47\n",
      "2020-08-17 04:50:51,605 INFO: | epoch   2 | step 33950 | batch 15950/18000 | lr 0.00000 0.00002 | loss 0.1535 | s/batch 0.51\n",
      "2020-08-17 04:51:16,311 INFO: | epoch   2 | step 34000 | batch 16000/18000 | lr 0.00000 0.00002 | loss 0.1459 | s/batch 0.49\n",
      "2020-08-17 04:51:36,548 INFO: | epoch   2 | step 34050 | batch 16050/18000 | lr 0.00000 0.00002 | loss 0.1301 | s/batch 0.40\n",
      "2020-08-17 04:52:02,252 INFO: | epoch   2 | step 34100 | batch 16100/18000 | lr 0.00000 0.00002 | loss 0.0970 | s/batch 0.51\n",
      "2020-08-17 04:52:25,594 INFO: | epoch   2 | step 34150 | batch 16150/18000 | lr 0.00000 0.00002 | loss 0.1211 | s/batch 0.47\n",
      "2020-08-17 04:52:48,006 INFO: | epoch   2 | step 34200 | batch 16200/18000 | lr 0.00000 0.00002 | loss 0.1630 | s/batch 0.45\n",
      "2020-08-17 04:53:09,070 INFO: | epoch   2 | step 34250 | batch 16250/18000 | lr 0.00000 0.00002 | loss 0.1954 | s/batch 0.42\n",
      "2020-08-17 04:53:33,877 INFO: | epoch   2 | step 34300 | batch 16300/18000 | lr 0.00000 0.00002 | loss 0.1578 | s/batch 0.50\n",
      "2020-08-17 04:53:56,338 INFO: | epoch   2 | step 34350 | batch 16350/18000 | lr 0.00000 0.00002 | loss 0.1702 | s/batch 0.45\n",
      "2020-08-17 04:54:21,186 INFO: | epoch   2 | step 34400 | batch 16400/18000 | lr 0.00000 0.00002 | loss 0.1555 | s/batch 0.50\n",
      "2020-08-17 04:54:44,429 INFO: | epoch   2 | step 34450 | batch 16450/18000 | lr 0.00000 0.00002 | loss 0.1201 | s/batch 0.46\n",
      "2020-08-17 04:55:05,364 INFO: | epoch   2 | step 34500 | batch 16500/18000 | lr 0.00000 0.00002 | loss 0.0984 | s/batch 0.42\n",
      "2020-08-17 04:55:27,802 INFO: | epoch   2 | step 34550 | batch 16550/18000 | lr 0.00000 0.00002 | loss 0.1552 | s/batch 0.45\n",
      "2020-08-17 04:55:50,839 INFO: | epoch   2 | step 34600 | batch 16600/18000 | lr 0.00000 0.00002 | loss 0.1469 | s/batch 0.46\n",
      "2020-08-17 04:56:15,199 INFO: | epoch   2 | step 34650 | batch 16650/18000 | lr 0.00000 0.00002 | loss 0.1462 | s/batch 0.49\n",
      "2020-08-17 04:56:40,735 INFO: | epoch   2 | step 34700 | batch 16700/18000 | lr 0.00000 0.00002 | loss 0.1397 | s/batch 0.51\n",
      "2020-08-17 04:57:03,897 INFO: | epoch   2 | step 34750 | batch 16750/18000 | lr 0.00000 0.00002 | loss 0.1469 | s/batch 0.46\n",
      "2020-08-17 04:57:30,128 INFO: | epoch   2 | step 34800 | batch 16800/18000 | lr 0.00000 0.00002 | loss 0.1108 | s/batch 0.52\n",
      "2020-08-17 04:57:52,213 INFO: | epoch   2 | step 34850 | batch 16850/18000 | lr 0.00000 0.00002 | loss 0.1547 | s/batch 0.44\n",
      "2020-08-17 04:58:17,721 INFO: | epoch   2 | step 34900 | batch 16900/18000 | lr 0.00000 0.00002 | loss 0.1197 | s/batch 0.51\n",
      "2020-08-17 04:58:37,312 INFO: | epoch   2 | step 34950 | batch 16950/18000 | lr 0.00000 0.00002 | loss 0.1402 | s/batch 0.39\n",
      "2020-08-17 04:59:00,684 INFO: | epoch   2 | step 35000 | batch 17000/18000 | lr 0.00000 0.00002 | loss 0.1230 | s/batch 0.47\n",
      "2020-08-17 04:59:24,018 INFO: | epoch   2 | step 35050 | batch 17050/18000 | lr 0.00000 0.00002 | loss 0.1286 | s/batch 0.47\n",
      "2020-08-17 04:59:48,803 INFO: | epoch   2 | step 35100 | batch 17100/18000 | lr 0.00000 0.00002 | loss 0.1578 | s/batch 0.50\n",
      "2020-08-17 05:00:10,962 INFO: | epoch   2 | step 35150 | batch 17150/18000 | lr 0.00000 0.00002 | loss 0.1517 | s/batch 0.44\n",
      "2020-08-17 05:00:33,666 INFO: | epoch   2 | step 35200 | batch 17200/18000 | lr 0.00000 0.00002 | loss 0.1222 | s/batch 0.45\n",
      "2020-08-17 05:00:53,995 INFO: | epoch   2 | step 35250 | batch 17250/18000 | lr 0.00000 0.00002 | loss 0.1332 | s/batch 0.41\n",
      "2020-08-17 05:01:17,138 INFO: | epoch   2 | step 35300 | batch 17300/18000 | lr 0.00000 0.00002 | loss 0.1637 | s/batch 0.46\n",
      "2020-08-17 05:01:43,322 INFO: | epoch   2 | step 35350 | batch 17350/18000 | lr 0.00000 0.00002 | loss 0.1879 | s/batch 0.52\n",
      "2020-08-17 05:02:10,896 INFO: | epoch   2 | step 35400 | batch 17400/18000 | lr 0.00000 0.00002 | loss 0.2129 | s/batch 0.55\n",
      "2020-08-17 05:02:38,598 INFO: | epoch   2 | step 35450 | batch 17450/18000 | lr 0.00000 0.00002 | loss 0.1111 | s/batch 0.55\n",
      "2020-08-17 05:03:02,690 INFO: | epoch   2 | step 35500 | batch 17500/18000 | lr 0.00000 0.00002 | loss 0.1542 | s/batch 0.48\n",
      "2020-08-17 05:03:22,449 INFO: | epoch   2 | step 35550 | batch 17550/18000 | lr 0.00000 0.00002 | loss 0.0912 | s/batch 0.40\n",
      "2020-08-17 05:03:45,604 INFO: | epoch   2 | step 35600 | batch 17600/18000 | lr 0.00000 0.00002 | loss 0.1107 | s/batch 0.46\n",
      "2020-08-17 05:04:10,677 INFO: | epoch   2 | step 35650 | batch 17650/18000 | lr 0.00000 0.00002 | loss 0.1468 | s/batch 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-17 05:04:37,111 INFO: | epoch   2 | step 35700 | batch 17700/18000 | lr 0.00000 0.00002 | loss 0.1551 | s/batch 0.53\n",
      "2020-08-17 05:05:00,046 INFO: | epoch   2 | step 35750 | batch 17750/18000 | lr 0.00000 0.00002 | loss 0.1214 | s/batch 0.46\n",
      "2020-08-17 05:05:24,251 INFO: | epoch   2 | step 35800 | batch 17800/18000 | lr 0.00000 0.00002 | loss 0.1832 | s/batch 0.48\n",
      "2020-08-17 05:05:49,273 INFO: | epoch   2 | step 35850 | batch 17850/18000 | lr 0.00000 0.00002 | loss 0.1913 | s/batch 0.50\n",
      "2020-08-17 05:06:12,793 INFO: | epoch   2 | step 35900 | batch 17900/18000 | lr 0.00000 0.00002 | loss 0.1685 | s/batch 0.47\n",
      "2020-08-17 05:06:34,073 INFO: | epoch   2 | step 35950 | batch 17950/18000 | lr 0.00000 0.00002 | loss 0.1147 | s/batch 0.43\n",
      "2020-08-17 05:06:58,157 INFO: | epoch   2 | step 36000 | batch 18000/18000 | lr 0.00000 0.00002 | loss 0.1393 | s/batch 0.48\n",
      "2020-08-17 05:06:58,385 INFO: | epoch   2 | score (94.83, 94.39, 94.61) | f1 94.61 | loss 0.1550 | time 8450.74\n",
      "2020-08-17 05:06:58,714 INFO: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9530    0.9550    0.9540     35027\n",
      "          股票     0.9582    0.9594    0.9588     33251\n",
      "          体育     0.9886    0.9890    0.9888     28283\n",
      "          娱乐     0.9633    0.9734    0.9683     19920\n",
      "          时政     0.9178    0.9315    0.9246     13515\n",
      "          社会     0.9175    0.9118    0.9146     11009\n",
      "          教育     0.9586    0.9566    0.9576      8987\n",
      "          财经     0.9058    0.8863    0.8959      7957\n",
      "          家居     0.9477    0.9432    0.9454      7063\n",
      "          游戏     0.9432    0.9227    0.9328      5291\n",
      "          房产     0.9852    0.9745    0.9798      4428\n",
      "          时尚     0.9466    0.9308    0.9386      2818\n",
      "          彩票     0.9567    0.9298    0.9431      1639\n",
      "          星座     0.9346    0.9507    0.9426       812\n",
      "\n",
      "    accuracy                         0.9543    180000\n",
      "   macro avg     0.9483    0.9439    0.9461    180000\n",
      "weighted avg     0.9542    0.9543    0.9542    180000\n",
      "\n",
      "2020-08-17 05:17:31,771 INFO: | epoch   2 | dev | score (94.96, 95.25, 95.09) | f1 95.09 | time 633.05\n",
      "2020-08-17 05:17:31,815 INFO: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9628    0.9512    0.9569      3891\n",
      "          股票     0.9644    0.9618    0.9631      3694\n",
      "          体育     0.9898    0.9892    0.9895      3142\n",
      "          娱乐     0.9655    0.9738    0.9696      2213\n",
      "          时政     0.9134    0.9420    0.9275      1501\n",
      "          社会     0.9491    0.9141    0.9313      1223\n",
      "          教育     0.9417    0.9709    0.9561       998\n",
      "          财经     0.9185    0.8925    0.9053       884\n",
      "          家居     0.9424    0.9592    0.9507       784\n",
      "          游戏     0.9118    0.9506    0.9308       587\n",
      "          房产     0.9939    0.9858    0.9898       492\n",
      "          时尚     0.9361    0.9361    0.9361       313\n",
      "          彩票     0.9657    0.9286    0.9468       182\n",
      "          星座     0.9400    0.9792    0.9592        96\n",
      "\n",
      "    accuracy                         0.9579     20000\n",
      "   macro avg     0.9496    0.9525    0.9509     20000\n",
      "weighted avg     0.9581    0.9579    0.9579     20000\n",
      "\n",
      "2020-08-17 05:17:31,818 INFO: Exceed history dev = 94.63, current dev = 95.09\n",
      "2020-08-17 05:17:56,698 INFO: | epoch   3 | step 36050 | batch  50/18000 | lr 0.00000 0.00002 | loss 0.1146 | s/batch 0.50\n",
      "2020-08-17 05:18:17,168 INFO: | epoch   3 | step 36100 | batch 100/18000 | lr 0.00000 0.00002 | loss 0.0672 | s/batch 0.41\n",
      "2020-08-17 05:18:42,199 INFO: | epoch   3 | step 36150 | batch 150/18000 | lr 0.00000 0.00002 | loss 0.1423 | s/batch 0.50\n",
      "2020-08-17 05:19:04,098 INFO: | epoch   3 | step 36200 | batch 200/18000 | lr 0.00000 0.00002 | loss 0.1016 | s/batch 0.44\n",
      "2020-08-17 05:19:26,496 INFO: | epoch   3 | step 36250 | batch 250/18000 | lr 0.00000 0.00002 | loss 0.0870 | s/batch 0.45\n",
      "2020-08-17 05:19:51,339 INFO: | epoch   3 | step 36300 | batch 300/18000 | lr 0.00000 0.00002 | loss 0.1679 | s/batch 0.50\n",
      "2020-08-17 05:20:14,576 INFO: | epoch   3 | step 36350 | batch 350/18000 | lr 0.00000 0.00002 | loss 0.1478 | s/batch 0.46\n",
      "2020-08-17 05:20:37,786 INFO: | epoch   3 | step 36400 | batch 400/18000 | lr 0.00000 0.00002 | loss 0.0936 | s/batch 0.46\n",
      "2020-08-17 05:20:58,671 INFO: | epoch   3 | step 36450 | batch 450/18000 | lr 0.00000 0.00002 | loss 0.1309 | s/batch 0.42\n",
      "2020-08-17 05:21:22,692 INFO: | epoch   3 | step 36500 | batch 500/18000 | lr 0.00000 0.00002 | loss 0.1426 | s/batch 0.48\n",
      "2020-08-17 05:21:49,450 INFO: | epoch   3 | step 36550 | batch 550/18000 | lr 0.00000 0.00002 | loss 0.1160 | s/batch 0.54\n",
      "2020-08-17 05:22:13,593 INFO: | epoch   3 | step 36600 | batch 600/18000 | lr 0.00000 0.00002 | loss 0.1075 | s/batch 0.48\n",
      "2020-08-17 05:22:38,769 INFO: | epoch   3 | step 36650 | batch 650/18000 | lr 0.00000 0.00002 | loss 0.1134 | s/batch 0.50\n",
      "2020-08-17 05:23:03,590 INFO: | epoch   3 | step 36700 | batch 700/18000 | lr 0.00000 0.00002 | loss 0.1700 | s/batch 0.50\n",
      "2020-08-17 05:23:26,338 INFO: | epoch   3 | step 36750 | batch 750/18000 | lr 0.00000 0.00002 | loss 0.1652 | s/batch 0.45\n",
      "2020-08-17 05:23:47,799 INFO: | epoch   3 | step 36800 | batch 800/18000 | lr 0.00000 0.00002 | loss 0.1128 | s/batch 0.43\n",
      "2020-08-17 05:24:12,861 INFO: | epoch   3 | step 36850 | batch 850/18000 | lr 0.00000 0.00002 | loss 0.1131 | s/batch 0.50\n",
      "2020-08-17 05:24:34,598 INFO: | epoch   3 | step 36900 | batch 900/18000 | lr 0.00000 0.00002 | loss 0.1309 | s/batch 0.43\n",
      "2020-08-17 05:24:58,087 INFO: | epoch   3 | step 36950 | batch 950/18000 | lr 0.00000 0.00002 | loss 0.1202 | s/batch 0.47\n",
      "2020-08-17 05:25:18,148 INFO: | epoch   3 | step 37000 | batch 1000/18000 | lr 0.00000 0.00002 | loss 0.1396 | s/batch 0.40\n",
      "2020-08-17 05:25:41,944 INFO: | epoch   3 | step 37050 | batch 1050/18000 | lr 0.00000 0.00002 | loss 0.1399 | s/batch 0.48\n",
      "2020-08-17 05:26:03,832 INFO: | epoch   3 | step 37100 | batch 1100/18000 | lr 0.00000 0.00002 | loss 0.1160 | s/batch 0.44\n",
      "2020-08-17 05:26:26,771 INFO: | epoch   3 | step 37150 | batch 1150/18000 | lr 0.00000 0.00002 | loss 0.1538 | s/batch 0.46\n",
      "2020-08-17 05:26:47,990 INFO: | epoch   3 | step 37200 | batch 1200/18000 | lr 0.00000 0.00002 | loss 0.1464 | s/batch 0.42\n",
      "2020-08-17 05:27:10,976 INFO: | epoch   3 | step 37250 | batch 1250/18000 | lr 0.00000 0.00002 | loss 0.1548 | s/batch 0.46\n",
      "2020-08-17 05:27:38,328 INFO: | epoch   3 | step 37300 | batch 1300/18000 | lr 0.00000 0.00002 | loss 0.1479 | s/batch 0.55\n",
      "2020-08-17 05:28:01,915 INFO: | epoch   3 | step 37350 | batch 1350/18000 | lr 0.00000 0.00002 | loss 0.1499 | s/batch 0.47\n",
      "2020-08-17 05:28:25,101 INFO: | epoch   3 | step 37400 | batch 1400/18000 | lr 0.00000 0.00002 | loss 0.0742 | s/batch 0.46\n",
      "2020-08-17 05:28:49,823 INFO: | epoch   3 | step 37450 | batch 1450/18000 | lr 0.00000 0.00002 | loss 0.1600 | s/batch 0.49\n",
      "2020-08-17 05:29:14,997 INFO: | epoch   3 | step 37500 | batch 1500/18000 | lr 0.00000 0.00002 | loss 0.1098 | s/batch 0.50\n",
      "2020-08-17 05:29:40,132 INFO: | epoch   3 | step 37550 | batch 1550/18000 | lr 0.00000 0.00002 | loss 0.1687 | s/batch 0.50\n",
      "2020-08-17 05:30:02,864 INFO: | epoch   3 | step 37600 | batch 1600/18000 | lr 0.00000 0.00002 | loss 0.1439 | s/batch 0.45\n",
      "2020-08-17 05:30:27,511 INFO: | epoch   3 | step 37650 | batch 1650/18000 | lr 0.00000 0.00002 | loss 0.1651 | s/batch 0.49\n",
      "2020-08-17 05:30:49,904 INFO: | epoch   3 | step 37700 | batch 1700/18000 | lr 0.00000 0.00002 | loss 0.1539 | s/batch 0.45\n",
      "2020-08-17 05:31:11,594 INFO: | epoch   3 | step 37750 | batch 1750/18000 | lr 0.00000 0.00002 | loss 0.1110 | s/batch 0.43\n",
      "2020-08-17 05:31:37,239 INFO: | epoch   3 | step 37800 | batch 1800/18000 | lr 0.00000 0.00002 | loss 0.1959 | s/batch 0.51\n",
      "2020-08-17 05:32:01,200 INFO: | epoch   3 | step 37850 | batch 1850/18000 | lr 0.00000 0.00001 | loss 0.1059 | s/batch 0.48\n",
      "2020-08-17 05:32:23,616 INFO: | epoch   3 | step 37900 | batch 1900/18000 | lr 0.00000 0.00001 | loss 0.0702 | s/batch 0.45\n",
      "2020-08-17 05:32:47,117 INFO: | epoch   3 | step 37950 | batch 1950/18000 | lr 0.00000 0.00001 | loss 0.1429 | s/batch 0.47\n",
      "2020-08-17 05:33:10,270 INFO: | epoch   3 | step 38000 | batch 2000/18000 | lr 0.00000 0.00001 | loss 0.0843 | s/batch 0.46\n",
      "2020-08-17 05:33:32,368 INFO: | epoch   3 | step 38050 | batch 2050/18000 | lr 0.00000 0.00001 | loss 0.1186 | s/batch 0.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-17 05:33:56,907 INFO: | epoch   3 | step 38100 | batch 2100/18000 | lr 0.00000 0.00001 | loss 0.1230 | s/batch 0.49\n",
      "2020-08-17 05:34:18,350 INFO: | epoch   3 | step 38150 | batch 2150/18000 | lr 0.00000 0.00001 | loss 0.1182 | s/batch 0.43\n",
      "2020-08-17 05:34:42,526 INFO: | epoch   3 | step 38200 | batch 2200/18000 | lr 0.00000 0.00001 | loss 0.1258 | s/batch 0.48\n",
      "2020-08-17 05:35:04,720 INFO: | epoch   3 | step 38250 | batch 2250/18000 | lr 0.00000 0.00001 | loss 0.0904 | s/batch 0.44\n",
      "2020-08-17 05:35:28,100 INFO: | epoch   3 | step 38300 | batch 2300/18000 | lr 0.00000 0.00001 | loss 0.1052 | s/batch 0.47\n",
      "2020-08-17 05:35:51,352 INFO: | epoch   3 | step 38350 | batch 2350/18000 | lr 0.00000 0.00001 | loss 0.0846 | s/batch 0.46\n",
      "2020-08-17 05:36:16,817 INFO: | epoch   3 | step 38400 | batch 2400/18000 | lr 0.00000 0.00001 | loss 0.1018 | s/batch 0.51\n",
      "2020-08-17 05:36:40,079 INFO: | epoch   3 | step 38450 | batch 2450/18000 | lr 0.00000 0.00001 | loss 0.1351 | s/batch 0.47\n",
      "2020-08-17 05:36:59,203 INFO: | epoch   3 | step 38500 | batch 2500/18000 | lr 0.00000 0.00001 | loss 0.1413 | s/batch 0.38\n",
      "2020-08-17 05:37:22,217 INFO: | epoch   3 | step 38550 | batch 2550/18000 | lr 0.00000 0.00001 | loss 0.1514 | s/batch 0.46\n",
      "2020-08-17 05:37:44,181 INFO: | epoch   3 | step 38600 | batch 2600/18000 | lr 0.00000 0.00001 | loss 0.1334 | s/batch 0.44\n",
      "2020-08-17 05:38:06,368 INFO: | epoch   3 | step 38650 | batch 2650/18000 | lr 0.00000 0.00001 | loss 0.1053 | s/batch 0.44\n",
      "2020-08-17 05:38:29,466 INFO: | epoch   3 | step 38700 | batch 2700/18000 | lr 0.00000 0.00001 | loss 0.1108 | s/batch 0.46\n",
      "2020-08-17 05:38:54,057 INFO: | epoch   3 | step 38750 | batch 2750/18000 | lr 0.00000 0.00001 | loss 0.1279 | s/batch 0.49\n",
      "2020-08-17 05:39:16,478 INFO: | epoch   3 | step 38800 | batch 2800/18000 | lr 0.00000 0.00001 | loss 0.1318 | s/batch 0.45\n",
      "2020-08-17 05:39:39,887 INFO: | epoch   3 | step 38850 | batch 2850/18000 | lr 0.00000 0.00001 | loss 0.0644 | s/batch 0.47\n",
      "2020-08-17 05:40:05,603 INFO: | epoch   3 | step 38900 | batch 2900/18000 | lr 0.00000 0.00001 | loss 0.1487 | s/batch 0.51\n",
      "2020-08-17 05:40:30,049 INFO: | epoch   3 | step 38950 | batch 2950/18000 | lr 0.00000 0.00001 | loss 0.1802 | s/batch 0.49\n",
      "2020-08-17 05:40:54,259 INFO: | epoch   3 | step 39000 | batch 3000/18000 | lr 0.00000 0.00001 | loss 0.0810 | s/batch 0.48\n",
      "2020-08-17 05:41:18,067 INFO: | epoch   3 | step 39050 | batch 3050/18000 | lr 0.00000 0.00001 | loss 0.1076 | s/batch 0.48\n",
      "2020-08-17 05:41:41,499 INFO: | epoch   3 | step 39100 | batch 3100/18000 | lr 0.00000 0.00001 | loss 0.0678 | s/batch 0.47\n",
      "2020-08-17 05:42:04,277 INFO: | epoch   3 | step 39150 | batch 3150/18000 | lr 0.00000 0.00001 | loss 0.1617 | s/batch 0.46\n",
      "2020-08-17 05:42:27,227 INFO: | epoch   3 | step 39200 | batch 3200/18000 | lr 0.00000 0.00001 | loss 0.1094 | s/batch 0.46\n",
      "2020-08-17 05:42:49,584 INFO: | epoch   3 | step 39250 | batch 3250/18000 | lr 0.00000 0.00001 | loss 0.1208 | s/batch 0.45\n",
      "2020-08-17 05:43:12,695 INFO: | epoch   3 | step 39300 | batch 3300/18000 | lr 0.00000 0.00001 | loss 0.0848 | s/batch 0.46\n",
      "2020-08-17 05:43:38,682 INFO: | epoch   3 | step 39350 | batch 3350/18000 | lr 0.00000 0.00001 | loss 0.0941 | s/batch 0.52\n",
      "2020-08-17 05:44:03,011 INFO: | epoch   3 | step 39400 | batch 3400/18000 | lr 0.00000 0.00001 | loss 0.1191 | s/batch 0.49\n",
      "2020-08-17 05:44:25,035 INFO: | epoch   3 | step 39450 | batch 3450/18000 | lr 0.00000 0.00001 | loss 0.0602 | s/batch 0.44\n",
      "2020-08-17 05:44:49,561 INFO: | epoch   3 | step 39500 | batch 3500/18000 | lr 0.00000 0.00001 | loss 0.1202 | s/batch 0.49\n",
      "2020-08-17 05:45:14,124 INFO: | epoch   3 | step 39550 | batch 3550/18000 | lr 0.00000 0.00001 | loss 0.1753 | s/batch 0.49\n",
      "2020-08-17 05:45:38,029 INFO: | epoch   3 | step 39600 | batch 3600/18000 | lr 0.00000 0.00001 | loss 0.0991 | s/batch 0.48\n",
      "2020-08-17 05:46:01,248 INFO: | epoch   3 | step 39650 | batch 3650/18000 | lr 0.00000 0.00001 | loss 0.1061 | s/batch 0.46\n",
      "2020-08-17 05:46:25,718 INFO: | epoch   3 | step 39700 | batch 3700/18000 | lr 0.00000 0.00001 | loss 0.1529 | s/batch 0.49\n",
      "2020-08-17 05:46:49,525 INFO: | epoch   3 | step 39750 | batch 3750/18000 | lr 0.00000 0.00001 | loss 0.1310 | s/batch 0.48\n",
      "2020-08-17 05:47:14,089 INFO: | epoch   3 | step 39800 | batch 3800/18000 | lr 0.00000 0.00001 | loss 0.1067 | s/batch 0.49\n",
      "2020-08-17 05:47:39,852 INFO: | epoch   3 | step 39850 | batch 3850/18000 | lr 0.00000 0.00001 | loss 0.1218 | s/batch 0.52\n",
      "2020-08-17 05:48:03,519 INFO: | epoch   3 | step 39900 | batch 3900/18000 | lr 0.00000 0.00001 | loss 0.1013 | s/batch 0.47\n",
      "2020-08-17 05:48:25,329 INFO: | epoch   3 | step 39950 | batch 3950/18000 | lr 0.00000 0.00001 | loss 0.1517 | s/batch 0.44\n",
      "2020-08-17 05:48:48,408 INFO: | epoch   3 | step 40000 | batch 4000/18000 | lr 0.00000 0.00001 | loss 0.0975 | s/batch 0.46\n",
      "2020-08-17 05:49:13,143 INFO: | epoch   3 | step 40050 | batch 4050/18000 | lr 0.00000 0.00001 | loss 0.1381 | s/batch 0.49\n",
      "2020-08-17 05:49:35,035 INFO: | epoch   3 | step 40100 | batch 4100/18000 | lr 0.00000 0.00001 | loss 0.0906 | s/batch 0.44\n",
      "2020-08-17 05:49:59,008 INFO: | epoch   3 | step 40150 | batch 4150/18000 | lr 0.00000 0.00001 | loss 0.1093 | s/batch 0.48\n",
      "2020-08-17 05:50:24,888 INFO: | epoch   3 | step 40200 | batch 4200/18000 | lr 0.00000 0.00001 | loss 0.1106 | s/batch 0.52\n",
      "2020-08-17 05:50:47,172 INFO: | epoch   3 | step 40250 | batch 4250/18000 | lr 0.00000 0.00001 | loss 0.0953 | s/batch 0.45\n",
      "2020-08-17 05:51:10,188 INFO: | epoch   3 | step 40300 | batch 4300/18000 | lr 0.00000 0.00001 | loss 0.0886 | s/batch 0.46\n",
      "2020-08-17 05:51:33,626 INFO: | epoch   3 | step 40350 | batch 4350/18000 | lr 0.00000 0.00001 | loss 0.1510 | s/batch 0.47\n",
      "2020-08-17 05:51:55,654 INFO: | epoch   3 | step 40400 | batch 4400/18000 | lr 0.00000 0.00001 | loss 0.1697 | s/batch 0.44\n",
      "2020-08-17 05:52:17,003 INFO: | epoch   3 | step 40450 | batch 4450/18000 | lr 0.00000 0.00001 | loss 0.1160 | s/batch 0.43\n",
      "2020-08-17 05:52:40,677 INFO: | epoch   3 | step 40500 | batch 4500/18000 | lr 0.00000 0.00001 | loss 0.1712 | s/batch 0.47\n",
      "2020-08-17 05:53:07,414 INFO: | epoch   3 | step 40550 | batch 4550/18000 | lr 0.00000 0.00001 | loss 0.1446 | s/batch 0.53\n",
      "2020-08-17 05:53:30,605 INFO: | epoch   3 | step 40600 | batch 4600/18000 | lr 0.00000 0.00001 | loss 0.1049 | s/batch 0.46\n",
      "2020-08-17 05:53:50,573 INFO: | epoch   3 | step 40650 | batch 4650/18000 | lr 0.00000 0.00001 | loss 0.0930 | s/batch 0.40\n",
      "2020-08-17 05:54:11,436 INFO: | epoch   3 | step 40700 | batch 4700/18000 | lr 0.00000 0.00001 | loss 0.1294 | s/batch 0.42\n",
      "2020-08-17 05:54:33,066 INFO: | epoch   3 | step 40750 | batch 4750/18000 | lr 0.00000 0.00001 | loss 0.0876 | s/batch 0.43\n",
      "2020-08-17 05:54:55,919 INFO: | epoch   3 | step 40800 | batch 4800/18000 | lr 0.00000 0.00001 | loss 0.1457 | s/batch 0.46\n",
      "2020-08-17 05:55:20,721 INFO: | epoch   3 | step 40850 | batch 4850/18000 | lr 0.00000 0.00001 | loss 0.0919 | s/batch 0.50\n",
      "2020-08-17 05:55:44,672 INFO: | epoch   3 | step 40900 | batch 4900/18000 | lr 0.00000 0.00001 | loss 0.1311 | s/batch 0.48\n",
      "2020-08-17 05:56:10,066 INFO: | epoch   3 | step 40950 | batch 4950/18000 | lr 0.00000 0.00001 | loss 0.1664 | s/batch 0.51\n",
      "2020-08-17 05:56:35,641 INFO: | epoch   3 | step 41000 | batch 5000/18000 | lr 0.00000 0.00001 | loss 0.0749 | s/batch 0.51\n",
      "2020-08-17 05:56:56,290 INFO: | epoch   3 | step 41050 | batch 5050/18000 | lr 0.00000 0.00001 | loss 0.1860 | s/batch 0.41\n",
      "2020-08-17 05:57:18,633 INFO: | epoch   3 | step 41100 | batch 5100/18000 | lr 0.00000 0.00001 | loss 0.1529 | s/batch 0.45\n",
      "2020-08-17 05:57:43,244 INFO: | epoch   3 | step 41150 | batch 5150/18000 | lr 0.00000 0.00001 | loss 0.1258 | s/batch 0.49\n",
      "2020-08-17 05:58:08,289 INFO: | epoch   3 | step 41200 | batch 5200/18000 | lr 0.00000 0.00001 | loss 0.1006 | s/batch 0.50\n",
      "2020-08-17 05:58:33,884 INFO: | epoch   3 | step 41250 | batch 5250/18000 | lr 0.00000 0.00001 | loss 0.1827 | s/batch 0.51\n",
      "2020-08-17 05:58:58,541 INFO: | epoch   3 | step 41300 | batch 5300/18000 | lr 0.00000 0.00001 | loss 0.1688 | s/batch 0.49\n",
      "2020-08-17 05:59:20,783 INFO: | epoch   3 | step 41350 | batch 5350/18000 | lr 0.00000 0.00001 | loss 0.0927 | s/batch 0.44\n",
      "2020-08-17 05:59:40,564 INFO: | epoch   3 | step 41400 | batch 5400/18000 | lr 0.00000 0.00001 | loss 0.1236 | s/batch 0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-17 06:00:06,072 INFO: | epoch   3 | step 41450 | batch 5450/18000 | lr 0.00000 0.00001 | loss 0.1017 | s/batch 0.51\n",
      "2020-08-17 06:00:29,298 INFO: | epoch   3 | step 41500 | batch 5500/18000 | lr 0.00000 0.00001 | loss 0.0918 | s/batch 0.46\n",
      "2020-08-17 06:00:52,472 INFO: | epoch   3 | step 41550 | batch 5550/18000 | lr 0.00000 0.00001 | loss 0.1077 | s/batch 0.46\n",
      "2020-08-17 06:01:18,198 INFO: | epoch   3 | step 41600 | batch 5600/18000 | lr 0.00000 0.00001 | loss 0.1886 | s/batch 0.51\n",
      "2020-08-17 06:01:42,610 INFO: | epoch   3 | step 41650 | batch 5650/18000 | lr 0.00000 0.00001 | loss 0.1408 | s/batch 0.49\n",
      "2020-08-17 06:02:03,974 INFO: | epoch   3 | step 41700 | batch 5700/18000 | lr 0.00000 0.00001 | loss 0.0961 | s/batch 0.43\n",
      "2020-08-17 06:02:29,535 INFO: | epoch   3 | step 41750 | batch 5750/18000 | lr 0.00000 0.00001 | loss 0.1383 | s/batch 0.51\n",
      "2020-08-17 06:02:51,392 INFO: | epoch   3 | step 41800 | batch 5800/18000 | lr 0.00000 0.00001 | loss 0.0928 | s/batch 0.44\n",
      "2020-08-17 06:03:14,099 INFO: | epoch   3 | step 41850 | batch 5850/18000 | lr 0.00000 0.00001 | loss 0.1262 | s/batch 0.45\n",
      "2020-08-17 06:03:37,061 INFO: | epoch   3 | step 41900 | batch 5900/18000 | lr 0.00000 0.00001 | loss 0.1414 | s/batch 0.46\n",
      "2020-08-17 06:03:59,931 INFO: | epoch   3 | step 41950 | batch 5950/18000 | lr 0.00000 0.00001 | loss 0.0793 | s/batch 0.46\n",
      "2020-08-17 06:04:22,685 INFO: | epoch   3 | step 42000 | batch 6000/18000 | lr 0.00000 0.00001 | loss 0.0916 | s/batch 0.46\n",
      "2020-08-17 06:04:45,665 INFO: | epoch   3 | step 42050 | batch 6050/18000 | lr 0.00000 0.00001 | loss 0.1352 | s/batch 0.46\n",
      "2020-08-17 06:05:11,054 INFO: | epoch   3 | step 42100 | batch 6100/18000 | lr 0.00000 0.00001 | loss 0.1448 | s/batch 0.51\n",
      "2020-08-17 06:05:34,215 INFO: | epoch   3 | step 42150 | batch 6150/18000 | lr 0.00000 0.00001 | loss 0.1222 | s/batch 0.46\n",
      "2020-08-17 06:05:58,411 INFO: | epoch   3 | step 42200 | batch 6200/18000 | lr 0.00000 0.00001 | loss 0.1343 | s/batch 0.48\n",
      "2020-08-17 06:06:25,684 INFO: | epoch   3 | step 42250 | batch 6250/18000 | lr 0.00000 0.00001 | loss 0.1416 | s/batch 0.55\n",
      "2020-08-17 06:06:49,138 INFO: | epoch   3 | step 42300 | batch 6300/18000 | lr 0.00000 0.00001 | loss 0.0747 | s/batch 0.47\n",
      "2020-08-17 06:07:12,456 INFO: | epoch   3 | step 42350 | batch 6350/18000 | lr 0.00000 0.00001 | loss 0.0884 | s/batch 0.47\n",
      "2020-08-17 06:07:34,730 INFO: | epoch   3 | step 42400 | batch 6400/18000 | lr 0.00000 0.00001 | loss 0.0953 | s/batch 0.45\n",
      "2020-08-17 06:07:58,805 INFO: | epoch   3 | step 42450 | batch 6450/18000 | lr 0.00000 0.00001 | loss 0.1073 | s/batch 0.48\n",
      "2020-08-17 06:08:22,337 INFO: | epoch   3 | step 42500 | batch 6500/18000 | lr 0.00000 0.00001 | loss 0.1156 | s/batch 0.47\n",
      "2020-08-17 06:08:45,069 INFO: | epoch   3 | step 42550 | batch 6550/18000 | lr 0.00000 0.00001 | loss 0.0642 | s/batch 0.45\n",
      "2020-08-17 06:09:10,680 INFO: | epoch   3 | step 42600 | batch 6600/18000 | lr 0.00000 0.00001 | loss 0.1311 | s/batch 0.51\n",
      "2020-08-17 06:09:32,468 INFO: | epoch   3 | step 42650 | batch 6650/18000 | lr 0.00000 0.00001 | loss 0.1807 | s/batch 0.44\n",
      "2020-08-17 06:09:54,925 INFO: | epoch   3 | step 42700 | batch 6700/18000 | lr 0.00000 0.00001 | loss 0.1451 | s/batch 0.45\n",
      "2020-08-17 06:10:16,651 INFO: | epoch   3 | step 42750 | batch 6750/18000 | lr 0.00000 0.00001 | loss 0.1364 | s/batch 0.43\n",
      "2020-08-17 06:10:39,090 INFO: | epoch   3 | step 42800 | batch 6800/18000 | lr 0.00000 0.00001 | loss 0.1284 | s/batch 0.45\n",
      "2020-08-17 06:11:00,879 INFO: | epoch   3 | step 42850 | batch 6850/18000 | lr 0.00000 0.00001 | loss 0.1787 | s/batch 0.44\n",
      "2020-08-17 06:11:23,521 INFO: | epoch   3 | step 42900 | batch 6900/18000 | lr 0.00000 0.00001 | loss 0.1217 | s/batch 0.45\n",
      "2020-08-17 06:11:47,117 INFO: | epoch   3 | step 42950 | batch 6950/18000 | lr 0.00000 0.00001 | loss 0.1287 | s/batch 0.47\n",
      "2020-08-17 06:12:10,252 INFO: | epoch   3 | step 43000 | batch 7000/18000 | lr 0.00000 0.00001 | loss 0.0559 | s/batch 0.46\n",
      "2020-08-17 06:12:31,567 INFO: | epoch   3 | step 43050 | batch 7050/18000 | lr 0.00000 0.00001 | loss 0.0953 | s/batch 0.43\n",
      "2020-08-17 06:12:53,381 INFO: | epoch   3 | step 43100 | batch 7100/18000 | lr 0.00000 0.00001 | loss 0.1227 | s/batch 0.44\n",
      "2020-08-17 06:13:20,725 INFO: | epoch   3 | step 43150 | batch 7150/18000 | lr 0.00000 0.00001 | loss 0.1466 | s/batch 0.55\n",
      "2020-08-17 06:13:42,661 INFO: | epoch   3 | step 43200 | batch 7200/18000 | lr 0.00000 0.00001 | loss 0.1244 | s/batch 0.44\n",
      "2020-08-17 06:14:08,081 INFO: | epoch   3 | step 43250 | batch 7250/18000 | lr 0.00000 0.00001 | loss 0.1306 | s/batch 0.51\n",
      "2020-08-17 06:14:31,111 INFO: | epoch   3 | step 43300 | batch 7300/18000 | lr 0.00000 0.00001 | loss 0.1545 | s/batch 0.46\n",
      "2020-08-17 06:14:55,268 INFO: | epoch   3 | step 43350 | batch 7350/18000 | lr 0.00000 0.00001 | loss 0.1285 | s/batch 0.48\n",
      "2020-08-17 06:15:17,622 INFO: | epoch   3 | step 43400 | batch 7400/18000 | lr 0.00000 0.00001 | loss 0.1123 | s/batch 0.45\n",
      "2020-08-17 06:15:42,134 INFO: | epoch   3 | step 43450 | batch 7450/18000 | lr 0.00000 0.00001 | loss 0.1125 | s/batch 0.49\n",
      "2020-08-17 06:16:04,267 INFO: | epoch   3 | step 43500 | batch 7500/18000 | lr 0.00000 0.00001 | loss 0.1486 | s/batch 0.44\n",
      "2020-08-17 06:16:28,632 INFO: | epoch   3 | step 43550 | batch 7550/18000 | lr 0.00000 0.00001 | loss 0.1326 | s/batch 0.49\n",
      "2020-08-17 06:16:51,407 INFO: | epoch   3 | step 43600 | batch 7600/18000 | lr 0.00000 0.00001 | loss 0.1139 | s/batch 0.46\n",
      "2020-08-17 06:17:14,785 INFO: | epoch   3 | step 43650 | batch 7650/18000 | lr 0.00000 0.00001 | loss 0.1702 | s/batch 0.47\n",
      "2020-08-17 06:17:36,064 INFO: | epoch   3 | step 43700 | batch 7700/18000 | lr 0.00000 0.00001 | loss 0.0917 | s/batch 0.43\n",
      "2020-08-17 06:18:00,479 INFO: | epoch   3 | step 43750 | batch 7750/18000 | lr 0.00000 0.00001 | loss 0.0840 | s/batch 0.49\n",
      "2020-08-17 06:18:19,928 INFO: | epoch   3 | step 43800 | batch 7800/18000 | lr 0.00000 0.00001 | loss 0.0974 | s/batch 0.39\n",
      "2020-08-17 06:18:40,438 INFO: | epoch   3 | step 43850 | batch 7850/18000 | lr 0.00000 0.00001 | loss 0.0907 | s/batch 0.41\n",
      "2020-08-17 06:19:04,924 INFO: | epoch   3 | step 43900 | batch 7900/18000 | lr 0.00000 0.00001 | loss 0.1133 | s/batch 0.49\n",
      "2020-08-17 06:19:25,501 INFO: | epoch   3 | step 43950 | batch 7950/18000 | lr 0.00000 0.00001 | loss 0.0970 | s/batch 0.41\n",
      "2020-08-17 06:19:48,725 INFO: | epoch   3 | step 44000 | batch 8000/18000 | lr 0.00000 0.00001 | loss 0.0720 | s/batch 0.46\n",
      "2020-08-17 06:20:10,245 INFO: | epoch   3 | step 44050 | batch 8050/18000 | lr 0.00000 0.00001 | loss 0.1128 | s/batch 0.43\n",
      "2020-08-17 06:20:32,605 INFO: | epoch   3 | step 44100 | batch 8100/18000 | lr 0.00000 0.00001 | loss 0.1076 | s/batch 0.45\n",
      "2020-08-17 06:20:57,327 INFO: | epoch   3 | step 44150 | batch 8150/18000 | lr 0.00000 0.00001 | loss 0.0878 | s/batch 0.49\n",
      "2020-08-17 06:21:20,956 INFO: | epoch   3 | step 44200 | batch 8200/18000 | lr 0.00000 0.00001 | loss 0.1569 | s/batch 0.47\n",
      "2020-08-17 06:21:43,964 INFO: | epoch   3 | step 44250 | batch 8250/18000 | lr 0.00000 0.00001 | loss 0.1136 | s/batch 0.46\n",
      "2020-08-17 06:22:07,764 INFO: | epoch   3 | step 44300 | batch 8300/18000 | lr 0.00000 0.00001 | loss 0.1125 | s/batch 0.48\n",
      "2020-08-17 06:22:27,924 INFO: | epoch   3 | step 44350 | batch 8350/18000 | lr 0.00000 0.00001 | loss 0.0907 | s/batch 0.40\n",
      "2020-08-17 06:22:51,555 INFO: | epoch   3 | step 44400 | batch 8400/18000 | lr 0.00000 0.00001 | loss 0.0781 | s/batch 0.47\n",
      "2020-08-17 06:23:15,598 INFO: | epoch   3 | step 44450 | batch 8450/18000 | lr 0.00000 0.00001 | loss 0.1022 | s/batch 0.48\n",
      "2020-08-17 06:23:37,842 INFO: | epoch   3 | step 44500 | batch 8500/18000 | lr 0.00000 0.00001 | loss 0.0914 | s/batch 0.44\n",
      "2020-08-17 06:24:01,863 INFO: | epoch   3 | step 44550 | batch 8550/18000 | lr 0.00000 0.00001 | loss 0.1353 | s/batch 0.48\n",
      "2020-08-17 06:24:25,269 INFO: | epoch   3 | step 44600 | batch 8600/18000 | lr 0.00000 0.00001 | loss 0.1356 | s/batch 0.47\n",
      "2020-08-17 06:24:46,099 INFO: | epoch   3 | step 44650 | batch 8650/18000 | lr 0.00000 0.00001 | loss 0.1046 | s/batch 0.42\n",
      "2020-08-17 06:25:11,844 INFO: | epoch   3 | step 44700 | batch 8700/18000 | lr 0.00000 0.00001 | loss 0.1811 | s/batch 0.51\n",
      "2020-08-17 06:25:34,497 INFO: | epoch   3 | step 44750 | batch 8750/18000 | lr 0.00000 0.00001 | loss 0.1335 | s/batch 0.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-17 06:25:57,484 INFO: | epoch   3 | step 44800 | batch 8800/18000 | lr 0.00000 0.00001 | loss 0.1128 | s/batch 0.46\n",
      "2020-08-17 06:26:20,408 INFO: | epoch   3 | step 44850 | batch 8850/18000 | lr 0.00000 0.00001 | loss 0.0690 | s/batch 0.46\n",
      "2020-08-17 06:26:46,383 INFO: | epoch   3 | step 44900 | batch 8900/18000 | lr 0.00000 0.00001 | loss 0.1429 | s/batch 0.52\n",
      "2020-08-17 06:27:08,149 INFO: | epoch   3 | step 44950 | batch 8950/18000 | lr 0.00000 0.00001 | loss 0.1164 | s/batch 0.44\n",
      "2020-08-17 06:27:30,041 INFO: | epoch   3 | step 45000 | batch 9000/18000 | lr 0.00000 0.00001 | loss 0.1058 | s/batch 0.44\n",
      "2020-08-17 06:27:53,564 INFO: | epoch   3 | step 45050 | batch 9050/18000 | lr 0.00000 0.00001 | loss 0.1134 | s/batch 0.47\n",
      "2020-08-17 06:28:17,108 INFO: | epoch   3 | step 45100 | batch 9100/18000 | lr 0.00000 0.00001 | loss 0.1267 | s/batch 0.47\n",
      "2020-08-17 06:28:39,107 INFO: | epoch   3 | step 45150 | batch 9150/18000 | lr 0.00000 0.00001 | loss 0.0868 | s/batch 0.44\n",
      "2020-08-17 06:29:03,968 INFO: | epoch   3 | step 45200 | batch 9200/18000 | lr 0.00000 0.00001 | loss 0.1176 | s/batch 0.50\n",
      "2020-08-17 06:29:29,620 INFO: | epoch   3 | step 45250 | batch 9250/18000 | lr 0.00000 0.00001 | loss 0.0787 | s/batch 0.51\n",
      "2020-08-17 06:29:51,708 INFO: | epoch   3 | step 45300 | batch 9300/18000 | lr 0.00000 0.00001 | loss 0.1196 | s/batch 0.44\n",
      "2020-08-17 06:30:13,099 INFO: | epoch   3 | step 45350 | batch 9350/18000 | lr 0.00000 0.00001 | loss 0.0482 | s/batch 0.43\n",
      "2020-08-17 06:30:35,838 INFO: | epoch   3 | step 45400 | batch 9400/18000 | lr 0.00000 0.00001 | loss 0.1645 | s/batch 0.45\n",
      "2020-08-17 06:30:59,082 INFO: | epoch   3 | step 45450 | batch 9450/18000 | lr 0.00000 0.00001 | loss 0.1508 | s/batch 0.46\n",
      "2020-08-17 06:31:25,602 INFO: | epoch   3 | step 45500 | batch 9500/18000 | lr 0.00000 0.00001 | loss 0.1398 | s/batch 0.53\n",
      "2020-08-17 06:31:50,209 INFO: | epoch   3 | step 45550 | batch 9550/18000 | lr 0.00000 0.00001 | loss 0.1172 | s/batch 0.49\n",
      "2020-08-17 06:32:16,179 INFO: | epoch   3 | step 45600 | batch 9600/18000 | lr 0.00000 0.00001 | loss 0.1065 | s/batch 0.52\n",
      "2020-08-17 06:32:36,326 INFO: | epoch   3 | step 45650 | batch 9650/18000 | lr 0.00000 0.00001 | loss 0.0768 | s/batch 0.40\n",
      "2020-08-17 06:32:58,713 INFO: | epoch   3 | step 45700 | batch 9700/18000 | lr 0.00000 0.00001 | loss 0.0823 | s/batch 0.45\n",
      "2020-08-17 06:33:20,309 INFO: | epoch   3 | step 45750 | batch 9750/18000 | lr 0.00000 0.00001 | loss 0.1453 | s/batch 0.43\n",
      "2020-08-17 06:33:43,091 INFO: | epoch   3 | step 45800 | batch 9800/18000 | lr 0.00000 0.00001 | loss 0.1485 | s/batch 0.46\n",
      "2020-08-17 06:34:08,886 INFO: | epoch   3 | step 45850 | batch 9850/18000 | lr 0.00000 0.00001 | loss 0.1569 | s/batch 0.52\n",
      "2020-08-17 06:34:32,876 INFO: | epoch   3 | step 45900 | batch 9900/18000 | lr 0.00000 0.00001 | loss 0.1610 | s/batch 0.48\n",
      "2020-08-17 06:34:56,383 INFO: | epoch   3 | step 45950 | batch 9950/18000 | lr 0.00000 0.00001 | loss 0.1339 | s/batch 0.47\n",
      "2020-08-17 06:35:21,916 INFO: | epoch   3 | step 46000 | batch 10000/18000 | lr 0.00000 0.00001 | loss 0.1495 | s/batch 0.51\n",
      "2020-08-17 06:35:47,171 INFO: | epoch   3 | step 46050 | batch 10050/18000 | lr 0.00000 0.00001 | loss 0.1617 | s/batch 0.51\n",
      "2020-08-17 06:36:10,049 INFO: | epoch   3 | step 46100 | batch 10100/18000 | lr 0.00000 0.00001 | loss 0.1294 | s/batch 0.46\n",
      "2020-08-17 06:36:33,748 INFO: | epoch   3 | step 46150 | batch 10150/18000 | lr 0.00000 0.00001 | loss 0.1020 | s/batch 0.47\n",
      "2020-08-17 06:36:56,703 INFO: | epoch   3 | step 46200 | batch 10200/18000 | lr 0.00000 0.00001 | loss 0.1300 | s/batch 0.46\n",
      "2020-08-17 06:37:20,281 INFO: | epoch   3 | step 46250 | batch 10250/18000 | lr 0.00000 0.00001 | loss 0.0962 | s/batch 0.47\n",
      "2020-08-17 06:37:43,345 INFO: | epoch   3 | step 46300 | batch 10300/18000 | lr 0.00000 0.00001 | loss 0.0758 | s/batch 0.46\n",
      "2020-08-17 06:38:05,572 INFO: | epoch   3 | step 46350 | batch 10350/18000 | lr 0.00000 0.00001 | loss 0.1328 | s/batch 0.44\n",
      "2020-08-17 06:38:27,843 INFO: | epoch   3 | step 46400 | batch 10400/18000 | lr 0.00000 0.00001 | loss 0.0913 | s/batch 0.45\n",
      "2020-08-17 06:38:50,534 INFO: | epoch   3 | step 46450 | batch 10450/18000 | lr 0.00000 0.00001 | loss 0.1106 | s/batch 0.45\n",
      "2020-08-17 06:39:13,407 INFO: | epoch   3 | step 46500 | batch 10500/18000 | lr 0.00000 0.00001 | loss 0.1333 | s/batch 0.46\n",
      "2020-08-17 06:39:38,757 INFO: | epoch   3 | step 46550 | batch 10550/18000 | lr 0.00000 0.00001 | loss 0.0771 | s/batch 0.51\n",
      "2020-08-17 06:39:59,462 INFO: | epoch   3 | step 46600 | batch 10600/18000 | lr 0.00000 0.00001 | loss 0.0398 | s/batch 0.41\n",
      "2020-08-17 06:40:24,855 INFO: | epoch   3 | step 46650 | batch 10650/18000 | lr 0.00000 0.00001 | loss 0.1436 | s/batch 0.51\n",
      "2020-08-17 06:40:48,469 INFO: | epoch   3 | step 46700 | batch 10700/18000 | lr 0.00000 0.00001 | loss 0.1192 | s/batch 0.47\n",
      "2020-08-17 06:41:14,432 INFO: | epoch   3 | step 46750 | batch 10750/18000 | lr 0.00000 0.00001 | loss 0.1082 | s/batch 0.52\n",
      "2020-08-17 06:41:37,580 INFO: | epoch   3 | step 46800 | batch 10800/18000 | lr 0.00000 0.00001 | loss 0.1146 | s/batch 0.46\n",
      "2020-08-17 06:42:01,798 INFO: | epoch   3 | step 46850 | batch 10850/18000 | lr 0.00000 0.00001 | loss 0.1194 | s/batch 0.48\n",
      "2020-08-17 06:42:24,044 INFO: | epoch   3 | step 46900 | batch 10900/18000 | lr 0.00000 0.00001 | loss 0.0748 | s/batch 0.44\n",
      "2020-08-17 06:42:46,813 INFO: | epoch   3 | step 46950 | batch 10950/18000 | lr 0.00000 0.00001 | loss 0.1144 | s/batch 0.46\n",
      "2020-08-17 06:43:07,967 INFO: | epoch   3 | step 47000 | batch 11000/18000 | lr 0.00000 0.00001 | loss 0.1609 | s/batch 0.42\n",
      "2020-08-17 06:43:30,139 INFO: | epoch   3 | step 47050 | batch 11050/18000 | lr 0.00000 0.00001 | loss 0.1665 | s/batch 0.44\n",
      "2020-08-17 06:43:52,880 INFO: | epoch   3 | step 47100 | batch 11100/18000 | lr 0.00000 0.00001 | loss 0.1325 | s/batch 0.45\n",
      "2020-08-17 06:44:17,139 INFO: | epoch   3 | step 47150 | batch 11150/18000 | lr 0.00000 0.00001 | loss 0.1242 | s/batch 0.49\n",
      "2020-08-17 06:44:38,338 INFO: | epoch   3 | step 47200 | batch 11200/18000 | lr 0.00000 0.00001 | loss 0.1241 | s/batch 0.42\n",
      "2020-08-17 06:45:03,806 INFO: | epoch   3 | step 47250 | batch 11250/18000 | lr 0.00000 0.00001 | loss 0.1622 | s/batch 0.51\n",
      "2020-08-17 06:45:27,225 INFO: | epoch   3 | step 47300 | batch 11300/18000 | lr 0.00000 0.00001 | loss 0.1537 | s/batch 0.47\n",
      "2020-08-17 06:45:50,705 INFO: | epoch   3 | step 47350 | batch 11350/18000 | lr 0.00000 0.00001 | loss 0.1416 | s/batch 0.47\n",
      "2020-08-17 06:46:13,632 INFO: | epoch   3 | step 47400 | batch 11400/18000 | lr 0.00000 0.00001 | loss 0.1046 | s/batch 0.46\n",
      "2020-08-17 06:46:35,887 INFO: | epoch   3 | step 47450 | batch 11450/18000 | lr 0.00000 0.00001 | loss 0.1194 | s/batch 0.45\n",
      "2020-08-17 06:46:57,829 INFO: | epoch   3 | step 47500 | batch 11500/18000 | lr 0.00000 0.00001 | loss 0.1073 | s/batch 0.44\n",
      "2020-08-17 06:47:24,778 INFO: | epoch   3 | step 47550 | batch 11550/18000 | lr 0.00000 0.00001 | loss 0.1257 | s/batch 0.54\n",
      "2020-08-17 06:47:46,395 INFO: | epoch   3 | step 47600 | batch 11600/18000 | lr 0.00000 0.00001 | loss 0.1652 | s/batch 0.43\n",
      "2020-08-17 06:48:08,202 INFO: | epoch   3 | step 47650 | batch 11650/18000 | lr 0.00000 0.00001 | loss 0.0856 | s/batch 0.44\n",
      "2020-08-17 06:48:34,489 INFO: | epoch   3 | step 47700 | batch 11700/18000 | lr 0.00000 0.00001 | loss 0.0735 | s/batch 0.53\n",
      "2020-08-17 06:48:59,118 INFO: | epoch   3 | step 47750 | batch 11750/18000 | lr 0.00000 0.00001 | loss 0.1045 | s/batch 0.49\n",
      "2020-08-17 06:49:20,109 INFO: | epoch   3 | step 47800 | batch 11800/18000 | lr 0.00000 0.00001 | loss 0.0897 | s/batch 0.42\n",
      "2020-08-17 06:49:43,025 INFO: | epoch   3 | step 47850 | batch 11850/18000 | lr 0.00000 0.00001 | loss 0.1446 | s/batch 0.46\n",
      "2020-08-17 06:50:05,537 INFO: | epoch   3 | step 47900 | batch 11900/18000 | lr 0.00000 0.00001 | loss 0.1342 | s/batch 0.45\n",
      "2020-08-17 06:50:31,063 INFO: | epoch   3 | step 47950 | batch 11950/18000 | lr 0.00000 0.00001 | loss 0.1467 | s/batch 0.51\n",
      "2020-08-17 06:50:57,858 INFO: | epoch   3 | step 48000 | batch 12000/18000 | lr 0.00000 0.00001 | loss 0.0944 | s/batch 0.54\n",
      "2020-08-17 06:51:21,679 INFO: | epoch   3 | step 48050 | batch 12050/18000 | lr 0.00000 0.00001 | loss 0.1511 | s/batch 0.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-17 06:51:44,165 INFO: | epoch   3 | step 48100 | batch 12100/18000 | lr 0.00000 0.00001 | loss 0.1351 | s/batch 0.45\n",
      "2020-08-17 06:52:08,436 INFO: | epoch   3 | step 48150 | batch 12150/18000 | lr 0.00000 0.00001 | loss 0.1154 | s/batch 0.49\n",
      "2020-08-17 06:52:29,198 INFO: | epoch   3 | step 48200 | batch 12200/18000 | lr 0.00000 0.00001 | loss 0.1429 | s/batch 0.42\n",
      "2020-08-17 06:52:50,751 INFO: | epoch   3 | step 48250 | batch 12250/18000 | lr 0.00000 0.00001 | loss 0.1093 | s/batch 0.43\n",
      "2020-08-17 06:53:15,721 INFO: | epoch   3 | step 48300 | batch 12300/18000 | lr 0.00000 0.00001 | loss 0.0878 | s/batch 0.50\n",
      "2020-08-17 06:53:40,237 INFO: | epoch   3 | step 48350 | batch 12350/18000 | lr 0.00000 0.00001 | loss 0.1126 | s/batch 0.49\n",
      "2020-08-17 06:54:03,896 INFO: | epoch   3 | step 48400 | batch 12400/18000 | lr 0.00000 0.00001 | loss 0.0648 | s/batch 0.47\n",
      "2020-08-17 06:54:24,894 INFO: | epoch   3 | step 48450 | batch 12450/18000 | lr 0.00000 0.00001 | loss 0.1266 | s/batch 0.42\n",
      "2020-08-17 06:54:52,444 INFO: | epoch   3 | step 48500 | batch 12500/18000 | lr 0.00000 0.00001 | loss 0.0965 | s/batch 0.55\n",
      "2020-08-17 06:55:17,854 INFO: | epoch   3 | step 48550 | batch 12550/18000 | lr 0.00000 0.00001 | loss 0.1345 | s/batch 0.51\n",
      "2020-08-17 06:55:44,445 INFO: | epoch   3 | step 48600 | batch 12600/18000 | lr 0.00000 0.00001 | loss 0.1351 | s/batch 0.53\n",
      "2020-08-17 06:56:08,901 INFO: | epoch   3 | step 48650 | batch 12650/18000 | lr 0.00000 0.00000 | loss 0.1432 | s/batch 0.49\n",
      "2020-08-17 06:56:33,539 INFO: | epoch   3 | step 48700 | batch 12700/18000 | lr 0.00000 0.00000 | loss 0.0953 | s/batch 0.49\n",
      "2020-08-17 06:56:55,808 INFO: | epoch   3 | step 48750 | batch 12750/18000 | lr 0.00000 0.00000 | loss 0.1264 | s/batch 0.45\n",
      "2020-08-17 06:57:20,013 INFO: | epoch   3 | step 48800 | batch 12800/18000 | lr 0.00000 0.00000 | loss 0.0869 | s/batch 0.48\n",
      "2020-08-17 06:57:40,751 INFO: | epoch   3 | step 48850 | batch 12850/18000 | lr 0.00000 0.00000 | loss 0.0877 | s/batch 0.41\n",
      "2020-08-17 06:58:05,613 INFO: | epoch   3 | step 48900 | batch 12900/18000 | lr 0.00000 0.00000 | loss 0.1146 | s/batch 0.50\n",
      "2020-08-17 06:58:31,090 INFO: | epoch   3 | step 48950 | batch 12950/18000 | lr 0.00000 0.00000 | loss 0.1278 | s/batch 0.51\n",
      "2020-08-17 06:58:56,449 INFO: | epoch   3 | step 49000 | batch 13000/18000 | lr 0.00000 0.00000 | loss 0.1168 | s/batch 0.51\n",
      "2020-08-17 06:59:21,007 INFO: | epoch   3 | step 49050 | batch 13050/18000 | lr 0.00000 0.00000 | loss 0.1307 | s/batch 0.49\n",
      "2020-08-17 06:59:45,573 INFO: | epoch   3 | step 49100 | batch 13100/18000 | lr 0.00000 0.00000 | loss 0.1221 | s/batch 0.49\n",
      "2020-08-17 07:00:10,798 INFO: | epoch   3 | step 49150 | batch 13150/18000 | lr 0.00000 0.00000 | loss 0.1050 | s/batch 0.50\n",
      "2020-08-17 07:00:36,820 INFO: | epoch   3 | step 49200 | batch 13200/18000 | lr 0.00000 0.00000 | loss 0.1064 | s/batch 0.52\n",
      "2020-08-17 07:00:58,729 INFO: | epoch   3 | step 49250 | batch 13250/18000 | lr 0.00000 0.00000 | loss 0.0729 | s/batch 0.44\n",
      "2020-08-17 07:01:20,400 INFO: | epoch   3 | step 49300 | batch 13300/18000 | lr 0.00000 0.00000 | loss 0.1443 | s/batch 0.43\n",
      "2020-08-17 07:01:46,556 INFO: | epoch   3 | step 49350 | batch 13350/18000 | lr 0.00000 0.00000 | loss 0.0928 | s/batch 0.52\n",
      "2020-08-17 07:02:09,284 INFO: | epoch   3 | step 49400 | batch 13400/18000 | lr 0.00000 0.00000 | loss 0.1282 | s/batch 0.45\n",
      "2020-08-17 07:02:37,564 INFO: | epoch   3 | step 49450 | batch 13450/18000 | lr 0.00000 0.00000 | loss 0.1160 | s/batch 0.57\n",
      "2020-08-17 07:02:59,841 INFO: | epoch   3 | step 49500 | batch 13500/18000 | lr 0.00000 0.00000 | loss 0.1245 | s/batch 0.45\n",
      "2020-08-17 07:03:26,187 INFO: | epoch   3 | step 49550 | batch 13550/18000 | lr 0.00000 0.00000 | loss 0.1302 | s/batch 0.53\n",
      "2020-08-17 07:03:50,771 INFO: | epoch   3 | step 49600 | batch 13600/18000 | lr 0.00000 0.00000 | loss 0.0693 | s/batch 0.49\n",
      "2020-08-17 07:04:13,420 INFO: | epoch   3 | step 49650 | batch 13650/18000 | lr 0.00000 0.00000 | loss 0.1144 | s/batch 0.45\n",
      "2020-08-17 07:04:38,334 INFO: | epoch   3 | step 49700 | batch 13700/18000 | lr 0.00000 0.00000 | loss 0.1369 | s/batch 0.50\n",
      "2020-08-17 07:04:59,053 INFO: | epoch   3 | step 49750 | batch 13750/18000 | lr 0.00000 0.00000 | loss 0.1169 | s/batch 0.41\n",
      "2020-08-17 07:05:21,047 INFO: | epoch   3 | step 49800 | batch 13800/18000 | lr 0.00000 0.00000 | loss 0.0710 | s/batch 0.44\n",
      "2020-08-17 07:05:43,572 INFO: | epoch   3 | step 49850 | batch 13850/18000 | lr 0.00000 0.00000 | loss 0.0898 | s/batch 0.45\n",
      "2020-08-17 07:06:05,608 INFO: | epoch   3 | step 49900 | batch 13900/18000 | lr 0.00000 0.00000 | loss 0.1347 | s/batch 0.44\n",
      "2020-08-17 07:06:26,269 INFO: | epoch   3 | step 49950 | batch 13950/18000 | lr 0.00000 0.00000 | loss 0.0867 | s/batch 0.41\n",
      "2020-08-17 07:06:49,893 INFO: | epoch   3 | step 50000 | batch 14000/18000 | lr 0.00000 0.00000 | loss 0.0738 | s/batch 0.47\n",
      "2020-08-17 07:07:15,464 INFO: | epoch   3 | step 50050 | batch 14050/18000 | lr 0.00000 0.00000 | loss 0.1489 | s/batch 0.51\n",
      "2020-08-17 07:07:41,083 INFO: | epoch   3 | step 50100 | batch 14100/18000 | lr 0.00000 0.00000 | loss 0.1113 | s/batch 0.51\n",
      "2020-08-17 07:08:03,390 INFO: | epoch   3 | step 50150 | batch 14150/18000 | lr 0.00000 0.00000 | loss 0.0647 | s/batch 0.45\n",
      "2020-08-17 07:08:27,126 INFO: | epoch   3 | step 50200 | batch 14200/18000 | lr 0.00000 0.00000 | loss 0.0737 | s/batch 0.47\n",
      "2020-08-17 07:08:49,959 INFO: | epoch   3 | step 50250 | batch 14250/18000 | lr 0.00000 0.00000 | loss 0.1487 | s/batch 0.46\n",
      "2020-08-17 07:09:12,610 INFO: | epoch   3 | step 50300 | batch 14300/18000 | lr 0.00000 0.00000 | loss 0.1203 | s/batch 0.45\n",
      "2020-08-17 07:09:36,348 INFO: | epoch   3 | step 50350 | batch 14350/18000 | lr 0.00000 0.00000 | loss 0.0931 | s/batch 0.47\n",
      "2020-08-17 07:10:00,438 INFO: | epoch   3 | step 50400 | batch 14400/18000 | lr 0.00000 0.00000 | loss 0.0712 | s/batch 0.48\n",
      "2020-08-17 07:10:22,063 INFO: | epoch   3 | step 50450 | batch 14450/18000 | lr 0.00000 0.00000 | loss 0.1429 | s/batch 0.43\n",
      "2020-08-17 07:10:44,440 INFO: | epoch   3 | step 50500 | batch 14500/18000 | lr 0.00000 0.00000 | loss 0.1398 | s/batch 0.45\n",
      "2020-08-17 07:11:09,810 INFO: | epoch   3 | step 50550 | batch 14550/18000 | lr 0.00000 0.00000 | loss 0.0940 | s/batch 0.51\n",
      "2020-08-17 07:11:32,491 INFO: | epoch   3 | step 50600 | batch 14600/18000 | lr 0.00000 0.00000 | loss 0.1333 | s/batch 0.45\n",
      "2020-08-17 07:11:56,811 INFO: | epoch   3 | step 50650 | batch 14650/18000 | lr 0.00000 0.00000 | loss 0.1181 | s/batch 0.49\n",
      "2020-08-17 07:12:16,695 INFO: | epoch   3 | step 50700 | batch 14700/18000 | lr 0.00000 0.00000 | loss 0.0957 | s/batch 0.40\n",
      "2020-08-17 07:12:42,504 INFO: | epoch   3 | step 50750 | batch 14750/18000 | lr 0.00000 0.00000 | loss 0.1089 | s/batch 0.52\n",
      "2020-08-17 07:13:09,039 INFO: | epoch   3 | step 50800 | batch 14800/18000 | lr 0.00000 0.00000 | loss 0.1576 | s/batch 0.53\n",
      "2020-08-17 07:13:32,602 INFO: | epoch   3 | step 50850 | batch 14850/18000 | lr 0.00000 0.00000 | loss 0.1038 | s/batch 0.47\n",
      "2020-08-17 07:13:58,456 INFO: | epoch   3 | step 50900 | batch 14900/18000 | lr 0.00000 0.00000 | loss 0.1486 | s/batch 0.52\n",
      "2020-08-17 07:14:19,561 INFO: | epoch   3 | step 50950 | batch 14950/18000 | lr 0.00000 0.00000 | loss 0.0902 | s/batch 0.42\n",
      "2020-08-17 07:14:42,300 INFO: | epoch   3 | step 51000 | batch 15000/18000 | lr 0.00000 0.00000 | loss 0.1728 | s/batch 0.45\n",
      "2020-08-17 07:15:05,380 INFO: | epoch   3 | step 51050 | batch 15050/18000 | lr 0.00000 0.00000 | loss 0.1147 | s/batch 0.46\n",
      "2020-08-17 07:15:27,235 INFO: | epoch   3 | step 51100 | batch 15100/18000 | lr 0.00000 0.00000 | loss 0.0673 | s/batch 0.44\n",
      "2020-08-17 07:15:49,092 INFO: | epoch   3 | step 51150 | batch 15150/18000 | lr 0.00000 0.00000 | loss 0.1497 | s/batch 0.44\n",
      "2020-08-17 07:16:11,057 INFO: | epoch   3 | step 51200 | batch 15200/18000 | lr 0.00000 0.00000 | loss 0.1036 | s/batch 0.44\n",
      "2020-08-17 07:16:34,453 INFO: | epoch   3 | step 51250 | batch 15250/18000 | lr 0.00000 0.00000 | loss 0.1313 | s/batch 0.47\n",
      "2020-08-17 07:17:01,606 INFO: | epoch   3 | step 51300 | batch 15300/18000 | lr 0.00000 0.00000 | loss 0.1811 | s/batch 0.54\n",
      "2020-08-17 07:17:23,135 INFO: | epoch   3 | step 51350 | batch 15350/18000 | lr 0.00000 0.00000 | loss 0.0563 | s/batch 0.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-17 07:17:46,471 INFO: | epoch   3 | step 51400 | batch 15400/18000 | lr 0.00000 0.00000 | loss 0.0702 | s/batch 0.47\n",
      "2020-08-17 07:18:10,809 INFO: | epoch   3 | step 51450 | batch 15450/18000 | lr 0.00000 0.00000 | loss 0.1050 | s/batch 0.49\n",
      "2020-08-17 07:18:37,757 INFO: | epoch   3 | step 51500 | batch 15500/18000 | lr 0.00000 0.00000 | loss 0.0915 | s/batch 0.54\n",
      "2020-08-17 07:19:01,177 INFO: | epoch   3 | step 51550 | batch 15550/18000 | lr 0.00000 0.00000 | loss 0.0615 | s/batch 0.47\n",
      "2020-08-17 07:19:27,778 INFO: | epoch   3 | step 51600 | batch 15600/18000 | lr 0.00000 0.00000 | loss 0.1688 | s/batch 0.53\n",
      "2020-08-17 07:19:50,442 INFO: | epoch   3 | step 51650 | batch 15650/18000 | lr 0.00000 0.00000 | loss 0.1385 | s/batch 0.45\n",
      "2020-08-17 07:20:14,128 INFO: | epoch   3 | step 51700 | batch 15700/18000 | lr 0.00000 0.00000 | loss 0.1031 | s/batch 0.47\n",
      "2020-08-17 07:20:40,193 INFO: | epoch   3 | step 51750 | batch 15750/18000 | lr 0.00000 0.00000 | loss 0.0992 | s/batch 0.52\n",
      "2020-08-17 07:21:06,829 INFO: | epoch   3 | step 51800 | batch 15800/18000 | lr 0.00000 0.00000 | loss 0.1220 | s/batch 0.53\n",
      "2020-08-17 07:21:33,562 INFO: | epoch   3 | step 51850 | batch 15850/18000 | lr 0.00000 0.00000 | loss 0.0930 | s/batch 0.53\n",
      "2020-08-17 07:21:55,922 INFO: | epoch   3 | step 51900 | batch 15900/18000 | lr 0.00000 0.00000 | loss 0.0846 | s/batch 0.45\n",
      "2020-08-17 07:22:19,897 INFO: | epoch   3 | step 51950 | batch 15950/18000 | lr 0.00000 0.00000 | loss 0.0785 | s/batch 0.48\n",
      "2020-08-17 07:22:41,398 INFO: | epoch   3 | step 52000 | batch 16000/18000 | lr 0.00000 0.00000 | loss 0.0786 | s/batch 0.43\n",
      "2020-08-17 07:23:04,229 INFO: | epoch   3 | step 52050 | batch 16050/18000 | lr 0.00000 0.00000 | loss 0.0960 | s/batch 0.46\n",
      "2020-08-17 07:23:29,393 INFO: | epoch   3 | step 52100 | batch 16100/18000 | lr 0.00000 0.00000 | loss 0.1009 | s/batch 0.50\n",
      "2020-08-17 07:23:53,770 INFO: | epoch   3 | step 52150 | batch 16150/18000 | lr 0.00000 0.00000 | loss 0.1545 | s/batch 0.49\n",
      "2020-08-17 07:24:16,702 INFO: | epoch   3 | step 52200 | batch 16200/18000 | lr 0.00000 0.00000 | loss 0.0801 | s/batch 0.46\n",
      "2020-08-17 07:24:38,586 INFO: | epoch   3 | step 52250 | batch 16250/18000 | lr 0.00000 0.00000 | loss 0.1228 | s/batch 0.44\n",
      "2020-08-17 07:25:00,506 INFO: | epoch   3 | step 52300 | batch 16300/18000 | lr 0.00000 0.00000 | loss 0.0980 | s/batch 0.44\n",
      "2020-08-17 07:25:23,354 INFO: | epoch   3 | step 52350 | batch 16350/18000 | lr 0.00000 0.00000 | loss 0.0723 | s/batch 0.46\n",
      "2020-08-17 07:25:45,037 INFO: | epoch   3 | step 52400 | batch 16400/18000 | lr 0.00000 0.00000 | loss 0.1466 | s/batch 0.43\n",
      "2020-08-17 07:26:09,240 INFO: | epoch   3 | step 52450 | batch 16450/18000 | lr 0.00000 0.00000 | loss 0.1562 | s/batch 0.48\n",
      "2020-08-17 07:26:30,399 INFO: | epoch   3 | step 52500 | batch 16500/18000 | lr 0.00000 0.00000 | loss 0.1110 | s/batch 0.42\n",
      "2020-08-17 07:26:58,361 INFO: | epoch   3 | step 52550 | batch 16550/18000 | lr 0.00000 0.00000 | loss 0.0629 | s/batch 0.56\n",
      "2020-08-17 07:27:23,380 INFO: | epoch   3 | step 52600 | batch 16600/18000 | lr 0.00000 0.00000 | loss 0.1365 | s/batch 0.50\n",
      "2020-08-17 07:27:44,958 INFO: | epoch   3 | step 52650 | batch 16650/18000 | lr 0.00000 0.00000 | loss 0.0653 | s/batch 0.43\n",
      "2020-08-17 07:28:09,514 INFO: | epoch   3 | step 52700 | batch 16700/18000 | lr 0.00000 0.00000 | loss 0.0936 | s/batch 0.49\n",
      "2020-08-17 07:28:33,165 INFO: | epoch   3 | step 52750 | batch 16750/18000 | lr 0.00000 0.00000 | loss 0.1013 | s/batch 0.47\n",
      "2020-08-17 07:28:55,265 INFO: | epoch   3 | step 52800 | batch 16800/18000 | lr 0.00000 0.00000 | loss 0.1086 | s/batch 0.44\n",
      "2020-08-17 07:29:15,204 INFO: | epoch   3 | step 52850 | batch 16850/18000 | lr 0.00000 0.00000 | loss 0.0994 | s/batch 0.40\n",
      "2020-08-17 07:29:38,138 INFO: | epoch   3 | step 52900 | batch 16900/18000 | lr 0.00000 0.00000 | loss 0.1348 | s/batch 0.46\n",
      "2020-08-17 07:29:59,026 INFO: | epoch   3 | step 52950 | batch 16950/18000 | lr 0.00000 0.00000 | loss 0.0823 | s/batch 0.42\n",
      "2020-08-17 07:30:22,174 INFO: | epoch   3 | step 53000 | batch 17000/18000 | lr 0.00000 0.00000 | loss 0.0608 | s/batch 0.46\n",
      "2020-08-17 07:30:47,816 INFO: | epoch   3 | step 53050 | batch 17050/18000 | lr 0.00000 0.00000 | loss 0.1359 | s/batch 0.51\n",
      "2020-08-17 07:31:12,204 INFO: | epoch   3 | step 53100 | batch 17100/18000 | lr 0.00000 0.00000 | loss 0.1491 | s/batch 0.49\n",
      "2020-08-17 07:31:33,047 INFO: | epoch   3 | step 53150 | batch 17150/18000 | lr 0.00000 0.00000 | loss 0.0833 | s/batch 0.42\n",
      "2020-08-17 07:31:56,529 INFO: | epoch   3 | step 53200 | batch 17200/18000 | lr 0.00000 0.00000 | loss 0.1133 | s/batch 0.47\n",
      "2020-08-17 07:32:20,010 INFO: | epoch   3 | step 53250 | batch 17250/18000 | lr 0.00000 0.00000 | loss 0.1268 | s/batch 0.47\n",
      "2020-08-17 07:32:43,023 INFO: | epoch   3 | step 53300 | batch 17300/18000 | lr 0.00000 0.00000 | loss 0.1323 | s/batch 0.46\n",
      "2020-08-17 07:33:07,493 INFO: | epoch   3 | step 53350 | batch 17350/18000 | lr 0.00000 0.00000 | loss 0.0782 | s/batch 0.49\n",
      "2020-08-17 07:33:28,579 INFO: | epoch   3 | step 53400 | batch 17400/18000 | lr 0.00000 0.00000 | loss 0.0911 | s/batch 0.42\n",
      "2020-08-17 07:33:52,139 INFO: | epoch   3 | step 53450 | batch 17450/18000 | lr 0.00000 0.00000 | loss 0.1193 | s/batch 0.47\n",
      "2020-08-17 07:34:15,961 INFO: | epoch   3 | step 53500 | batch 17500/18000 | lr 0.00000 0.00000 | loss 0.0985 | s/batch 0.48\n",
      "2020-08-17 07:34:41,566 INFO: | epoch   3 | step 53550 | batch 17550/18000 | lr 0.00000 0.00000 | loss 0.1131 | s/batch 0.51\n",
      "2020-08-17 07:35:05,363 INFO: | epoch   3 | step 53600 | batch 17600/18000 | lr 0.00000 0.00000 | loss 0.1186 | s/batch 0.48\n",
      "2020-08-17 07:35:30,260 INFO: | epoch   3 | step 53650 | batch 17650/18000 | lr 0.00000 0.00000 | loss 0.0959 | s/batch 0.50\n",
      "2020-08-17 07:35:53,369 INFO: | epoch   3 | step 53700 | batch 17700/18000 | lr 0.00000 0.00000 | loss 0.1385 | s/batch 0.46\n",
      "2020-08-17 07:36:19,481 INFO: | epoch   3 | step 53750 | batch 17750/18000 | lr 0.00000 0.00000 | loss 0.1823 | s/batch 0.52\n",
      "2020-08-17 07:36:42,807 INFO: | epoch   3 | step 53800 | batch 17800/18000 | lr 0.00000 0.00000 | loss 0.1287 | s/batch 0.47\n",
      "2020-08-17 07:37:07,738 INFO: | epoch   3 | step 53850 | batch 17850/18000 | lr 0.00000 0.00000 | loss 0.1163 | s/batch 0.50\n",
      "2020-08-17 07:37:28,622 INFO: | epoch   3 | step 53900 | batch 17900/18000 | lr 0.00000 0.00000 | loss 0.1342 | s/batch 0.42\n",
      "2020-08-17 07:37:47,935 INFO: | epoch   3 | step 53950 | batch 17950/18000 | lr 0.00000 0.00000 | loss 0.1280 | s/batch 0.39\n",
      "2020-08-17 07:38:12,633 INFO: | epoch   3 | step 54000 | batch 18000/18000 | lr 0.00000 0.00000 | loss 0.1117 | s/batch 0.49\n",
      "2020-08-17 07:38:12,856 INFO: | epoch   3 | score (96.18, 95.87, 96.03) | f1 96.03 | loss 0.1178 | time 8440.80\n",
      "2020-08-17 07:38:13,193 INFO: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9631    0.9653    0.9642     35027\n",
      "          股票     0.9682    0.9677    0.9679     33251\n",
      "          体育     0.9910    0.9920    0.9915     28283\n",
      "          娱乐     0.9748    0.9812    0.9780     19920\n",
      "          时政     0.9357    0.9472    0.9414     13515\n",
      "          社会     0.9383    0.9341    0.9362     11009\n",
      "          教育     0.9688    0.9660    0.9674      8987\n",
      "          财经     0.9274    0.9100    0.9186      7957\n",
      "          家居     0.9644    0.9636    0.9640      7063\n",
      "          游戏     0.9558    0.9410    0.9484      5291\n",
      "          房产     0.9927    0.9874    0.9900      4428\n",
      "          时尚     0.9652    0.9546    0.9599      2818\n",
      "          彩票     0.9578    0.9420    0.9499      1639\n",
      "          星座     0.9621    0.9704    0.9663       812\n",
      "\n",
      "    accuracy                         0.9654    180000\n",
      "   macro avg     0.9618    0.9587    0.9603    180000\n",
      "weighted avg     0.9654    0.9654    0.9654    180000\n",
      "\n",
      "2020-08-17 07:48:48,138 INFO: | epoch   3 | dev | score (95.69, 95.32, 95.49) | f1 95.49 | time 634.94\n",
      "2020-08-17 07:48:48,188 INFO: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          科技     0.9612    0.9561    0.9586      3891\n",
      "          股票     0.9659    0.9656    0.9658      3694\n",
      "          体育     0.9867    0.9930    0.9898      3142\n",
      "          娱乐     0.9695    0.9756    0.9725      2213\n",
      "          时政     0.9208    0.9454    0.9329      1501\n",
      "          社会     0.9419    0.9272    0.9345      1223\n",
      "          教育     0.9602    0.9669    0.9636       998\n",
      "          财经     0.9215    0.9027    0.9120       884\n",
      "          家居     0.9576    0.9515    0.9546       784\n",
      "          游戏     0.9468    0.9404    0.9436       587\n",
      "          房产     0.9959    0.9878    0.9918       492\n",
      "          时尚     0.9490    0.9521    0.9506       313\n",
      "          彩票     0.9704    0.9011    0.9345       182\n",
      "          星座     0.9495    0.9792    0.9641        96\n",
      "\n",
      "    accuracy                         0.9611     20000\n",
      "   macro avg     0.9569    0.9532    0.9549     20000\n",
      "weighted avg     0.9611    0.9611    0.9611     20000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-17 07:48:48,191 INFO: Exceed history dev = 95.09, current dev = 95.49\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test\n",
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert 优化参数比较\n",
    "\n",
    "以rows=2000为测试行数，进行调优\n",
    "\n",
    "| 优化器              | sent_hidden_size | batch_size | loss   | f1       | time   | fold_num | 排行 |\n",
    "| :------------------- | :---------------- | :---------- | :------ | :-------- | :------ | :-------- | :---- |\n",
    "| torch.optim.SGD     | 256              | 10         | 2.4983 | 8.3      | 99.81  | 10       |10|\n",
    "| torch.optim.RMSprop | 256              | 10         | 1.8028 | 26.84    | 99.65  | 10       | 3 |\n",
    "| torch.optim.Adagrad | 256              | 10         | 1.9230 | 14.65    | 98.70  | 10       |9|\n",
    "| torch.optim.RMSprop | 512              | 10         | 1.9720 | 23.27    | 101.48 | 10       |7|\n",
    "|                     | 128              | 10         | 1.7905 | 23.56 | 105.51 | 10       |5 |\n",
    "|                     | 256              | 13         | 1.8140 | 22.94    | 97.86  | 10       |8|\n",
    "|                     | 256              | 16         | 1.9063 | 23.56    | 96.54  | 10       |6|\n",
    "|                     | 256              | 6          | 1.8069 | 25.22    | 123.51 | 10       | 4 |\n",
    "|                     | 256              | 10         | 1.3445 | 54.43    | 111.90 | 15       | 1 |\n",
    "|                     | 256              | 10         | 1.3791 | 52.17    | 111.60 | 20       | 2 |\n",
    "\n",
    "\n",
    "由以上对比结果，选择排名第一的组合：\n",
    "\n",
    "| 优化器              | sent_hidden_size | batch_size | loss   | f1       | time   | fold_num | 排行 |\n",
    "| :------------------- | :---------------- | :---------- | :------ | :-------- | :------ | :-------- | :---- |\n",
    "| torch.optim.RMSprop  | 256              | 10         | 1.3445 | 54.43    | 111.90 | 15       | 1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以此训练后，得分：0.9562"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
